{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Define the configuration for GPT-2\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,        # The size of the vocabulary\n",
    "    n_positions=1024,        # The maximum sequence length\n",
    "    n_ctx=1024,              # The context size\n",
    "    n_embd=768,              # Dimensionality of the embeddings\n",
    "    n_layer=12,              # Number of hidden layers (transformer blocks)\n",
    "    n_head=12,               # Number of attention heads\n",
    "    intermediate_size=3072,  # Dimensionality of the feed-forward layer\n",
    "    activation_function='gelu_new',  # Activation function (GELU)\n",
    "    initializer_range=0.02,  # Weight initialization range\n",
    ")\n",
    "\n",
    "# Construct a GPT-2 model from the configuration\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Count the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
