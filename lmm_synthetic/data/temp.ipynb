{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from tqdm import trange\n",
    "import os \n",
    "\n",
    "def create_grid(num_rows: int, num_cols: int, vocab: List[str], vocab_subset_size: int) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Creates a grid with the specified number of rows and columns,\n",
    "    randomly sampling objects from the provided vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    - num_rows: int - The number of rows in the grid.\n",
    "    - num_cols: int - The number of columns in the grid.\n",
    "    - vocab: List[str] - The vocabulary of objects to populate the grid.\n",
    "    - vocab_subset_size: int - The size of the subset of the vocabulary to use.\n",
    "\n",
    "    Returns:\n",
    "    - List[List[str]] - The generated grid.\n",
    "    \"\"\"\n",
    "    vocab_subset = random.sample(vocab, vocab_subset_size)\n",
    "    grid = []\n",
    "\n",
    "\n",
    "    grid = [[random.choice(vocab_subset) for _ in range(num_cols)] for _ in range(num_rows)]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def convert_grid_to_str(grid: List[List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Converts a 2D grid into a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - grid: List[List[str]] - The grid to convert.\n",
    "\n",
    "    Returns:\n",
    "    - str - The grid formatted as a string.\n",
    "    \"\"\"\n",
    "    rows = ['| ' + ' | '.join(row) + ' |' for row in grid]\n",
    "    return '\\n'.join(rows)\n",
    "\n",
    "def add_grid_instruction(grid: List[List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Adds an instruction describing the grid.\n",
    "\n",
    "    Parameters:\n",
    "    - grid: List[List[str]] - The grid to describe.\n",
    "\n",
    "    Returns:\n",
    "    - str - The instruction describing the grid.\n",
    "    \"\"\"\n",
    "    num_rows = len(grid)\n",
    "    num_cols = len(grid[0])\n",
    "    used_vocab_subset = set()\n",
    "    for row in grid:\n",
    "        for element in row:\n",
    "            used_vocab_subset.add(element)\n",
    "    used_vocab_subset = list(used_vocab_subset)\n",
    "    return f\"The grid above is size {num_rows} by {num_cols}. Each cell contains an object from {used_vocab_subset}.\"\n",
    "\n",
    "def create_position_questions(grid: List[List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Adds questions about the position of each object in the grid.\n",
    "\n",
    "    Parameters:\n",
    "    - grid: List[List[str]] - The grid to generate questions for.\n",
    "\n",
    "    Returns:\n",
    "    - List[str] - The questions about the grid.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            questions.append(f\"What object is in row {i}, column {j}? \" + f\"A: {grid[i][j]}\")\n",
    "    return questions\n",
    "\n",
    "def create_position_assertions(grid: List[List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Adds assertions about the position of each object in the grid.\n",
    "\n",
    "    Parameters:\n",
    "    - grid: List[List[str]] - The grid to generate questions for.\n",
    "\n",
    "    Returns:\n",
    "    - List[str] - The questions about the grid.\n",
    "    \"\"\"\n",
    "    assertions = []\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            assertions.append(f\"row {i}, column {j}\" + f\"A: {grid[i][j]}\")\n",
    "    return assertions\n",
    "\n",
    "def create_dataset_from_json(args) -> datasets.DatasetDict:\n",
    "    \"\"\"\n",
    "    Creates a synthetic text dataset based on parameters from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - args: argparse.Namespace - The arguments containing the path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "    - datasets.DatasetDict - The Hugging Face DatasetDict object containing the synthetic dataset with splits.\n",
    "    \"\"\"\n",
    "    # Load parameters from JSON file\n",
    "    with open(args.config, 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    # Validate parameters\n",
    "    required_keys = ['num_samples', 'num_rows', 'num_cols', 'vocab', 'vocab_subset_size']\n",
    "    for key in required_keys:\n",
    "        if key not in params:\n",
    "            raise ValueError(f\"JSON file must contain '{key}'.\")\n",
    "\n",
    "    num_samples = dict(params['num_samples'])\n",
    "    num_train_samples = int(num_samples['train'])\n",
    "    num_val_samples = int(num_samples['validation'])\n",
    "    num_test_samples = int(num_samples['test'])\n",
    "    total_samples = sum([num_train_samples, num_val_samples, num_test_samples])\n",
    "    num_rows = int(params['num_rows'])\n",
    "    num_cols = int(params['num_cols'])\n",
    "    vocab = list(params['vocab'])\n",
    "    vocab_subset_size = int(params['vocab_subset_size'])\n",
    "    num_questions = int(params['num_questions'])\n",
    "    dataset_name = os.path.basename(args.config).split('.')[0]\n",
    "    dt_str = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    logger.info(f\"Generating synthetic dataset with {num_samples} samples, {num_rows} rows, {num_cols} columns, and vocabulary: {vocab}\")\n",
    "\n",
    "    samples = []\n",
    "    for _ in trange(total_samples, desc=\"Generating samples\"):\n",
    "        grid = create_grid(num_rows, num_cols, vocab, vocab_subset_size)\n",
    "        sample_str = convert_grid_to_str(grid)\n",
    "        sample_str += \"\\n\" + add_grid_instruction(grid)\n",
    "        for question in random.sample(create_position_questions(grid), num_questions):\n",
    "            sample_str += \"\\n\" + question\n",
    "        samples.append({'text': sample_str, 'grid': grid})\n",
    "\n",
    "    # Convert to a pandas DataFrame for easier dataset creation\n",
    "    df = pd.DataFrame(samples)\n",
    "\n",
    "    # Save dataset as a Hugging Face-friendly dataset\n",
    "    dataset = datasets.Dataset.from_pandas(df)\n",
    "    \n",
    "    # Split dataset into train, validation, and test sets using the specified sample sizes\n",
    "    train_test_split = dataset.train_test_split(test_size=(num_val_samples + num_test_samples) / total_samples)\n",
    "    test_val_split = train_test_split['test'].train_test_split(test_size=num_test_samples / (num_val_samples + num_test_samples))\n",
    "    \n",
    "    # Combine splits into a DatasetDict\n",
    "    dataset_dict = datasets.DatasetDict({\n",
    "        'train': train_test_split['train'],\n",
    "        'validation': test_val_split['train'],\n",
    "        'test': test_val_split['test']\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Generated dataset with {len(dataset_dict['train'])} training samples, {len(dataset_dict['validation'])} validation samples, and {len(dataset_dict['test'])} test samples.\")\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    dataset_dir = os.path.join(args.output_dir, f\"{dataset_name}_{dt_str}\")\n",
    "    \n",
    "    # Save the entire dataset dictionary to disk\n",
    "    dataset_dict.save_to_disk(dataset_dir)\n",
    "    logger.info(f\"Saved dataset to disk at: {dataset_dir}\")\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['automobile', 'ship', 'truck'], ['truck', 'airplane', 'automobile'], ['automobile', 'automobile', 'truck']]\n"
     ]
    }
   ],
   "source": [
    "test = create_grid(3, 3, ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog', 'dog', 'truck'], ['cat', 'dog', 'deer'], ['truck', 'deer', 'deer']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| cat | airplane | cat |\\n| ship | airplane | bird |\\n| cat | airplane | cat |'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_grid_to_str(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
