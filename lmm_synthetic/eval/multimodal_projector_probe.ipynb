{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2024-12-17 21:03:06.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mLoading model and tokenizer from /home/allanz/data/vlm_checkpoint/final_model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-17 21:03:08.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-17 21:03:08.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n",
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-12-17 21:03:09.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_multimodal_projector\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mLoading multimodal projector: linear\u001b[0m\n",
      "\u001b[32m2024-12-17 21:03:09.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-17 21:03:09.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_from_disk\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, GPTJForCausalLM\n",
    "from lmm_synthetic.mm_train.gptj_vlm import GPTJ_VLM\n",
    "from lmm_synthetic.mm_train.utils import load_vision_encoder\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths \n",
    "dataset_path = '/data/lmm/generated/v3_spatial_grid_multimodal'\n",
    "vlm_path = '/home/allanz/data/vlm_checkpoint/final_model'\n",
    "lm_path = \"/data/lmm/checkpoints/lm/lm-pretrain-only-checkpoint-1953\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(dataset)\n",
    "\n",
    "# Load VLM and CLIP model\n",
    "def load_model_and_tokenizer(model_path, multimodal=False):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from the specified path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model and tokenizer from {model_path}\")\n",
    "    model, tokenizer = None, None\n",
    "    if multimodal:\n",
    "        model = GPTJ_VLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model.config.pretrained_lm_path)\n",
    "    else:\n",
    "        model = GPTJForCausalLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "vlm, vlm_tokenizer = load_model_and_tokenizer(vlm_path, multimodal=True)\n",
    "\n",
    "clip_vision_model = vlm.vision_encoder.clip_vision_model\n",
    "encoder, image_transforms, _ = load_vision_encoder(\"clip\")\n",
    "\n",
    "#Parse grid\n",
    "def parse_grid(grid_str, K):\n",
    "    \"\"\"\n",
    "    Parse the grid string into a 2D list of grid cells.\n",
    "    \"\"\"\n",
    "    grid_str = '\\n'.join(grid_str.split('\\n')[:K])\n",
    "    rows = grid_str.strip().split('\\n')\n",
    "    return [[cell.strip() for cell in row.split('|') if cell.strip()] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlm.multimodal_projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [[0, 1, 2, 7, 8, 9, 14, 15, 16], [2, 3, 4, 9, 10, 11, 16, 17, 18], [4, 5, 6, 11, 12, 13, 18, 19, 20],\n",
    "           [14, 15, 16, 21, 22, 23, 28, 29, 30], [16, 17, 18, 23, 24, 25, 30, 31, 32], [18, 19, 20, 25, 26, 27, 32, 33, 34],\n",
    "           [28, 29, 30, 35, 36, 37, 42, 43, 44], [30, 31, 32, 37, 38, 39, 44, 45, 46], [32, 33, 34, 39, 40, 41, 46, 47, 48]]\n",
    "\n",
    "def position_concat(image_tensor, patch = patches):\n",
    "    \"\"\"\n",
    "    Concats tensors, should return shape 9x1x6912, \n",
    "    1x6912 tensor fore each position in patch, \n",
    "    9 patches\n",
    "    \"\"\"\n",
    "    concat = []\n",
    "    for position in patch:\n",
    "        concat.append(torch.cat([image_tensor[0][i] for i in position]).unsqueeze(0))\n",
    "    \n",
    "    return torch.stack(concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refactor \n",
    "def prepare_data_no_batch(set_type, num_samples, prints = False):\n",
    "    \"\"\"\n",
    "    Prepares data to feed into model by turning images \n",
    "    into 1 x 9 x 6912 tensors, grid cell information \n",
    "    into 1 x 9 x 10 tensors \n",
    "    \"\"\" \n",
    "    ANIMALS = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder.to(device)\n",
    "    multimodal_projector = vlm.multimodal_projector\n",
    "    multimodal_projector.to(device) \n",
    "\n",
    "    reformatted_data = []\n",
    "    t_0 = time.perf_counter()\n",
    "    \n",
    "    # Extract image tensors and grid cells in each batch\n",
    "    for i in range(num_samples):\n",
    "        image = Image.open(f\"/data/lmm/generated/v3_spatial_grid_multimodal/images/{set_type}_{i}.png\")\n",
    "        image_tensor = image_transforms(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embeddings = encoder(image_tensor)\n",
    "            # Projector\n",
    "            image_tensor_tokens = vlm.multimodal_projector(embeddings)\n",
    "\n",
    "        grid = parse_grid(dataset[set_type][i]['text'], 3)\n",
    "        \n",
    "        temp = []\n",
    "        for row in grid:\n",
    "            for animal in row:\n",
    "                #temp_tensor = torch.zeros(1,10)\n",
    "                #temp_tensor[0][ANIMALS.index(animal)] = 1\n",
    "                #temp.append(temp_tensor)\n",
    "                temp.append(ANIMALS.index(animal)) \n",
    "\n",
    "        # Tuple of (image tokens (9 x 1 x 6912), correct labels is list. 1 x 9)\n",
    "        reformatted_data.append((position_concat(image_tensor_tokens.to(device)), temp)) \n",
    "\n",
    "        if i % 200 == 0 and prints == True:\n",
    "            print(f\"Processed {i} images\") \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    t_3 = time.perf_counter()\n",
    "    print(f\"Finished preparing data in {t_3 - t_0} seconds\")\n",
    "    return reformatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbfgs_linear_classifier(position, dataset, num_iterations = 100, lr = 1.0):\n",
    "    \"\"\"\n",
    "    Trains a linear cliassifier using L-BFGS\n",
    "\n",
    "    Args:\n",
    "        position (int): position of the grid cell to train on (0-8)\n",
    "        dataset (list): each tuple should contain 9 x 1 x 6912 tensor and a list with 9 values\n",
    "            1 x 6912 tensor for each position in patch, value corresponding with animal index in position\n",
    "        num_iterations (int): number of iterations for L-BFGS optimization\n",
    "        lr (float): learning rate for L-BFGS optimization\n",
    "\n",
    "    Returns:  \n",
    "        model (nn.Module): trained linear classifier\n",
    "        log_losses (list): list of losses at each iteration\n",
    "    \"\"\"\n",
    "    # Initialize data and labels \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.concat([dataset[i][0][position] for i in range(len(dataset))], dim = 0).to(device)\n",
    "    temp_label = []\n",
    "    for i in range(len(dataset)):\n",
    "        temp_label.append(dataset[i][1][position])\n",
    "    labels = torch.tensor(temp_label).to(device)\n",
    "\n",
    "    # Check inputs\n",
    "    if len(data) == len(labels) and data.dim() == 2 and labels.dim() == 1:\n",
    "        print(\"Data is formatted correctly\")\n",
    "    else:\n",
    "        print(\"Look over format of dataset\")\n",
    "\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "\n",
    "\n",
    "    class LinearClassifier(nn.Module):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super(LinearClassifier, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "    \n",
    "    input_dim = data.shape[1]\n",
    "    print(input_dim)\n",
    "    #num_classes = len(torch.unique(labels))\n",
    "    num_classes = 10\n",
    "    print(num_classes)\n",
    "\n",
    "    model = LinearClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "    # Use cross entropy loss for classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr = lr, max_iter = num_iterations)\n",
    "\n",
    "    log_losses = []\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        log_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "\n",
    "    print(f\"Training complete for position {position}. Final loss: {log_losses[-1]:.4f}\")\n",
    "    return model, log_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(position, dataset, num_samples = 1000, premodel = None):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model on the either test or validation set \n",
    "\n",
    "    Args:\n",
    "        position (int): position of the grid cell to train on (0-8)\n",
    "        dataset (str): either \"test\" or \"validation\"\n",
    "        num_samples (int): number of samples to calculate accuracy on\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): accuracy of the model on the dataset\n",
    "        lowest (str): worst performing class\n",
    "    \"\"\"\n",
    "    if premodel is None:\n",
    "        model = models[position]\n",
    "    else:\n",
    "        model = premodel\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = prepare_data_no_batch(dataset, num_samples)\n",
    "\n",
    "    cleaned_data = torch.concat([data[i][0][position] for i in range(len(data))], dim = 0).to(device)\n",
    "    temp_label = []\n",
    "    for i in range(len(data)):\n",
    "        temp_label.append(data[i][1][position])\n",
    "\n",
    "    labels = torch.tensor(temp_label).to(device)\n",
    "\n",
    "                                         \n",
    "\n",
    "    incorrect = 0 \n",
    "    total = 0\n",
    "\n",
    "    incorrect_guesses = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        with torch.no_grad():\n",
    "            output = model(cleaned_data[i])\n",
    "            prediction = torch.argmax(output)\n",
    "            if prediction != labels[i]:\n",
    "                incorrect += 1\n",
    "                incorrect_guesses.append((prediction, labels[i]))\n",
    "            total += 1\n",
    "    \n",
    "    print(f\"Accuracy for position {position} on {dataset}: {1 - incorrect/total:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing data in 122.77460218966007 seconds\n"
     ]
    }
   ],
   "source": [
    "train_data = prepare_data_no_batch(\"train\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 0. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 1. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 2. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 3. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 4. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 5. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 6. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 7. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 8. Final loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "linear0, linear0_log = train_lbfgs_linear_classifier(0, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear1, linear1_log = train_lbfgs_linear_classifier(1, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear2, linear2_log = train_lbfgs_linear_classifier(2, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear3, linear3_log = train_lbfgs_linear_classifier(3, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear4, linear4_log = train_lbfgs_linear_classifier(4, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear5, linear5_log = train_lbfgs_linear_classifier(5, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear6, linear6_log = train_lbfgs_linear_classifier(6, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear7, linear7_log = train_lbfgs_linear_classifier(7, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear8, linear8_log = train_lbfgs_linear_classifier(8, train_data, num_iterations = 100, lr = 1.0)\n",
    "\n",
    "models = {0: linear0, 1: linear1, 2: linear2, 3: linear3, 4: linear4, 5: linear5, 6: linear6, 7: linear7, 8: linear8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing data in 12.15949971973896 seconds\n",
      "Accuracy for position 0 on test: 1.0000\n",
      "Finished preparing data in 12.347612246870995 seconds\n",
      "Accuracy for position 1 on test: 1.0000\n",
      "Finished preparing data in 11.875313021242619 seconds\n",
      "Accuracy for position 2 on test: 1.0000\n",
      "Finished preparing data in 12.323789902031422 seconds\n",
      "Accuracy for position 3 on test: 1.0000\n",
      "Finished preparing data in 11.902767654508352 seconds\n",
      "Accuracy for position 4 on test: 1.0000\n",
      "Finished preparing data in 12.121486186981201 seconds\n",
      "Accuracy for position 5 on test: 1.0000\n",
      "Finished preparing data in 12.358559433370829 seconds\n",
      "Accuracy for position 6 on test: 1.0000\n",
      "Finished preparing data in 12.432540699839592 seconds\n",
      "Accuracy for position 7 on test: 1.0000\n",
      "Finished preparing data in 11.95513591915369 seconds\n",
      "Accuracy for position 8 on test: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    accuracy(i, \"test\", 1000, premodel = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
