{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DEVELOPMENT_MODE = False\n",
    "# Detect if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install if in Colab\n",
    "if IN_COLAB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    # Install a faster Node version\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
    "\n",
    "# Hot reload in development mode & not running on the CD\n",
    "if not IN_COLAB:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if not ip.extension_manager.loaded:\n",
    "        ip.extension_manager.load('autoreload')\n",
    "        %autoreload 2\n",
    "\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "print(\"works\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: colab\n"
     ]
    }
   ],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jennyni/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7cea46384670>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "test = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\allan\\ResearchStuff\\checkpoint-1953\"\n",
    "\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "pretrained_model = GPTJForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTJBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc_out): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "import math\n",
    "\n",
    "config = {\n",
    "    \"d_model\" : 2048, \n",
    "    \"d_head\" : 128, \n",
    "    \"n_heads\" : 16, \n",
    "    \"d_mlp\" : 8192,\n",
    "    \"n_layers\" : 28, \n",
    "    \"n_ctx\" : 2048,\n",
    "    \"eps\" : 1e-5, \n",
    "    \"d_vocab\" : 50257, \n",
    "    \"act_fn\" : \"gelu\", \n",
    "    \"use_attn_scale\" : math.sqrt(128),\n",
    "    \"use_local_attn\" : False, \n",
    "    \"scale_attn_by_inverse_layer_idx\": False,\n",
    "    \"parallel_attn_mlp\": True,\n",
    "    \"positional_embedding_type\": \"rotary\",\n",
    "    \"rotary_dim\": 64,\n",
    "    \"rotary_adjacent_pairs\": True,\n",
    "    \"normalization_type\": \"LN\",\n",
    "    \"model_name\" : \"custom\",\n",
    "    \"original_architecture\" : \"gptj\",\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"        model_name (str): the name of the model, used to load\n",
    "            weights from HuggingFace or initialized to \"custom\" if not passed\n",
    "        original_architecture (str, *optional*): the family of the model, used\n",
    "        to help load\n",
    "            weights from HuggingFace or initialized to \"custom\" if not passed\n",
    "        from_checkpoint (bool): Whether the model weights were\n",
    "            loaded from a checkpoint (only applies to pretrained models)\n",
    "        checkpoint_index (int, *optional*): The index of the\n",
    "            checkpoint loaded (only applies to pretrained models).\n",
    "        checkpoint_label_type (str, *optional*): Whether\n",
    "            checkpoints are labelled by the number of steps or number of tokens.\n",
    "        checkpoint_value (int, *optional*): The value of the\n",
    "            checkpoint label (whether of steps or tokens).\n",
    "        tokenizer_name (str, *optional*): the full name of the model, passed into\n",
    "            HuggingFace to access the tokenizer. Only used when passing in\n",
    "            custom config, if loading from pretrained then this is not needed.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-27): 28 x TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"/home/sjoshi/lmm/lmm_synthetic/lm_train/checkpoints/v3_spatial_grid_gptj/checkpoint-1953\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "\n",
    "with open(f\"{CHECKPOINT_DIR}/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# create the hooked transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json',\n",
       " 'generation_config.json',\n",
       " 'model.safetensors',\n",
       " 'tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'added_tokens.json',\n",
       " 'vocab.json',\n",
       " 'merges.txt',\n",
       " 'tokenizer.json',\n",
       " 'training_args.bin',\n",
       " 'global_step1953',\n",
       " 'zero_to_fp32.py',\n",
       " 'latest',\n",
       " 'scheduler.pt',\n",
       " 'rng_state_1.pth',\n",
       " 'rng_state_6.pth',\n",
       " 'rng_state_2.pth',\n",
       " 'rng_state_7.pth',\n",
       " 'rng_state_3.pth',\n",
       " 'rng_state_5.pth',\n",
       " 'rng_state_0.pth',\n",
       " 'rng_state_4.pth',\n",
       " 'trainer_state.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.listdir(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors.torch \n",
    "\n",
    "state_dict = safetensors.torch.load_file(f\"{CHECKPOINT_DIR}/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HookedTransformer:\n\tMissing key(s) in state_dict: \"embed.W_E\", \"blocks.0.ln1.w\", \"blocks.0.ln1.b\", \"blocks.0.ln2.w\", \"blocks.0.ln2.b\", \"blocks.0.attn.W_Q\", \"blocks.0.attn.W_O\", \"blocks.0.attn.b_Q\", \"blocks.0.attn.b_O\", \"blocks.0.attn.W_K\", \"blocks.0.attn.W_V\", \"blocks.0.attn.b_K\", \"blocks.0.attn.b_V\", \"blocks.0.attn.mask\", \"blocks.0.attn.IGNORE\", \"blocks.0.attn.rotary_sin\", \"blocks.0.attn.rotary_cos\", \"blocks.0.mlp.W_in\", \"blocks.0.mlp.b_in\", \"blocks.0.mlp.W_out\", \"blocks.0.mlp.b_out\", \"blocks.1.ln1.w\", \"blocks.1.ln1.b\", \"blocks.1.ln2.w\", \"blocks.1.ln2.b\", \"blocks.1.attn.W_Q\", \"blocks.1.attn.W_O\", \"blocks.1.attn.b_Q\", \"blocks.1.attn.b_O\", \"blocks.1.attn.W_K\", \"blocks.1.attn.W_V\", \"blocks.1.attn.b_K\", \"blocks.1.attn.b_V\", \"blocks.1.attn.mask\", \"blocks.1.attn.IGNORE\", \"blocks.1.attn.rotary_sin\", \"blocks.1.attn.rotary_cos\", \"blocks.1.mlp.W_in\", \"blocks.1.mlp.b_in\", \"blocks.1.mlp.W_out\", \"blocks.1.mlp.b_out\", \"blocks.2.ln1.w\", \"blocks.2.ln1.b\", \"blocks.2.ln2.w\", \"blocks.2.ln2.b\", \"blocks.2.attn.W_Q\", \"blocks.2.attn.W_O\", \"blocks.2.attn.b_Q\", \"blocks.2.attn.b_O\", \"blocks.2.attn.W_K\", \"blocks.2.attn.W_V\", \"blocks.2.attn.b_K\", \"blocks.2.attn.b_V\", \"blocks.2.attn.mask\", \"blocks.2.attn.IGNORE\", \"blocks.2.attn.rotary_sin\", \"blocks.2.attn.rotary_cos\", \"blocks.2.mlp.W_in\", \"blocks.2.mlp.b_in\", \"blocks.2.mlp.W_out\", \"blocks.2.mlp.b_out\", \"blocks.3.ln1.w\", \"blocks.3.ln1.b\", \"blocks.3.ln2.w\", \"blocks.3.ln2.b\", \"blocks.3.attn.W_Q\", \"blocks.3.attn.W_O\", \"blocks.3.attn.b_Q\", \"blocks.3.attn.b_O\", \"blocks.3.attn.W_K\", \"blocks.3.attn.W_V\", \"blocks.3.attn.b_K\", \"blocks.3.attn.b_V\", \"blocks.3.attn.mask\", \"blocks.3.attn.IGNORE\", \"blocks.3.attn.rotary_sin\", \"blocks.3.attn.rotary_cos\", \"blocks.3.mlp.W_in\", \"blocks.3.mlp.b_in\", \"blocks.3.mlp.W_out\", \"blocks.3.mlp.b_out\", \"blocks.4.ln1.w\", \"blocks.4.ln1.b\", \"blocks.4.ln2.w\", \"blocks.4.ln2.b\", \"blocks.4.attn.W_Q\", \"blocks.4.attn.W_O\", \"blocks.4.attn.b_Q\", \"blocks.4.attn.b_O\", \"blocks.4.attn.W_K\", \"blocks.4.attn.W_V\", \"blocks.4.attn.b_K\", \"blocks.4.attn.b_V\", \"blocks.4.attn.mask\", \"blocks.4.attn.IGNORE\", \"blocks.4.attn.rotary_sin\", \"blocks.4.attn.rotary_cos\", \"blocks.4.mlp.W_in\", \"blocks.4.mlp.b_in\", \"blocks.4.mlp.W_out\", \"blocks.4.mlp.b_out\", \"blocks.5.ln1.w\", \"blocks.5.ln1.b\", \"blocks.5.ln2.w\", \"blocks.5.ln2.b\", \"blocks.5.attn.W_Q\", \"blocks.5.attn.W_O\", \"blocks.5.attn.b_Q\", \"blocks.5.attn.b_O\", \"blocks.5.attn.W_K\", \"blocks.5.attn.W_V\", \"blocks.5.attn.b_K\", \"blocks.5.attn.b_V\", \"blocks.5.attn.mask\", \"blocks.5.attn.IGNORE\", \"blocks.5.attn.rotary_sin\", \"blocks.5.attn.rotary_cos\", \"blocks.5.mlp.W_in\", \"blocks.5.mlp.b_in\", \"blocks.5.mlp.W_out\", \"blocks.5.mlp.b_out\", \"blocks.6.ln1.w\", \"blocks.6.ln1.b\", \"blocks.6.ln2.w\", \"blocks.6.ln2.b\", \"blocks.6.attn.W_Q\", \"blocks.6.attn.W_O\", \"blocks.6.attn.b_Q\", \"blocks.6.attn.b_O\", \"blocks.6.attn.W_K\", \"blocks.6.attn.W_V\", \"blocks.6.attn.b_K\", \"blocks.6.attn.b_V\", \"blocks.6.attn.mask\", \"blocks.6.attn.IGNORE\", \"blocks.6.attn.rotary_sin\", \"blocks.6.attn.rotary_cos\", \"blocks.6.mlp.W_in\", \"blocks.6.mlp.b_in\", \"blocks.6.mlp.W_out\", \"blocks.6.mlp.b_out\", \"blocks.7.ln1.w\", \"blocks.7.ln1.b\", \"blocks.7.ln2.w\", \"blocks.7.ln2.b\", \"blocks.7.attn.W_Q\", \"blocks.7.attn.W_O\", \"blocks.7.attn.b_Q\", \"blocks.7.attn.b_O\", \"blocks.7.attn.W_K\", \"blocks.7.attn.W_V\", \"blocks.7.attn.b_K\", \"blocks.7.attn.b_V\", \"blocks.7.attn.mask\", \"blocks.7.attn.IGNORE\", \"blocks.7.attn.rotary_sin\", \"blocks.7.attn.rotary_cos\", \"blocks.7.mlp.W_in\", \"blocks.7.mlp.b_in\", \"blocks.7.mlp.W_out\", \"blocks.7.mlp.b_out\", \"blocks.8.ln1.w\", \"blocks.8.ln1.b\", \"blocks.8.ln2.w\", \"blocks.8.ln2.b\", \"blocks.8.attn.W_Q\", \"blocks.8.attn.W_O\", \"blocks.8.attn.b_Q\", \"blocks.8.attn.b_O\", \"blocks.8.attn.W_K\", \"blocks.8.attn.W_V\", \"blocks.8.attn.b_K\", \"blocks.8.attn.b_V\", \"blocks.8.attn.mask\", \"blocks.8.attn.IGNORE\", \"blocks.8.attn.rotary_sin\", \"blocks.8.attn.rotary_cos\", \"blocks.8.mlp.W_in\", \"blocks.8.mlp.b_in\", \"blocks.8.mlp.W_out\", \"blocks.8.mlp.b_out\", \"blocks.9.ln1.w\", \"blocks.9.ln1.b\", \"blocks.9.ln2.w\", \"blocks.9.ln2.b\", \"blocks.9.attn.W_Q\", \"blocks.9.attn.W_O\", \"blocks.9.attn.b_Q\", \"blocks.9.attn.b_O\", \"blocks.9.attn.W_K\", \"blocks.9.attn.W_V\", \"blocks.9.attn.b_K\", \"blocks.9.attn.b_V\", \"blocks.9.attn.mask\", \"blocks.9.attn.IGNORE\", \"blocks.9.attn.rotary_sin\", \"blocks.9.attn.rotary_cos\", \"blocks.9.mlp.W_in\", \"blocks.9.mlp.b_in\", \"blocks.9.mlp.W_out\", \"blocks.9.mlp.b_out\", \"blocks.10.ln1.w\", \"blocks.10.ln1.b\", \"blocks.10.ln2.w\", \"blocks.10.ln2.b\", \"blocks.10.attn.W_Q\", \"blocks.10.attn.W_O\", \"blocks.10.attn.b_Q\", \"blocks.10.attn.b_O\", \"blocks.10.attn.W_K\", \"blocks.10.attn.W_V\", \"blocks.10.attn.b_K\", \"blocks.10.attn.b_V\", \"blocks.10.attn.mask\", \"blocks.10.attn.IGNORE\", \"blocks.10.attn.rotary_sin\", \"blocks.10.attn.rotary_cos\", \"blocks.10.mlp.W_in\", \"blocks.10.mlp.b_in\", \"blocks.10.mlp.W_out\", \"blocks.10.mlp.b_out\", \"blocks.11.ln1.w\", \"blocks.11.ln1.b\", \"blocks.11.ln2.w\", \"blocks.11.ln2.b\", \"blocks.11.attn.W_Q\", \"blocks.11.attn.W_O\", \"blocks.11.attn.b_Q\", \"blocks.11.attn.b_O\", \"blocks.11.attn.W_K\", \"blocks.11.attn.W_V\", \"blocks.11.attn.b_K\", \"blocks.11.attn.b_V\", \"blocks.11.attn.mask\", \"blocks.11.attn.IGNORE\", \"blocks.11.attn.rotary_sin\", \"blocks.11.attn.rotary_cos\", \"blocks.11.mlp.W_in\", \"blocks.11.mlp.b_in\", \"blocks.11.mlp.W_out\", \"blocks.11.mlp.b_out\", \"blocks.12.ln1.w\", \"blocks.12.ln1.b\", \"blocks.12.ln2.w\", \"blocks.12.ln2.b\", \"blocks.12.attn.W_Q\", \"blocks.12.attn.W_O\", \"blocks.12.attn.b_Q\", \"blocks.12.attn.b_O\", \"blocks.12.attn.W_K\", \"blocks.12.attn.W_V\", \"blocks.12.attn.b_K\", \"blocks.12.attn.b_V\", \"blocks.12.attn.mask\", \"blocks.12.attn.IGNORE\", \"blocks.12.attn.rotary_sin\", \"blocks.12.attn.rotary_cos\", \"blocks.12.mlp.W_in\", \"blocks.12.mlp.b_in\", \"blocks.12.mlp.W_out\", \"blocks.12.mlp.b_out\", \"blocks.13.ln1.w\", \"blocks.13.ln1.b\", \"blocks.13.ln2.w\", \"blocks.13.ln2.b\", \"blocks.13.attn.W_Q\", \"blocks.13.attn.W_O\", \"blocks.13.attn.b_Q\", \"blocks.13.attn.b_O\", \"blocks.13.attn.W_K\", \"blocks.13.attn.W_V\", \"blocks.13.attn.b_K\", \"blocks.13.attn.b_V\", \"blocks.13.attn.mask\", \"blocks.13.attn.IGNORE\", \"blocks.13.attn.rotary_sin\", \"blocks.13.attn.rotary_cos\", \"blocks.13.mlp.W_in\", \"blocks.13.mlp.b_in\", \"blocks.13.mlp.W_out\", \"blocks.13.mlp.b_out\", \"blocks.14.ln1.w\", \"blocks.14.ln1.b\", \"blocks.14.ln2.w\", \"blocks.14.ln2.b\", \"blocks.14.attn.W_Q\", \"blocks.14.attn.W_O\", \"blocks.14.attn.b_Q\", \"blocks.14.attn.b_O\", \"blocks.14.attn.W_K\", \"blocks.14.attn.W_V\", \"blocks.14.attn.b_K\", \"blocks.14.attn.b_V\", \"blocks.14.attn.mask\", \"blocks.14.attn.IGNORE\", \"blocks.14.attn.rotary_sin\", \"blocks.14.attn.rotary_cos\", \"blocks.14.mlp.W_in\", \"blocks.14.mlp.b_in\", \"blocks.14.mlp.W_out\", \"blocks.14.mlp.b_out\", \"blocks.15.ln1.w\", \"blocks.15.ln1.b\", \"blocks.15.ln2.w\", \"blocks.15.ln2.b\", \"blocks.15.attn.W_Q\", \"blocks.15.attn.W_O\", \"blocks.15.attn.b_Q\", \"blocks.15.attn.b_O\", \"blocks.15.attn.W_K\", \"blocks.15.attn.W_V\", \"blocks.15.attn.b_K\", \"blocks.15.attn.b_V\", \"blocks.15.attn.mask\", \"blocks.15.attn.IGNORE\", \"blocks.15.attn.rotary_sin\", \"blocks.15.attn.rotary_cos\", \"blocks.15.mlp.W_in\", \"blocks.15.mlp.b_in\", \"blocks.15.mlp.W_out\", \"blocks.15.mlp.b_out\", \"blocks.16.ln1.w\", \"blocks.16.ln1.b\", \"blocks.16.ln2.w\", \"blocks.16.ln2.b\", \"blocks.16.attn.W_Q\", \"blocks.16.attn.W_O\", \"blocks.16.attn.b_Q\", \"blocks.16.attn.b_O\", \"blocks.16.attn.W_K\", \"blocks.16.attn.W_V\", \"blocks.16.attn.b_K\", \"blocks.16.attn.b_V\", \"blocks.16.attn.mask\", \"blocks.16.attn.IGNORE\", \"blocks.16.attn.rotary_sin\", \"blocks.16.attn.rotary_cos\", \"blocks.16.mlp.W_in\", \"blocks.16.mlp.b_in\", \"blocks.16.mlp.W_out\", \"blocks.16.mlp.b_out\", \"blocks.17.ln1.w\", \"blocks.17.ln1.b\", \"blocks.17.ln2.w\", \"blocks.17.ln2.b\", \"blocks.17.attn.W_Q\", \"blocks.17.attn.W_O\", \"blocks.17.attn.b_Q\", \"blocks.17.attn.b_O\", \"blocks.17.attn.W_K\", \"blocks.17.attn.W_V\", \"blocks.17.attn.b_K\", \"blocks.17.attn.b_V\", \"blocks.17.attn.mask\", \"blocks.17.attn.IGNORE\", \"blocks.17.attn.rotary_sin\", \"blocks.17.attn.rotary_cos\", \"blocks.17.mlp.W_in\", \"blocks.17.mlp.b_in\", \"blocks.17.mlp.W_out\", \"blocks.17.mlp.b_out\", \"blocks.18.ln1.w\", \"blocks.18.ln1.b\", \"blocks.18.ln2.w\", \"blocks.18.ln2.b\", \"blocks.18.attn.W_Q\", \"blocks.18.attn.W_O\", \"blocks.18.attn.b_Q\", \"blocks.18.attn.b_O\", \"blocks.18.attn.W_K\", \"blocks.18.attn.W_V\", \"blocks.18.attn.b_K\", \"blocks.18.attn.b_V\", \"blocks.18.attn.mask\", \"blocks.18.attn.IGNORE\", \"blocks.18.attn.rotary_sin\", \"blocks.18.attn.rotary_cos\", \"blocks.18.mlp.W_in\", \"blocks.18.mlp.b_in\", \"blocks.18.mlp.W_out\", \"blocks.18.mlp.b_out\", \"blocks.19.ln1.w\", \"blocks.19.ln1.b\", \"blocks.19.ln2.w\", \"blocks.19.ln2.b\", \"blocks.19.attn.W_Q\", \"blocks.19.attn.W_O\", \"blocks.19.attn.b_Q\", \"blocks.19.attn.b_O\", \"blocks.19.attn.W_K\", \"blocks.19.attn.W_V\", \"blocks.19.attn.b_K\", \"blocks.19.attn.b_V\", \"blocks.19.attn.mask\", \"blocks.19.attn.IGNORE\", \"blocks.19.attn.rotary_sin\", \"blocks.19.attn.rotary_cos\", \"blocks.19.mlp.W_in\", \"blocks.19.mlp.b_in\", \"blocks.19.mlp.W_out\", \"blocks.19.mlp.b_out\", \"blocks.20.ln1.w\", \"blocks.20.ln1.b\", \"blocks.20.ln2.w\", \"blocks.20.ln2.b\", \"blocks.20.attn.W_Q\", \"blocks.20.attn.W_O\", \"blocks.20.attn.b_Q\", \"blocks.20.attn.b_O\", \"blocks.20.attn.W_K\", \"blocks.20.attn.W_V\", \"blocks.20.attn.b_K\", \"blocks.20.attn.b_V\", \"blocks.20.attn.mask\", \"blocks.20.attn.IGNORE\", \"blocks.20.attn.rotary_sin\", \"blocks.20.attn.rotary_cos\", \"blocks.20.mlp.W_in\", \"blocks.20.mlp.b_in\", \"blocks.20.mlp.W_out\", \"blocks.20.mlp.b_out\", \"blocks.21.ln1.w\", \"blocks.21.ln1.b\", \"blocks.21.ln2.w\", \"blocks.21.ln2.b\", \"blocks.21.attn.W_Q\", \"blocks.21.attn.W_O\", \"blocks.21.attn.b_Q\", \"blocks.21.attn.b_O\", \"blocks.21.attn.W_K\", \"blocks.21.attn.W_V\", \"blocks.21.attn.b_K\", \"blocks.21.attn.b_V\", \"blocks.21.attn.mask\", \"blocks.21.attn.IGNORE\", \"blocks.21.attn.rotary_sin\", \"blocks.21.attn.rotary_cos\", \"blocks.21.mlp.W_in\", \"blocks.21.mlp.b_in\", \"blocks.21.mlp.W_out\", \"blocks.21.mlp.b_out\", \"blocks.22.ln1.w\", \"blocks.22.ln1.b\", \"blocks.22.ln2.w\", \"blocks.22.ln2.b\", \"blocks.22.attn.W_Q\", \"blocks.22.attn.W_O\", \"blocks.22.attn.b_Q\", \"blocks.22.attn.b_O\", \"blocks.22.attn.W_K\", \"blocks.22.attn.W_V\", \"blocks.22.attn.b_K\", \"blocks.22.attn.b_V\", \"blocks.22.attn.mask\", \"blocks.22.attn.IGNORE\", \"blocks.22.attn.rotary_sin\", \"blocks.22.attn.rotary_cos\", \"blocks.22.mlp.W_in\", \"blocks.22.mlp.b_in\", \"blocks.22.mlp.W_out\", \"blocks.22.mlp.b_out\", \"blocks.23.ln1.w\", \"blocks.23.ln1.b\", \"blocks.23.ln2.w\", \"blocks.23.ln2.b\", \"blocks.23.attn.W_Q\", \"blocks.23.attn.W_O\", \"blocks.23.attn.b_Q\", \"blocks.23.attn.b_O\", \"blocks.23.attn.W_K\", \"blocks.23.attn.W_V\", \"blocks.23.attn.b_K\", \"blocks.23.attn.b_V\", \"blocks.23.attn.mask\", \"blocks.23.attn.IGNORE\", \"blocks.23.attn.rotary_sin\", \"blocks.23.attn.rotary_cos\", \"blocks.23.mlp.W_in\", \"blocks.23.mlp.b_in\", \"blocks.23.mlp.W_out\", \"blocks.23.mlp.b_out\", \"blocks.24.ln1.w\", \"blocks.24.ln1.b\", \"blocks.24.ln2.w\", \"blocks.24.ln2.b\", \"blocks.24.attn.W_Q\", \"blocks.24.attn.W_O\", \"blocks.24.attn.b_Q\", \"blocks.24.attn.b_O\", \"blocks.24.attn.W_K\", \"blocks.24.attn.W_V\", \"blocks.24.attn.b_K\", \"blocks.24.attn.b_V\", \"blocks.24.attn.mask\", \"blocks.24.attn.IGNORE\", \"blocks.24.attn.rotary_sin\", \"blocks.24.attn.rotary_cos\", \"blocks.24.mlp.W_in\", \"blocks.24.mlp.b_in\", \"blocks.24.mlp.W_out\", \"blocks.24.mlp.b_out\", \"blocks.25.ln1.w\", \"blocks.25.ln1.b\", \"blocks.25.ln2.w\", \"blocks.25.ln2.b\", \"blocks.25.attn.W_Q\", \"blocks.25.attn.W_O\", \"blocks.25.attn.b_Q\", \"blocks.25.attn.b_O\", \"blocks.25.attn.W_K\", \"blocks.25.attn.W_V\", \"blocks.25.attn.b_K\", \"blocks.25.attn.b_V\", \"blocks.25.attn.mask\", \"blocks.25.attn.IGNORE\", \"blocks.25.attn.rotary_sin\", \"blocks.25.attn.rotary_cos\", \"blocks.25.mlp.W_in\", \"blocks.25.mlp.b_in\", \"blocks.25.mlp.W_out\", \"blocks.25.mlp.b_out\", \"blocks.26.ln1.w\", \"blocks.26.ln1.b\", \"blocks.26.ln2.w\", \"blocks.26.ln2.b\", \"blocks.26.attn.W_Q\", \"blocks.26.attn.W_O\", \"blocks.26.attn.b_Q\", \"blocks.26.attn.b_O\", \"blocks.26.attn.W_K\", \"blocks.26.attn.W_V\", \"blocks.26.attn.b_K\", \"blocks.26.attn.b_V\", \"blocks.26.attn.mask\", \"blocks.26.attn.IGNORE\", \"blocks.26.attn.rotary_sin\", \"blocks.26.attn.rotary_cos\", \"blocks.26.mlp.W_in\", \"blocks.26.mlp.b_in\", \"blocks.26.mlp.W_out\", \"blocks.26.mlp.b_out\", \"blocks.27.ln1.w\", \"blocks.27.ln1.b\", \"blocks.27.ln2.w\", \"blocks.27.ln2.b\", \"blocks.27.attn.W_Q\", \"blocks.27.attn.W_O\", \"blocks.27.attn.b_Q\", \"blocks.27.attn.b_O\", \"blocks.27.attn.W_K\", \"blocks.27.attn.W_V\", \"blocks.27.attn.b_K\", \"blocks.27.attn.b_V\", \"blocks.27.attn.mask\", \"blocks.27.attn.IGNORE\", \"blocks.27.attn.rotary_sin\", \"blocks.27.attn.rotary_cos\", \"blocks.27.mlp.W_in\", \"blocks.27.mlp.b_in\", \"blocks.27.mlp.W_out\", \"blocks.27.mlp.b_out\", \"ln_final.w\", \"ln_final.b\", \"unembed.W_U\", \"unembed.b_U\". \n\tUnexpected key(s) in state_dict: \"lm_head.bias\", \"lm_head.weight\", \"transformer.h.0.attn.k_proj.weight\", \"transformer.h.0.attn.out_proj.weight\", \"transformer.h.0.attn.q_proj.weight\", \"transformer.h.0.attn.v_proj.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.mlp.fc_in.bias\", \"transformer.h.0.mlp.fc_in.weight\", \"transformer.h.0.mlp.fc_out.bias\", \"transformer.h.0.mlp.fc_out.weight\", \"transformer.h.1.attn.k_proj.weight\", \"transformer.h.1.attn.out_proj.weight\", \"transformer.h.1.attn.q_proj.weight\", \"transformer.h.1.attn.v_proj.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.mlp.fc_in.bias\", \"transformer.h.1.mlp.fc_in.weight\", \"transformer.h.1.mlp.fc_out.bias\", \"transformer.h.1.mlp.fc_out.weight\", \"transformer.h.10.attn.k_proj.weight\", \"transformer.h.10.attn.out_proj.weight\", \"transformer.h.10.attn.q_proj.weight\", \"transformer.h.10.attn.v_proj.weight\", \"transformer.h.10.ln_1.bias\", \"transformer.h.10.ln_1.weight\", \"transformer.h.10.mlp.fc_in.bias\", \"transformer.h.10.mlp.fc_in.weight\", \"transformer.h.10.mlp.fc_out.bias\", \"transformer.h.10.mlp.fc_out.weight\", \"transformer.h.11.attn.k_proj.weight\", \"transformer.h.11.attn.out_proj.weight\", \"transformer.h.11.attn.q_proj.weight\", \"transformer.h.11.attn.v_proj.weight\", \"transformer.h.11.ln_1.bias\", \"transformer.h.11.ln_1.weight\", \"transformer.h.11.mlp.fc_in.bias\", \"transformer.h.11.mlp.fc_in.weight\", \"transformer.h.11.mlp.fc_out.bias\", \"transformer.h.11.mlp.fc_out.weight\", \"transformer.h.2.attn.k_proj.weight\", \"transformer.h.2.attn.out_proj.weight\", \"transformer.h.2.attn.q_proj.weight\", \"transformer.h.2.attn.v_proj.weight\", \"transformer.h.2.ln_1.bias\", \"transformer.h.2.ln_1.weight\", \"transformer.h.2.mlp.fc_in.bias\", \"transformer.h.2.mlp.fc_in.weight\", \"transformer.h.2.mlp.fc_out.bias\", \"transformer.h.2.mlp.fc_out.weight\", \"transformer.h.3.attn.k_proj.weight\", \"transformer.h.3.attn.out_proj.weight\", \"transformer.h.3.attn.q_proj.weight\", \"transformer.h.3.attn.v_proj.weight\", \"transformer.h.3.ln_1.bias\", \"transformer.h.3.ln_1.weight\", \"transformer.h.3.mlp.fc_in.bias\", \"transformer.h.3.mlp.fc_in.weight\", \"transformer.h.3.mlp.fc_out.bias\", \"transformer.h.3.mlp.fc_out.weight\", \"transformer.h.4.attn.k_proj.weight\", \"transformer.h.4.attn.out_proj.weight\", \"transformer.h.4.attn.q_proj.weight\", \"transformer.h.4.attn.v_proj.weight\", \"transformer.h.4.ln_1.bias\", \"transformer.h.4.ln_1.weight\", \"transformer.h.4.mlp.fc_in.bias\", \"transformer.h.4.mlp.fc_in.weight\", \"transformer.h.4.mlp.fc_out.bias\", \"transformer.h.4.mlp.fc_out.weight\", \"transformer.h.5.attn.k_proj.weight\", \"transformer.h.5.attn.out_proj.weight\", \"transformer.h.5.attn.q_proj.weight\", \"transformer.h.5.attn.v_proj.weight\", \"transformer.h.5.ln_1.bias\", \"transformer.h.5.ln_1.weight\", \"transformer.h.5.mlp.fc_in.bias\", \"transformer.h.5.mlp.fc_in.weight\", \"transformer.h.5.mlp.fc_out.bias\", \"transformer.h.5.mlp.fc_out.weight\", \"transformer.h.6.attn.k_proj.weight\", \"transformer.h.6.attn.out_proj.weight\", \"transformer.h.6.attn.q_proj.weight\", \"transformer.h.6.attn.v_proj.weight\", \"transformer.h.6.ln_1.bias\", \"transformer.h.6.ln_1.weight\", \"transformer.h.6.mlp.fc_in.bias\", \"transformer.h.6.mlp.fc_in.weight\", \"transformer.h.6.mlp.fc_out.bias\", \"transformer.h.6.mlp.fc_out.weight\", \"transformer.h.7.attn.k_proj.weight\", \"transformer.h.7.attn.out_proj.weight\", \"transformer.h.7.attn.q_proj.weight\", \"transformer.h.7.attn.v_proj.weight\", \"transformer.h.7.ln_1.bias\", \"transformer.h.7.ln_1.weight\", \"transformer.h.7.mlp.fc_in.bias\", \"transformer.h.7.mlp.fc_in.weight\", \"transformer.h.7.mlp.fc_out.bias\", \"transformer.h.7.mlp.fc_out.weight\", \"transformer.h.8.attn.k_proj.weight\", \"transformer.h.8.attn.out_proj.weight\", \"transformer.h.8.attn.q_proj.weight\", \"transformer.h.8.attn.v_proj.weight\", \"transformer.h.8.ln_1.bias\", \"transformer.h.8.ln_1.weight\", \"transformer.h.8.mlp.fc_in.bias\", \"transformer.h.8.mlp.fc_in.weight\", \"transformer.h.8.mlp.fc_out.bias\", \"transformer.h.8.mlp.fc_out.weight\", \"transformer.h.9.attn.k_proj.weight\", \"transformer.h.9.attn.out_proj.weight\", \"transformer.h.9.attn.q_proj.weight\", \"transformer.h.9.attn.v_proj.weight\", \"transformer.h.9.ln_1.bias\", \"transformer.h.9.ln_1.weight\", \"transformer.h.9.mlp.fc_in.bias\", \"transformer.h.9.mlp.fc_in.weight\", \"transformer.h.9.mlp.fc_out.bias\", \"transformer.h.9.mlp.fc_out.weight\", \"transformer.ln_f.bias\", \"transformer.ln_f.weight\", \"transformer.wte.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HookedTransformer:\n\tMissing key(s) in state_dict: \"embed.W_E\", \"blocks.0.ln1.w\", \"blocks.0.ln1.b\", \"blocks.0.ln2.w\", \"blocks.0.ln2.b\", \"blocks.0.attn.W_Q\", \"blocks.0.attn.W_O\", \"blocks.0.attn.b_Q\", \"blocks.0.attn.b_O\", \"blocks.0.attn.W_K\", \"blocks.0.attn.W_V\", \"blocks.0.attn.b_K\", \"blocks.0.attn.b_V\", \"blocks.0.attn.mask\", \"blocks.0.attn.IGNORE\", \"blocks.0.attn.rotary_sin\", \"blocks.0.attn.rotary_cos\", \"blocks.0.mlp.W_in\", \"blocks.0.mlp.b_in\", \"blocks.0.mlp.W_out\", \"blocks.0.mlp.b_out\", \"blocks.1.ln1.w\", \"blocks.1.ln1.b\", \"blocks.1.ln2.w\", \"blocks.1.ln2.b\", \"blocks.1.attn.W_Q\", \"blocks.1.attn.W_O\", \"blocks.1.attn.b_Q\", \"blocks.1.attn.b_O\", \"blocks.1.attn.W_K\", \"blocks.1.attn.W_V\", \"blocks.1.attn.b_K\", \"blocks.1.attn.b_V\", \"blocks.1.attn.mask\", \"blocks.1.attn.IGNORE\", \"blocks.1.attn.rotary_sin\", \"blocks.1.attn.rotary_cos\", \"blocks.1.mlp.W_in\", \"blocks.1.mlp.b_in\", \"blocks.1.mlp.W_out\", \"blocks.1.mlp.b_out\", \"blocks.2.ln1.w\", \"blocks.2.ln1.b\", \"blocks.2.ln2.w\", \"blocks.2.ln2.b\", \"blocks.2.attn.W_Q\", \"blocks.2.attn.W_O\", \"blocks.2.attn.b_Q\", \"blocks.2.attn.b_O\", \"blocks.2.attn.W_K\", \"blocks.2.attn.W_V\", \"blocks.2.attn.b_K\", \"blocks.2.attn.b_V\", \"blocks.2.attn.mask\", \"blocks.2.attn.IGNORE\", \"blocks.2.attn.rotary_sin\", \"blocks.2.attn.rotary_cos\", \"blocks.2.mlp.W_in\", \"blocks.2.mlp.b_in\", \"blocks.2.mlp.W_out\", \"blocks.2.mlp.b_out\", \"blocks.3.ln1.w\", \"blocks.3.ln1.b\", \"blocks.3.ln2.w\", \"blocks.3.ln2.b\", \"blocks.3.attn.W_Q\", \"blocks.3.attn.W_O\", \"blocks.3.attn.b_Q\", \"blocks.3.attn.b_O\", \"blocks.3.attn.W_K\", \"blocks.3.attn.W_V\", \"blocks.3.attn.b_K\", \"blocks.3.attn.b_V\", \"blocks.3.attn.mask\", \"blocks.3.attn.IGNORE\", \"blocks.3.attn.rotary_sin\", \"blocks.3.attn.rotary_cos\", \"blocks.3.mlp.W_in\", \"blocks.3.mlp.b_in\", \"blocks.3.mlp.W_out\", \"blocks.3.mlp.b_out\", \"blocks.4.ln1.w\", \"blocks.4.ln1.b\", \"blocks.4.ln2.w\", \"blocks.4.ln2.b\", \"blocks.4.attn.W_Q\", \"blocks.4.attn.W_O\", \"blocks.4.attn.b_Q\", \"blocks.4.attn.b_O\", \"blocks.4.attn.W_K\", \"blocks.4.attn.W_V\", \"blocks.4.attn.b_K\", \"blocks.4.attn.b_V\", \"blocks.4.attn.mask\", \"blocks.4.attn.IGNORE\", \"blocks.4.attn.rotary_sin\", \"blocks.4.attn.rotary_cos\", \"blocks.4.mlp.W_in\", \"blocks.4.mlp.b_in\", \"blocks.4.mlp.W_out\", \"blocks.4.mlp.b_out\", \"blocks.5.ln1.w\", \"blocks.5.ln1.b\", \"blocks.5.ln2.w\", \"blocks.5.ln2.b\", \"blocks.5.attn.W_Q\", \"blocks.5.attn.W_O\", \"blocks.5.attn.b_Q\", \"blocks.5.attn.b_O\", \"blocks.5.attn.W_K\", \"blocks.5.attn.W_V\", \"blocks.5.attn.b_K\", \"blocks.5.attn.b_V\", \"blocks.5.attn.mask\", \"blocks.5.attn.IGNORE\", \"blocks.5.attn.rotary_sin\", \"blocks.5.attn.rotary_cos\", \"blocks.5.mlp.W_in\", \"blocks.5.mlp.b_in\", \"blocks.5.mlp.W_out\", \"blocks.5.mlp.b_out\", \"blocks.6.ln1.w\", \"blocks.6.ln1.b\", \"blocks.6.ln2.w\", \"blocks.6.ln2.b\", \"blocks.6.attn.W_Q\", \"blocks.6.attn.W_O\", \"blocks.6.attn.b_Q\", \"blocks.6.attn.b_O\", \"blocks.6.attn.W_K\", \"blocks.6.attn.W_V\", \"blocks.6.attn.b_K\", \"blocks.6.attn.b_V\", \"blocks.6.attn.mask\", \"blocks.6.attn.IGNORE\", \"blocks.6.attn.rotary_sin\", \"blocks.6.attn.rotary_cos\", \"blocks.6.mlp.W_in\", \"blocks.6.mlp.b_in\", \"blocks.6.mlp.W_out\", \"blocks.6.mlp.b_out\", \"blocks.7.ln1.w\", \"blocks.7.ln1.b\", \"blocks.7.ln2.w\", \"blocks.7.ln2.b\", \"blocks.7.attn.W_Q\", \"blocks.7.attn.W_O\", \"blocks.7.attn.b_Q\", \"blocks.7.attn.b_O\", \"blocks.7.attn.W_K\", \"blocks.7.attn.W_V\", \"blocks.7.attn.b_K\", \"blocks.7.attn.b_V\", \"blocks.7.attn.mask\", \"blocks.7.attn.IGNORE\", \"blocks.7.attn.rotary_sin\", \"blocks.7.attn.rotary_cos\", \"blocks.7.mlp.W_in\", \"blocks.7.mlp.b_in\", \"blocks.7.mlp.W_out\", \"blocks.7.mlp.b_out\", \"blocks.8.ln1.w\", \"blocks.8.ln1.b\", \"blocks.8.ln2.w\", \"blocks.8.ln2.b\", \"blocks.8.attn.W_Q\", \"blocks.8.attn.W_O\", \"blocks.8.attn.b_Q\", \"blocks.8.attn.b_O\", \"blocks.8.attn.W_K\", \"blocks.8.attn.W_V\", \"blocks.8.attn.b_K\", \"blocks.8.attn.b_V\", \"blocks.8.attn.mask\", \"blocks.8.attn.IGNORE\", \"blocks.8.attn.rotary_sin\", \"blocks.8.attn.rotary_cos\", \"blocks.8.mlp.W_in\", \"blocks.8.mlp.b_in\", \"blocks.8.mlp.W_out\", \"blocks.8.mlp.b_out\", \"blocks.9.ln1.w\", \"blocks.9.ln1.b\", \"blocks.9.ln2.w\", \"blocks.9.ln2.b\", \"blocks.9.attn.W_Q\", \"blocks.9.attn.W_O\", \"blocks.9.attn.b_Q\", \"blocks.9.attn.b_O\", \"blocks.9.attn.W_K\", \"blocks.9.attn.W_V\", \"blocks.9.attn.b_K\", \"blocks.9.attn.b_V\", \"blocks.9.attn.mask\", \"blocks.9.attn.IGNORE\", \"blocks.9.attn.rotary_sin\", \"blocks.9.attn.rotary_cos\", \"blocks.9.mlp.W_in\", \"blocks.9.mlp.b_in\", \"blocks.9.mlp.W_out\", \"blocks.9.mlp.b_out\", \"blocks.10.ln1.w\", \"blocks.10.ln1.b\", \"blocks.10.ln2.w\", \"blocks.10.ln2.b\", \"blocks.10.attn.W_Q\", \"blocks.10.attn.W_O\", \"blocks.10.attn.b_Q\", \"blocks.10.attn.b_O\", \"blocks.10.attn.W_K\", \"blocks.10.attn.W_V\", \"blocks.10.attn.b_K\", \"blocks.10.attn.b_V\", \"blocks.10.attn.mask\", \"blocks.10.attn.IGNORE\", \"blocks.10.attn.rotary_sin\", \"blocks.10.attn.rotary_cos\", \"blocks.10.mlp.W_in\", \"blocks.10.mlp.b_in\", \"blocks.10.mlp.W_out\", \"blocks.10.mlp.b_out\", \"blocks.11.ln1.w\", \"blocks.11.ln1.b\", \"blocks.11.ln2.w\", \"blocks.11.ln2.b\", \"blocks.11.attn.W_Q\", \"blocks.11.attn.W_O\", \"blocks.11.attn.b_Q\", \"blocks.11.attn.b_O\", \"blocks.11.attn.W_K\", \"blocks.11.attn.W_V\", \"blocks.11.attn.b_K\", \"blocks.11.attn.b_V\", \"blocks.11.attn.mask\", \"blocks.11.attn.IGNORE\", \"blocks.11.attn.rotary_sin\", \"blocks.11.attn.rotary_cos\", \"blocks.11.mlp.W_in\", \"blocks.11.mlp.b_in\", \"blocks.11.mlp.W_out\", \"blocks.11.mlp.b_out\", \"blocks.12.ln1.w\", \"blocks.12.ln1.b\", \"blocks.12.ln2.w\", \"blocks.12.ln2.b\", \"blocks.12.attn.W_Q\", \"blocks.12.attn.W_O\", \"blocks.12.attn.b_Q\", \"blocks.12.attn.b_O\", \"blocks.12.attn.W_K\", \"blocks.12.attn.W_V\", \"blocks.12.attn.b_K\", \"blocks.12.attn.b_V\", \"blocks.12.attn.mask\", \"blocks.12.attn.IGNORE\", \"blocks.12.attn.rotary_sin\", \"blocks.12.attn.rotary_cos\", \"blocks.12.mlp.W_in\", \"blocks.12.mlp.b_in\", \"blocks.12.mlp.W_out\", \"blocks.12.mlp.b_out\", \"blocks.13.ln1.w\", \"blocks.13.ln1.b\", \"blocks.13.ln2.w\", \"blocks.13.ln2.b\", \"blocks.13.attn.W_Q\", \"blocks.13.attn.W_O\", \"blocks.13.attn.b_Q\", \"blocks.13.attn.b_O\", \"blocks.13.attn.W_K\", \"blocks.13.attn.W_V\", \"blocks.13.attn.b_K\", \"blocks.13.attn.b_V\", \"blocks.13.attn.mask\", \"blocks.13.attn.IGNORE\", \"blocks.13.attn.rotary_sin\", \"blocks.13.attn.rotary_cos\", \"blocks.13.mlp.W_in\", \"blocks.13.mlp.b_in\", \"blocks.13.mlp.W_out\", \"blocks.13.mlp.b_out\", \"blocks.14.ln1.w\", \"blocks.14.ln1.b\", \"blocks.14.ln2.w\", \"blocks.14.ln2.b\", \"blocks.14.attn.W_Q\", \"blocks.14.attn.W_O\", \"blocks.14.attn.b_Q\", \"blocks.14.attn.b_O\", \"blocks.14.attn.W_K\", \"blocks.14.attn.W_V\", \"blocks.14.attn.b_K\", \"blocks.14.attn.b_V\", \"blocks.14.attn.mask\", \"blocks.14.attn.IGNORE\", \"blocks.14.attn.rotary_sin\", \"blocks.14.attn.rotary_cos\", \"blocks.14.mlp.W_in\", \"blocks.14.mlp.b_in\", \"blocks.14.mlp.W_out\", \"blocks.14.mlp.b_out\", \"blocks.15.ln1.w\", \"blocks.15.ln1.b\", \"blocks.15.ln2.w\", \"blocks.15.ln2.b\", \"blocks.15.attn.W_Q\", \"blocks.15.attn.W_O\", \"blocks.15.attn.b_Q\", \"blocks.15.attn.b_O\", \"blocks.15.attn.W_K\", \"blocks.15.attn.W_V\", \"blocks.15.attn.b_K\", \"blocks.15.attn.b_V\", \"blocks.15.attn.mask\", \"blocks.15.attn.IGNORE\", \"blocks.15.attn.rotary_sin\", \"blocks.15.attn.rotary_cos\", \"blocks.15.mlp.W_in\", \"blocks.15.mlp.b_in\", \"blocks.15.mlp.W_out\", \"blocks.15.mlp.b_out\", \"blocks.16.ln1.w\", \"blocks.16.ln1.b\", \"blocks.16.ln2.w\", \"blocks.16.ln2.b\", \"blocks.16.attn.W_Q\", \"blocks.16.attn.W_O\", \"blocks.16.attn.b_Q\", \"blocks.16.attn.b_O\", \"blocks.16.attn.W_K\", \"blocks.16.attn.W_V\", \"blocks.16.attn.b_K\", \"blocks.16.attn.b_V\", \"blocks.16.attn.mask\", \"blocks.16.attn.IGNORE\", \"blocks.16.attn.rotary_sin\", \"blocks.16.attn.rotary_cos\", \"blocks.16.mlp.W_in\", \"blocks.16.mlp.b_in\", \"blocks.16.mlp.W_out\", \"blocks.16.mlp.b_out\", \"blocks.17.ln1.w\", \"blocks.17.ln1.b\", \"blocks.17.ln2.w\", \"blocks.17.ln2.b\", \"blocks.17.attn.W_Q\", \"blocks.17.attn.W_O\", \"blocks.17.attn.b_Q\", \"blocks.17.attn.b_O\", \"blocks.17.attn.W_K\", \"blocks.17.attn.W_V\", \"blocks.17.attn.b_K\", \"blocks.17.attn.b_V\", \"blocks.17.attn.mask\", \"blocks.17.attn.IGNORE\", \"blocks.17.attn.rotary_sin\", \"blocks.17.attn.rotary_cos\", \"blocks.17.mlp.W_in\", \"blocks.17.mlp.b_in\", \"blocks.17.mlp.W_out\", \"blocks.17.mlp.b_out\", \"blocks.18.ln1.w\", \"blocks.18.ln1.b\", \"blocks.18.ln2.w\", \"blocks.18.ln2.b\", \"blocks.18.attn.W_Q\", \"blocks.18.attn.W_O\", \"blocks.18.attn.b_Q\", \"blocks.18.attn.b_O\", \"blocks.18.attn.W_K\", \"blocks.18.attn.W_V\", \"blocks.18.attn.b_K\", \"blocks.18.attn.b_V\", \"blocks.18.attn.mask\", \"blocks.18.attn.IGNORE\", \"blocks.18.attn.rotary_sin\", \"blocks.18.attn.rotary_cos\", \"blocks.18.mlp.W_in\", \"blocks.18.mlp.b_in\", \"blocks.18.mlp.W_out\", \"blocks.18.mlp.b_out\", \"blocks.19.ln1.w\", \"blocks.19.ln1.b\", \"blocks.19.ln2.w\", \"blocks.19.ln2.b\", \"blocks.19.attn.W_Q\", \"blocks.19.attn.W_O\", \"blocks.19.attn.b_Q\", \"blocks.19.attn.b_O\", \"blocks.19.attn.W_K\", \"blocks.19.attn.W_V\", \"blocks.19.attn.b_K\", \"blocks.19.attn.b_V\", \"blocks.19.attn.mask\", \"blocks.19.attn.IGNORE\", \"blocks.19.attn.rotary_sin\", \"blocks.19.attn.rotary_cos\", \"blocks.19.mlp.W_in\", \"blocks.19.mlp.b_in\", \"blocks.19.mlp.W_out\", \"blocks.19.mlp.b_out\", \"blocks.20.ln1.w\", \"blocks.20.ln1.b\", \"blocks.20.ln2.w\", \"blocks.20.ln2.b\", \"blocks.20.attn.W_Q\", \"blocks.20.attn.W_O\", \"blocks.20.attn.b_Q\", \"blocks.20.attn.b_O\", \"blocks.20.attn.W_K\", \"blocks.20.attn.W_V\", \"blocks.20.attn.b_K\", \"blocks.20.attn.b_V\", \"blocks.20.attn.mask\", \"blocks.20.attn.IGNORE\", \"blocks.20.attn.rotary_sin\", \"blocks.20.attn.rotary_cos\", \"blocks.20.mlp.W_in\", \"blocks.20.mlp.b_in\", \"blocks.20.mlp.W_out\", \"blocks.20.mlp.b_out\", \"blocks.21.ln1.w\", \"blocks.21.ln1.b\", \"blocks.21.ln2.w\", \"blocks.21.ln2.b\", \"blocks.21.attn.W_Q\", \"blocks.21.attn.W_O\", \"blocks.21.attn.b_Q\", \"blocks.21.attn.b_O\", \"blocks.21.attn.W_K\", \"blocks.21.attn.W_V\", \"blocks.21.attn.b_K\", \"blocks.21.attn.b_V\", \"blocks.21.attn.mask\", \"blocks.21.attn.IGNORE\", \"blocks.21.attn.rotary_sin\", \"blocks.21.attn.rotary_cos\", \"blocks.21.mlp.W_in\", \"blocks.21.mlp.b_in\", \"blocks.21.mlp.W_out\", \"blocks.21.mlp.b_out\", \"blocks.22.ln1.w\", \"blocks.22.ln1.b\", \"blocks.22.ln2.w\", \"blocks.22.ln2.b\", \"blocks.22.attn.W_Q\", \"blocks.22.attn.W_O\", \"blocks.22.attn.b_Q\", \"blocks.22.attn.b_O\", \"blocks.22.attn.W_K\", \"blocks.22.attn.W_V\", \"blocks.22.attn.b_K\", \"blocks.22.attn.b_V\", \"blocks.22.attn.mask\", \"blocks.22.attn.IGNORE\", \"blocks.22.attn.rotary_sin\", \"blocks.22.attn.rotary_cos\", \"blocks.22.mlp.W_in\", \"blocks.22.mlp.b_in\", \"blocks.22.mlp.W_out\", \"blocks.22.mlp.b_out\", \"blocks.23.ln1.w\", \"blocks.23.ln1.b\", \"blocks.23.ln2.w\", \"blocks.23.ln2.b\", \"blocks.23.attn.W_Q\", \"blocks.23.attn.W_O\", \"blocks.23.attn.b_Q\", \"blocks.23.attn.b_O\", \"blocks.23.attn.W_K\", \"blocks.23.attn.W_V\", \"blocks.23.attn.b_K\", \"blocks.23.attn.b_V\", \"blocks.23.attn.mask\", \"blocks.23.attn.IGNORE\", \"blocks.23.attn.rotary_sin\", \"blocks.23.attn.rotary_cos\", \"blocks.23.mlp.W_in\", \"blocks.23.mlp.b_in\", \"blocks.23.mlp.W_out\", \"blocks.23.mlp.b_out\", \"blocks.24.ln1.w\", \"blocks.24.ln1.b\", \"blocks.24.ln2.w\", \"blocks.24.ln2.b\", \"blocks.24.attn.W_Q\", \"blocks.24.attn.W_O\", \"blocks.24.attn.b_Q\", \"blocks.24.attn.b_O\", \"blocks.24.attn.W_K\", \"blocks.24.attn.W_V\", \"blocks.24.attn.b_K\", \"blocks.24.attn.b_V\", \"blocks.24.attn.mask\", \"blocks.24.attn.IGNORE\", \"blocks.24.attn.rotary_sin\", \"blocks.24.attn.rotary_cos\", \"blocks.24.mlp.W_in\", \"blocks.24.mlp.b_in\", \"blocks.24.mlp.W_out\", \"blocks.24.mlp.b_out\", \"blocks.25.ln1.w\", \"blocks.25.ln1.b\", \"blocks.25.ln2.w\", \"blocks.25.ln2.b\", \"blocks.25.attn.W_Q\", \"blocks.25.attn.W_O\", \"blocks.25.attn.b_Q\", \"blocks.25.attn.b_O\", \"blocks.25.attn.W_K\", \"blocks.25.attn.W_V\", \"blocks.25.attn.b_K\", \"blocks.25.attn.b_V\", \"blocks.25.attn.mask\", \"blocks.25.attn.IGNORE\", \"blocks.25.attn.rotary_sin\", \"blocks.25.attn.rotary_cos\", \"blocks.25.mlp.W_in\", \"blocks.25.mlp.b_in\", \"blocks.25.mlp.W_out\", \"blocks.25.mlp.b_out\", \"blocks.26.ln1.w\", \"blocks.26.ln1.b\", \"blocks.26.ln2.w\", \"blocks.26.ln2.b\", \"blocks.26.attn.W_Q\", \"blocks.26.attn.W_O\", \"blocks.26.attn.b_Q\", \"blocks.26.attn.b_O\", \"blocks.26.attn.W_K\", \"blocks.26.attn.W_V\", \"blocks.26.attn.b_K\", \"blocks.26.attn.b_V\", \"blocks.26.attn.mask\", \"blocks.26.attn.IGNORE\", \"blocks.26.attn.rotary_sin\", \"blocks.26.attn.rotary_cos\", \"blocks.26.mlp.W_in\", \"blocks.26.mlp.b_in\", \"blocks.26.mlp.W_out\", \"blocks.26.mlp.b_out\", \"blocks.27.ln1.w\", \"blocks.27.ln1.b\", \"blocks.27.ln2.w\", \"blocks.27.ln2.b\", \"blocks.27.attn.W_Q\", \"blocks.27.attn.W_O\", \"blocks.27.attn.b_Q\", \"blocks.27.attn.b_O\", \"blocks.27.attn.W_K\", \"blocks.27.attn.W_V\", \"blocks.27.attn.b_K\", \"blocks.27.attn.b_V\", \"blocks.27.attn.mask\", \"blocks.27.attn.IGNORE\", \"blocks.27.attn.rotary_sin\", \"blocks.27.attn.rotary_cos\", \"blocks.27.mlp.W_in\", \"blocks.27.mlp.b_in\", \"blocks.27.mlp.W_out\", \"blocks.27.mlp.b_out\", \"ln_final.w\", \"ln_final.b\", \"unembed.W_U\", \"unembed.b_U\". \n\tUnexpected key(s) in state_dict: \"lm_head.bias\", \"lm_head.weight\", \"transformer.h.0.attn.k_proj.weight\", \"transformer.h.0.attn.out_proj.weight\", \"transformer.h.0.attn.q_proj.weight\", \"transformer.h.0.attn.v_proj.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.mlp.fc_in.bias\", \"transformer.h.0.mlp.fc_in.weight\", \"transformer.h.0.mlp.fc_out.bias\", \"transformer.h.0.mlp.fc_out.weight\", \"transformer.h.1.attn.k_proj.weight\", \"transformer.h.1.attn.out_proj.weight\", \"transformer.h.1.attn.q_proj.weight\", \"transformer.h.1.attn.v_proj.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.mlp.fc_in.bias\", \"transformer.h.1.mlp.fc_in.weight\", \"transformer.h.1.mlp.fc_out.bias\", \"transformer.h.1.mlp.fc_out.weight\", \"transformer.h.10.attn.k_proj.weight\", \"transformer.h.10.attn.out_proj.weight\", \"transformer.h.10.attn.q_proj.weight\", \"transformer.h.10.attn.v_proj.weight\", \"transformer.h.10.ln_1.bias\", \"transformer.h.10.ln_1.weight\", \"transformer.h.10.mlp.fc_in.bias\", \"transformer.h.10.mlp.fc_in.weight\", \"transformer.h.10.mlp.fc_out.bias\", \"transformer.h.10.mlp.fc_out.weight\", \"transformer.h.11.attn.k_proj.weight\", \"transformer.h.11.attn.out_proj.weight\", \"transformer.h.11.attn.q_proj.weight\", \"transformer.h.11.attn.v_proj.weight\", \"transformer.h.11.ln_1.bias\", \"transformer.h.11.ln_1.weight\", \"transformer.h.11.mlp.fc_in.bias\", \"transformer.h.11.mlp.fc_in.weight\", \"transformer.h.11.mlp.fc_out.bias\", \"transformer.h.11.mlp.fc_out.weight\", \"transformer.h.2.attn.k_proj.weight\", \"transformer.h.2.attn.out_proj.weight\", \"transformer.h.2.attn.q_proj.weight\", \"transformer.h.2.attn.v_proj.weight\", \"transformer.h.2.ln_1.bias\", \"transformer.h.2.ln_1.weight\", \"transformer.h.2.mlp.fc_in.bias\", \"transformer.h.2.mlp.fc_in.weight\", \"transformer.h.2.mlp.fc_out.bias\", \"transformer.h.2.mlp.fc_out.weight\", \"transformer.h.3.attn.k_proj.weight\", \"transformer.h.3.attn.out_proj.weight\", \"transformer.h.3.attn.q_proj.weight\", \"transformer.h.3.attn.v_proj.weight\", \"transformer.h.3.ln_1.bias\", \"transformer.h.3.ln_1.weight\", \"transformer.h.3.mlp.fc_in.bias\", \"transformer.h.3.mlp.fc_in.weight\", \"transformer.h.3.mlp.fc_out.bias\", \"transformer.h.3.mlp.fc_out.weight\", \"transformer.h.4.attn.k_proj.weight\", \"transformer.h.4.attn.out_proj.weight\", \"transformer.h.4.attn.q_proj.weight\", \"transformer.h.4.attn.v_proj.weight\", \"transformer.h.4.ln_1.bias\", \"transformer.h.4.ln_1.weight\", \"transformer.h.4.mlp.fc_in.bias\", \"transformer.h.4.mlp.fc_in.weight\", \"transformer.h.4.mlp.fc_out.bias\", \"transformer.h.4.mlp.fc_out.weight\", \"transformer.h.5.attn.k_proj.weight\", \"transformer.h.5.attn.out_proj.weight\", \"transformer.h.5.attn.q_proj.weight\", \"transformer.h.5.attn.v_proj.weight\", \"transformer.h.5.ln_1.bias\", \"transformer.h.5.ln_1.weight\", \"transformer.h.5.mlp.fc_in.bias\", \"transformer.h.5.mlp.fc_in.weight\", \"transformer.h.5.mlp.fc_out.bias\", \"transformer.h.5.mlp.fc_out.weight\", \"transformer.h.6.attn.k_proj.weight\", \"transformer.h.6.attn.out_proj.weight\", \"transformer.h.6.attn.q_proj.weight\", \"transformer.h.6.attn.v_proj.weight\", \"transformer.h.6.ln_1.bias\", \"transformer.h.6.ln_1.weight\", \"transformer.h.6.mlp.fc_in.bias\", \"transformer.h.6.mlp.fc_in.weight\", \"transformer.h.6.mlp.fc_out.bias\", \"transformer.h.6.mlp.fc_out.weight\", \"transformer.h.7.attn.k_proj.weight\", \"transformer.h.7.attn.out_proj.weight\", \"transformer.h.7.attn.q_proj.weight\", \"transformer.h.7.attn.v_proj.weight\", \"transformer.h.7.ln_1.bias\", \"transformer.h.7.ln_1.weight\", \"transformer.h.7.mlp.fc_in.bias\", \"transformer.h.7.mlp.fc_in.weight\", \"transformer.h.7.mlp.fc_out.bias\", \"transformer.h.7.mlp.fc_out.weight\", \"transformer.h.8.attn.k_proj.weight\", \"transformer.h.8.attn.out_proj.weight\", \"transformer.h.8.attn.q_proj.weight\", \"transformer.h.8.attn.v_proj.weight\", \"transformer.h.8.ln_1.bias\", \"transformer.h.8.ln_1.weight\", \"transformer.h.8.mlp.fc_in.bias\", \"transformer.h.8.mlp.fc_in.weight\", \"transformer.h.8.mlp.fc_out.bias\", \"transformer.h.8.mlp.fc_out.weight\", \"transformer.h.9.attn.k_proj.weight\", \"transformer.h.9.attn.out_proj.weight\", \"transformer.h.9.attn.q_proj.weight\", \"transformer.h.9.attn.v_proj.weight\", \"transformer.h.9.ln_1.bias\", \"transformer.h.9.ln_1.weight\", \"transformer.h.9.mlp.fc_in.bias\", \"transformer.h.9.mlp.fc_in.weight\", \"transformer.h.9.mlp.fc_out.bias\", \"transformer.h.9.mlp.fc_out.weight\", \"transformer.ln_f.bias\", \"transformer.ln_f.weight\", \"transformer.wte.weight\". "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lm_head.bias', 'lm_head.weight', 'transformer.h.0.attn.k_proj.weight', 'transformer.h.0.attn.out_proj.weight', 'transformer.h.0.attn.q_proj.weight', 'transformer.h.0.attn.v_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.0.mlp.fc_in.bias', 'transformer.h.0.mlp.fc_in.weight', 'transformer.h.0.mlp.fc_out.bias', 'transformer.h.0.mlp.fc_out.weight', 'transformer.h.1.attn.k_proj.weight', 'transformer.h.1.attn.out_proj.weight', 'transformer.h.1.attn.q_proj.weight', 'transformer.h.1.attn.v_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.mlp.fc_in.bias', 'transformer.h.1.mlp.fc_in.weight', 'transformer.h.1.mlp.fc_out.bias', 'transformer.h.1.mlp.fc_out.weight', 'transformer.h.10.attn.k_proj.weight', 'transformer.h.10.attn.out_proj.weight', 'transformer.h.10.attn.q_proj.weight', 'transformer.h.10.attn.v_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.mlp.fc_in.bias', 'transformer.h.10.mlp.fc_in.weight', 'transformer.h.10.mlp.fc_out.bias', 'transformer.h.10.mlp.fc_out.weight', 'transformer.h.11.attn.k_proj.weight', 'transformer.h.11.attn.out_proj.weight', 'transformer.h.11.attn.q_proj.weight', 'transformer.h.11.attn.v_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.mlp.fc_in.bias', 'transformer.h.11.mlp.fc_in.weight', 'transformer.h.11.mlp.fc_out.bias', 'transformer.h.11.mlp.fc_out.weight', 'transformer.h.2.attn.k_proj.weight', 'transformer.h.2.attn.out_proj.weight', 'transformer.h.2.attn.q_proj.weight', 'transformer.h.2.attn.v_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.mlp.fc_in.bias', 'transformer.h.2.mlp.fc_in.weight', 'transformer.h.2.mlp.fc_out.bias', 'transformer.h.2.mlp.fc_out.weight', 'transformer.h.3.attn.k_proj.weight', 'transformer.h.3.attn.out_proj.weight', 'transformer.h.3.attn.q_proj.weight', 'transformer.h.3.attn.v_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.mlp.fc_in.bias', 'transformer.h.3.mlp.fc_in.weight', 'transformer.h.3.mlp.fc_out.bias', 'transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.attn.k_proj.weight', 'transformer.h.4.attn.out_proj.weight', 'transformer.h.4.attn.q_proj.weight', 'transformer.h.4.attn.v_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.mlp.fc_in.bias', 'transformer.h.4.mlp.fc_in.weight', 'transformer.h.4.mlp.fc_out.bias', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.attn.k_proj.weight', 'transformer.h.5.attn.out_proj.weight', 'transformer.h.5.attn.q_proj.weight', 'transformer.h.5.attn.v_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.mlp.fc_in.bias', 'transformer.h.5.mlp.fc_in.weight', 'transformer.h.5.mlp.fc_out.bias', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.attn.k_proj.weight', 'transformer.h.6.attn.out_proj.weight', 'transformer.h.6.attn.q_proj.weight', 'transformer.h.6.attn.v_proj.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.mlp.fc_in.bias', 'transformer.h.6.mlp.fc_in.weight', 'transformer.h.6.mlp.fc_out.bias', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.attn.k_proj.weight', 'transformer.h.7.attn.out_proj.weight', 'transformer.h.7.attn.q_proj.weight', 'transformer.h.7.attn.v_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.mlp.fc_in.bias', 'transformer.h.7.mlp.fc_in.weight', 'transformer.h.7.mlp.fc_out.bias', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.attn.k_proj.weight', 'transformer.h.8.attn.out_proj.weight', 'transformer.h.8.attn.q_proj.weight', 'transformer.h.8.attn.v_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.mlp.fc_in.bias', 'transformer.h.8.mlp.fc_in.weight', 'transformer.h.8.mlp.fc_out.bias', 'transformer.h.8.mlp.fc_out.weight', 'transformer.h.9.attn.k_proj.weight', 'transformer.h.9.attn.out_proj.weight', 'transformer.h.9.attn.q_proj.weight', 'transformer.h.9.attn.v_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.mlp.fc_in.bias', 'transformer.h.9.mlp.fc_in.weight', 'transformer.h.9.mlp.fc_out.bias', 'transformer.h.9.mlp.fc_out.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.wte.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(state_dict.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
