{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works\n",
      "Using renderer: notebook_connected\n"
     ]
    }
   ],
   "source": [
    "#Set up stuff from notebook\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "DEVELOPMENT_MODE = False\n",
    "# Detect if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install if in Colab\n",
    "if IN_COLAB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    # Install a faster Node version\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
    "\n",
    "# Hot reload in development mode & not running on the CD\n",
    "if not IN_COLAB:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if not ip.extension_manager.loaded:\n",
    "        ip.extension_manager.load('autoreload')\n",
    "        %autoreload 2\n",
    "\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "print(\"works\")\n",
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")\n",
    "\n",
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model, show_param = False):\n",
    "    count = 0\n",
    "    parameters = []\n",
    "    names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if show_param == True:\n",
    "            print(name, param.size())\n",
    "        names.append([name, param.size()])\n",
    "        parameters.append(param.data)\n",
    "        count += 1\n",
    "    print(f\"Model has {count} named parameters\")\n",
    "    return parameters, names\n",
    "\n",
    "\n",
    "def unpack_names(names, first = 0, last = None):\n",
    "    if last == None:\n",
    "        for item in names[first:len(names)-1]:\n",
    "            print(f\"Param: {item[0]:<40} Dimensions of tensor: {item[1]:<30}\")\n",
    "    else:\n",
    "        for item in names[first: last]:\n",
    "            print(f\"Param: {item[0]:<40} Dimensions of tensor: {str(item[1]):<30}\")\n",
    "\n",
    "\n",
    "def compare(model_1, model_2):\n",
    "    different = []\n",
    "    model_1_params, model_1_names = get_params(model_1)\n",
    "    model_2_params, model_2_names = get_params(model_2)\n",
    "\n",
    "    if len(model_1_params) != len(model_2_params):\n",
    "        print(\"There are a different number of parameters between the models. Please check model config\")\n",
    "        print(f\"Model 1 has {len(model_1_params)} parameters. Model 2 has {len(model_2_params)}\")\n",
    "\n",
    "    for i in range(min(len(model_1_params),(len(model_2_params)))):\n",
    "        if model_1_names[i][1] != model_2_names[i][1]:\n",
    "            different.append([model_1_names[i], model_2_names[i], (\"index\", i)])\n",
    "\n",
    "    print(f\"There are {len(different)} parameters that are different from each other\")\n",
    "    return different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM        \n",
    "model_path = r\"C:\\Users\\allan\\ResearchStuff\\checkpoint-1953\"\n",
    "\n",
    "gptj = GPTJForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTJBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc_out): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"_name_or_path\": \"C:\\\\Users\\\\allan\\\\ResearchStuff\\\\checkpoint-1953\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gptj)\n",
    "gptj.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 125 named parameters\n"
     ]
    }
   ],
   "source": [
    "gptj_params, gptj_names = get_params(gptj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: transformer.wte.weight                   Dimensions of tensor: torch.Size([50257, 768])      \n",
      "Param: transformer.h.0.ln_1.weight              Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.0.ln_1.bias                Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.0.attn.k_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.0.attn.v_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.0.attn.q_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.0.attn.out_proj.weight     Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.0.mlp.fc_in.weight         Dimensions of tensor: torch.Size([3072, 768])       \n",
      "Param: transformer.h.0.mlp.fc_in.bias           Dimensions of tensor: torch.Size([3072])            \n",
      "Param: transformer.h.0.mlp.fc_out.weight        Dimensions of tensor: torch.Size([768, 3072])       \n",
      "Param: transformer.h.0.mlp.fc_out.bias          Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.1.ln_1.weight              Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.1.ln_1.bias                Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.1.attn.k_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.1.attn.v_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.1.attn.q_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.1.attn.out_proj.weight     Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.1.mlp.fc_in.weight         Dimensions of tensor: torch.Size([3072, 768])       \n",
      "Param: transformer.h.1.mlp.fc_in.bias           Dimensions of tensor: torch.Size([3072])            \n",
      "Param: transformer.h.1.mlp.fc_out.weight        Dimensions of tensor: torch.Size([768, 3072])       \n",
      "Param: transformer.h.1.mlp.fc_out.bias          Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.2.ln_1.weight              Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.2.ln_1.bias                Dimensions of tensor: torch.Size([768])             \n",
      "Param: transformer.h.2.attn.k_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.2.attn.v_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.2.attn.q_proj.weight       Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.2.attn.out_proj.weight     Dimensions of tensor: torch.Size([768, 768])        \n",
      "Param: transformer.h.2.mlp.fc_in.weight         Dimensions of tensor: torch.Size([3072, 768])       \n",
      "Param: transformer.h.2.mlp.fc_in.bias           Dimensions of tensor: torch.Size([3072])            \n",
      "Param: transformer.h.2.mlp.fc_out.weight        Dimensions of tensor: torch.Size([768, 3072])       \n"
     ]
    }
   ],
   "source": [
    "unpack_names(gptj_names, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformerConfig\n",
    "\n",
    "config = {\n",
    "    \"d_model\" : 768,\n",
    "    \"d_head\" : 64,\n",
    "    \"n_layers\" : 12,\n",
    "    \"n_ctx\" : 1024,\n",
    "    \"n_heads\" : 12,\n",
    "    \"d_mlp\" : 3072, \n",
    "    \"d_vocab\" : 50257,\n",
    "    \"act_fn\" : \"gelu_new\", \n",
    "    \"eps\" : 1e-05,\n",
    "    \"normalization_type\" : \"LN\", \n",
    "    \"positional_embedding_type\" : \"rotary\",\n",
    "    \"rotary_dim\": 64,\n",
    "    \"post_embedding_ln\": False\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "hooked = HookedTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 197 named parameters\n"
     ]
    }
   ],
   "source": [
    "hooked_params, hooked_names = get_params(hooked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: embed.W_E                                Dimensions of tensor: torch.Size([50257, 768])      \n",
      "Param: blocks.0.ln1.w                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.0.ln1.b                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.0.ln2.w                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.0.ln2.b                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.0.attn.W_Q                        Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Param: blocks.0.attn.W_O                        Dimensions of tensor: torch.Size([12, 64, 768])     \n",
      "Param: blocks.0.attn.b_Q                        Dimensions of tensor: torch.Size([12, 64])          \n",
      "Param: blocks.0.attn.b_O                        Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.0.attn.W_K                        Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Param: blocks.0.attn.W_V                        Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Param: blocks.0.attn.b_K                        Dimensions of tensor: torch.Size([12, 64])          \n",
      "Param: blocks.0.attn.b_V                        Dimensions of tensor: torch.Size([12, 64])          \n",
      "Param: blocks.0.mlp.W_in                        Dimensions of tensor: torch.Size([768, 3072])       \n",
      "Param: blocks.0.mlp.b_in                        Dimensions of tensor: torch.Size([3072])            \n",
      "Param: blocks.0.mlp.W_out                       Dimensions of tensor: torch.Size([3072, 768])       \n",
      "Param: blocks.0.mlp.b_out                       Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.1.ln1.w                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.1.ln1.b                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.1.ln2.w                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.1.ln2.b                           Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.1.attn.W_Q                        Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Param: blocks.1.attn.W_O                        Dimensions of tensor: torch.Size([12, 64, 768])     \n",
      "Param: blocks.1.attn.b_Q                        Dimensions of tensor: torch.Size([12, 64])          \n",
      "Param: blocks.1.attn.b_O                        Dimensions of tensor: torch.Size([768])             \n",
      "Param: blocks.1.attn.W_K                        Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Param: blocks.1.attn.W_V                        Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Param: blocks.1.attn.b_K                        Dimensions of tensor: torch.Size([12, 64])          \n",
      "Param: blocks.1.attn.b_V                        Dimensions of tensor: torch.Size([12, 64])          \n",
      "Param: blocks.1.mlp.W_in                        Dimensions of tensor: torch.Size([768, 3072])       \n"
     ]
    }
   ],
   "source": [
    "unpack_names(hooked_names, first = 0, last = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 197 named parameters\n",
      "Model has 125 named parameters\n",
      "There are a different number of parameters between the models. Please check model config\n",
      "Model 1 has 197 parameters. Model 2 has 125\n",
      "There are 105 parameters that are different from each other\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['blocks.0.ln2.w', torch.Size([768])],\n",
       "  ['transformer.h.0.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 3)],\n",
       " [['blocks.0.ln2.b', torch.Size([768])],\n",
       "  ['transformer.h.0.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 4)],\n",
       " [['blocks.0.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.0.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 5)],\n",
       " [['blocks.0.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.0.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 6)],\n",
       " [['blocks.0.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.0.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 7)],\n",
       " [['blocks.0.attn.b_O', torch.Size([768])],\n",
       "  ['transformer.h.0.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 8)],\n",
       " [['blocks.0.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.0.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 9)],\n",
       " [['blocks.0.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.0.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 10)],\n",
       " [['blocks.0.attn.b_K', torch.Size([12, 64])],\n",
       "  ['transformer.h.1.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 11)],\n",
       " [['blocks.0.attn.b_V', torch.Size([12, 64])],\n",
       "  ['transformer.h.1.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 12)],\n",
       " [['blocks.0.mlp.W_in', torch.Size([768, 3072])],\n",
       "  ['transformer.h.1.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 13)],\n",
       " [['blocks.0.mlp.b_in', torch.Size([3072])],\n",
       "  ['transformer.h.1.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 14)],\n",
       " [['blocks.0.mlp.W_out', torch.Size([3072, 768])],\n",
       "  ['transformer.h.1.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 15)],\n",
       " [['blocks.0.mlp.b_out', torch.Size([768])],\n",
       "  ['transformer.h.1.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 16)],\n",
       " [['blocks.1.ln1.w', torch.Size([768])],\n",
       "  ['transformer.h.1.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 17)],\n",
       " [['blocks.1.ln1.b', torch.Size([768])],\n",
       "  ['transformer.h.1.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 18)],\n",
       " [['blocks.1.ln2.w', torch.Size([768])],\n",
       "  ['transformer.h.1.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 19)],\n",
       " [['blocks.1.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.2.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 21)],\n",
       " [['blocks.1.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.2.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 22)],\n",
       " [['blocks.1.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.2.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 23)],\n",
       " [['blocks.1.attn.b_O', torch.Size([768])],\n",
       "  ['transformer.h.2.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 24)],\n",
       " [['blocks.1.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.2.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 25)],\n",
       " [['blocks.1.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.2.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 26)],\n",
       " [['blocks.1.attn.b_K', torch.Size([12, 64])],\n",
       "  ['transformer.h.2.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 27)],\n",
       " [['blocks.1.attn.b_V', torch.Size([12, 64])],\n",
       "  ['transformer.h.2.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 28)],\n",
       " [['blocks.1.mlp.b_in', torch.Size([3072])],\n",
       "  ['transformer.h.2.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 30)],\n",
       " [['blocks.1.mlp.W_out', torch.Size([3072, 768])],\n",
       "  ['transformer.h.3.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 31)],\n",
       " [['blocks.2.ln1.w', torch.Size([768])],\n",
       "  ['transformer.h.3.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 33)],\n",
       " [['blocks.2.ln1.b', torch.Size([768])],\n",
       "  ['transformer.h.3.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 34)],\n",
       " [['blocks.2.ln2.w', torch.Size([768])],\n",
       "  ['transformer.h.3.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 35)],\n",
       " [['blocks.2.ln2.b', torch.Size([768])],\n",
       "  ['transformer.h.3.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 36)],\n",
       " [['blocks.2.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.3.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 37)],\n",
       " [['blocks.2.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.3.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 38)],\n",
       " [['blocks.2.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.3.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 39)],\n",
       " [['blocks.2.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.4.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 41)],\n",
       " [['blocks.2.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.4.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 42)],\n",
       " [['blocks.2.attn.b_K', torch.Size([12, 64])],\n",
       "  ['transformer.h.4.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 43)],\n",
       " [['blocks.2.attn.b_V', torch.Size([12, 64])],\n",
       "  ['transformer.h.4.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 44)],\n",
       " [['blocks.2.mlp.W_in', torch.Size([768, 3072])],\n",
       "  ['transformer.h.4.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 45)],\n",
       " [['blocks.2.mlp.b_in', torch.Size([3072])],\n",
       "  ['transformer.h.4.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 46)],\n",
       " [['blocks.2.mlp.b_out', torch.Size([768])],\n",
       "  ['transformer.h.4.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 48)],\n",
       " [['blocks.3.ln1.w', torch.Size([768])],\n",
       "  ['transformer.h.4.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 49)],\n",
       " [['blocks.3.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.5.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 53)],\n",
       " [['blocks.3.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.5.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 54)],\n",
       " [['blocks.3.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.5.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 55)],\n",
       " [['blocks.3.attn.b_O', torch.Size([768])],\n",
       "  ['transformer.h.5.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 56)],\n",
       " [['blocks.3.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.5.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 57)],\n",
       " [['blocks.3.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.5.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 58)],\n",
       " [['blocks.3.attn.b_K', torch.Size([12, 64])],\n",
       "  ['transformer.h.5.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 59)],\n",
       " [['blocks.3.attn.b_V', torch.Size([12, 64])],\n",
       "  ['transformer.h.5.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 60)],\n",
       " [['blocks.3.mlp.W_in', torch.Size([768, 3072])],\n",
       "  ['transformer.h.6.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 61)],\n",
       " [['blocks.3.mlp.b_in', torch.Size([3072])],\n",
       "  ['transformer.h.6.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 62)],\n",
       " [['blocks.3.mlp.W_out', torch.Size([3072, 768])],\n",
       "  ['transformer.h.6.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 63)],\n",
       " [['blocks.3.mlp.b_out', torch.Size([768])],\n",
       "  ['transformer.h.6.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 64)],\n",
       " [['blocks.4.ln1.w', torch.Size([768])],\n",
       "  ['transformer.h.6.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 65)],\n",
       " [['blocks.4.ln1.b', torch.Size([768])],\n",
       "  ['transformer.h.6.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 66)],\n",
       " [['blocks.4.ln2.w', torch.Size([768])],\n",
       "  ['transformer.h.6.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 67)],\n",
       " [['blocks.4.ln2.b', torch.Size([768])],\n",
       "  ['transformer.h.6.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 68)],\n",
       " [['blocks.4.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.6.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 69)],\n",
       " [['blocks.4.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.6.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 70)],\n",
       " [['blocks.4.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.7.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 71)],\n",
       " [['blocks.4.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.7.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 73)],\n",
       " [['blocks.4.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.7.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 74)],\n",
       " [['blocks.4.attn.b_K', torch.Size([12, 64])],\n",
       "  ['transformer.h.7.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 75)],\n",
       " [['blocks.4.attn.b_V', torch.Size([12, 64])],\n",
       "  ['transformer.h.7.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 76)],\n",
       " [['blocks.4.mlp.W_in', torch.Size([768, 3072])],\n",
       "  ['transformer.h.7.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 77)],\n",
       " [['blocks.4.mlp.W_out', torch.Size([3072, 768])],\n",
       "  ['transformer.h.7.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 79)],\n",
       " [['blocks.5.ln2.w', torch.Size([768])],\n",
       "  ['transformer.h.8.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 83)],\n",
       " [['blocks.5.ln2.b', torch.Size([768])],\n",
       "  ['transformer.h.8.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 84)],\n",
       " [['blocks.5.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.8.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 85)],\n",
       " [['blocks.5.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.8.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 86)],\n",
       " [['blocks.5.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.8.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 87)],\n",
       " [['blocks.5.attn.b_O', torch.Size([768])],\n",
       "  ['transformer.h.8.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 88)],\n",
       " [['blocks.5.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.8.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 89)],\n",
       " [['blocks.5.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.8.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 90)],\n",
       " [['blocks.5.attn.b_K', torch.Size([12, 64])],\n",
       "  ['transformer.h.9.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 91)],\n",
       " [['blocks.5.attn.b_V', torch.Size([12, 64])],\n",
       "  ['transformer.h.9.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 92)],\n",
       " [['blocks.5.mlp.W_in', torch.Size([768, 3072])],\n",
       "  ['transformer.h.9.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 93)],\n",
       " [['blocks.5.mlp.b_in', torch.Size([3072])],\n",
       "  ['transformer.h.9.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 94)],\n",
       " [['blocks.5.mlp.W_out', torch.Size([3072, 768])],\n",
       "  ['transformer.h.9.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 95)],\n",
       " [['blocks.5.mlp.b_out', torch.Size([768])],\n",
       "  ['transformer.h.9.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 96)],\n",
       " [['blocks.6.ln1.w', torch.Size([768])],\n",
       "  ['transformer.h.9.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 97)],\n",
       " [['blocks.6.ln1.b', torch.Size([768])],\n",
       "  ['transformer.h.9.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 98)],\n",
       " [['blocks.6.ln2.w', torch.Size([768])],\n",
       "  ['transformer.h.9.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 99)],\n",
       " [['blocks.6.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.10.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 101)],\n",
       " [['blocks.6.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.10.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 102)],\n",
       " [['blocks.6.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.10.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 103)],\n",
       " [['blocks.6.attn.b_O', torch.Size([768])],\n",
       "  ['transformer.h.10.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 104)],\n",
       " [['blocks.6.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.10.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 105)],\n",
       " [['blocks.6.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.10.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 106)],\n",
       " [['blocks.6.attn.b_K', torch.Size([12, 64])],\n",
       "  ['transformer.h.10.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 107)],\n",
       " [['blocks.6.attn.b_V', torch.Size([12, 64])],\n",
       "  ['transformer.h.10.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 108)],\n",
       " [['blocks.6.mlp.b_in', torch.Size([3072])],\n",
       "  ['transformer.h.10.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 110)],\n",
       " [['blocks.6.mlp.W_out', torch.Size([3072, 768])],\n",
       "  ['transformer.h.11.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 111)],\n",
       " [['blocks.7.ln1.w', torch.Size([768])],\n",
       "  ['transformer.h.11.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 113)],\n",
       " [['blocks.7.ln1.b', torch.Size([768])],\n",
       "  ['transformer.h.11.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 114)],\n",
       " [['blocks.7.ln2.w', torch.Size([768])],\n",
       "  ['transformer.h.11.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 115)],\n",
       " [['blocks.7.ln2.b', torch.Size([768])],\n",
       "  ['transformer.h.11.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 116)],\n",
       " [['blocks.7.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['transformer.h.11.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 117)],\n",
       " [['blocks.7.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['transformer.h.11.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 118)],\n",
       " [['blocks.7.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['transformer.h.11.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 119)],\n",
       " [['blocks.7.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['transformer.ln_f.weight', torch.Size([768])],\n",
       "  ('index', 121)],\n",
       " [['blocks.7.attn.W_V', torch.Size([12, 768, 64])],\n",
       "  ['transformer.ln_f.bias', torch.Size([768])],\n",
       "  ('index', 122)],\n",
       " [['blocks.7.attn.b_K', torch.Size([12, 64])],\n",
       "  ['lm_head.weight', torch.Size([50257, 768])],\n",
       "  ('index', 123)],\n",
       " [['blocks.7.attn.b_V', torch.Size([12, 64])],\n",
       "  ['lm_head.bias', torch.Size([50257])],\n",
       "  ('index', 124)]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(hooked, gptj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
