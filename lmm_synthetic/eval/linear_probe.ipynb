{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2024-12-17 14:20:51.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLoading model and tokenizer from /home/allanz/data/vlm_checkpoint/final_model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-17 14:20:53.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-17 14:20:53.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n",
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-12-17 14:20:53.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_multimodal_projector\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mLoading multimodal projector: linear\u001b[0m\n",
      "\u001b[32m2024-12-17 14:20:54.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-17 14:20:54.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from datasets import load_from_disk\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, GPTJForCausalLM\n",
    "from lmm_synthetic.mm_train.gptj_vlm import GPTJ_VLM\n",
    "from lmm_synthetic.mm_train.utils import load_vision_encoder\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths \n",
    "dataset_path = '/data/lmm/generated/v3_spatial_grid_multimodal'\n",
    "vlm_path = '/home/allanz/data/vlm_checkpoint/final_model'\n",
    "lm_path = \"/data/lmm/checkpoints/lm/lm-pretrain-only-checkpoint-1953\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(dataset)\n",
    "\n",
    "# Load VLM and CLIP model\n",
    "def load_model_and_tokenizer(model_path, multimodal=False):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from the specified path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model and tokenizer from {model_path}\")\n",
    "    model, tokenizer = None, None\n",
    "    if multimodal:\n",
    "        model = GPTJ_VLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model.config.pretrained_lm_path)\n",
    "    else:\n",
    "        model = GPTJForCausalLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "vlm, vlm_tokenizer = load_model_and_tokenizer(vlm_path, multimodal=True)\n",
    "\n",
    "clip_vision_model = vlm.vision_encoder.clip_vision_model\n",
    "encoder, image_transforms, _ = load_vision_encoder(\"clip\")\n",
    "\n",
    "#Parse grid\n",
    "def parse_grid(grid_str, K):\n",
    "    \"\"\"\n",
    "    Parse the grid string into a 2D list of grid cells.\n",
    "    \"\"\"\n",
    "    grid_str = '\\n'.join(grid_str.split('\\n')[:K])\n",
    "    rows = grid_str.strip().split('\\n')\n",
    "    return [[cell.strip() for cell in row.split('|') if cell.strip()] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [[1, 2, 3, 8, 9, 10, 15, 16, 17], [3, 4, 5, 10 , 11, 12, 17, 18, 19], [5, 6, 7, 12, 13, 14, 19, 20, 21],\n",
    "           [15, 16, 17, 22, 23, 24, 29, 30, 31], [17, 18, 19, 24, 25, 26, 31, 32, 33], [19, 20, 21, 26, 27, 28, 33, 34, 35],\n",
    "           [29, 30, 31, 36, 37, 38, 43, 44, 45], [31, 32, 33, 38, 39, 40, 45, 46, 47], [33, 34, 35, 40, 41, 42, 47, 48, 49]]\n",
    "\n",
    "def position_concat(image_tensor, patch = patches):\n",
    "    \"\"\"\n",
    "    Concats tensors, should return shape 1x9x6912 \n",
    "    \"\"\"\n",
    "    concat = []\n",
    "    for position in patch:\n",
    "        concat.append(torch.cat([image_tensor[0][i] for i in position], dim = 0))\n",
    "    \n",
    "    return torch.stack(concat).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_no_batch(set_type, num_samples):\n",
    "    \"\"\"\n",
    "    Prepares data to feed into model by turning images \n",
    "    into 1 x 9 x 6912 tensors, grid cell information \n",
    "    into 1 x 9 x 10 tensors \n",
    "    \"\"\" \n",
    "    ANIMALS = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder.to(device)\n",
    "\n",
    "    reformatted_data = []\n",
    "    t_0 = time.perf_counter()\n",
    "    \n",
    "    # Extract image tensors and grid cells in each batch\n",
    "    for i in range(num_samples):\n",
    "        image = Image.open(f\"/data/lmm/generated/v3_spatial_grid_multimodal/images/{set_type}_{i}.png\")\n",
    "        image_tensor = image_transforms(image).unsqueeze(0).to(device) \n",
    "        with torch.no_grad():\n",
    "            image_tensor_tokens = encoder(image_tensor)\n",
    "\n",
    "        grid = parse_grid(dataset[set_type][i]['text'], 3)\n",
    "\n",
    "        temp = []\n",
    "        for row in grid:\n",
    "            for animal in row:\n",
    "                temp_tensor = torch.zeros(1,10)\n",
    "                temp_tensor[0][ANIMALS.index(animal)] = 1\n",
    "                temp.append(temp_tensor)\n",
    "        \n",
    "        reformatted_data.append((position_concat(image_tensor_tokens), torch.stack(temp, dim = 1))) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i} samples\") \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    t_3 = time.perf_counter()\n",
    "    print(f\"Finished preparing data in {t_3 - t_0} seconds\")\n",
    "    return reformatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 samples\n",
      "Processed 50 samples\n",
      "Processed 100 samples\n",
      "Processed 150 samples\n",
      "Processed 200 samples\n",
      "Processed 250 samples\n",
      "Processed 300 samples\n",
      "Processed 350 samples\n",
      "Processed 400 samples\n",
      "Processed 450 samples\n",
      "Processed 500 samples\n",
      "Processed 550 samples\n",
      "Processed 600 samples\n",
      "Processed 650 samples\n",
      "Processed 700 samples\n",
      "Processed 750 samples\n",
      "Processed 800 samples\n",
      "Processed 850 samples\n",
      "Processed 900 samples\n",
      "Processed 950 samples\n",
      "Processed 1000 samples\n",
      "Processed 1050 samples\n",
      "Processed 1100 samples\n",
      "Processed 1150 samples\n",
      "Processed 1200 samples\n",
      "Processed 1250 samples\n",
      "Processed 1300 samples\n",
      "Processed 1350 samples\n",
      "Processed 1400 samples\n",
      "Processed 1450 samples\n",
      "Processed 1500 samples\n",
      "Processed 1550 samples\n",
      "Processed 1600 samples\n",
      "Processed 1650 samples\n",
      "Processed 1700 samples\n",
      "Processed 1750 samples\n",
      "Processed 1800 samples\n",
      "Processed 1850 samples\n",
      "Processed 1900 samples\n",
      "Processed 1950 samples\n",
      "Processed 2000 samples\n",
      "Processed 2050 samples\n",
      "Processed 2100 samples\n",
      "Processed 2150 samples\n",
      "Processed 2200 samples\n",
      "Processed 2250 samples\n",
      "Processed 2300 samples\n",
      "Processed 2350 samples\n",
      "Processed 2400 samples\n",
      "Processed 2450 samples\n",
      "Processed 2500 samples\n",
      "Processed 2550 samples\n",
      "Processed 2600 samples\n",
      "Processed 2650 samples\n",
      "Processed 2700 samples\n",
      "Processed 2750 samples\n",
      "Processed 2800 samples\n",
      "Processed 2850 samples\n",
      "Processed 2900 samples\n",
      "Processed 2950 samples\n",
      "Processed 3000 samples\n",
      "Processed 3050 samples\n",
      "Processed 3100 samples\n",
      "Processed 3150 samples\n",
      "Processed 3200 samples\n",
      "Processed 3250 samples\n",
      "Processed 3300 samples\n",
      "Processed 3350 samples\n",
      "Processed 3400 samples\n",
      "Processed 3450 samples\n",
      "Processed 3500 samples\n",
      "Processed 3550 samples\n",
      "Processed 3600 samples\n",
      "Processed 3650 samples\n",
      "Processed 3700 samples\n",
      "Processed 3750 samples\n",
      "Processed 3800 samples\n",
      "Processed 3850 samples\n",
      "Processed 3900 samples\n",
      "Processed 3950 samples\n",
      "Processed 4000 samples\n",
      "Processed 4050 samples\n",
      "Processed 4100 samples\n",
      "Processed 4150 samples\n",
      "Processed 4200 samples\n",
      "Processed 4250 samples\n",
      "Processed 4300 samples\n",
      "Processed 4350 samples\n",
      "Processed 4400 samples\n",
      "Processed 4450 samples\n",
      "Processed 4500 samples\n",
      "Processed 4550 samples\n",
      "Processed 4600 samples\n",
      "Processed 4650 samples\n",
      "Processed 4700 samples\n",
      "Processed 4750 samples\n",
      "Processed 4800 samples\n",
      "Processed 4850 samples\n",
      "Processed 4900 samples\n",
      "Processed 4950 samples\n",
      "Finished preparing data in 68.47862392663956 seconds\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data_no_batch(\"train\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 6912])\n",
      "torch.Size([1, 9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(data[4999][0].shape)\n",
    "print(data[4999][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, dataset = data, num_epochs = 10, learning_rate = 1e-4):\n",
    "    \"\"\"\n",
    "    Train linear layer on data. After running image \n",
    "    tensors through linear layer, apply softmax and\n",
    "    use cross entropy loss to calculate loss.\n",
    "    \"\"\"\n",
    "    linear = model\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    linear.to(device)\n",
    "    linear.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(linear.parameters(), lr=learning_rate)\n",
    "    cross_entropy_loss = F.cross_entropy\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    t_0 = time.perf_counter()\n",
    "    for epoch in range(num_epochs):\n",
    "        t_1 = time.perf_counter()\n",
    "        epoch_loss = 0\n",
    "        for i in range(len(dataset)):\n",
    "            image_tensor, grid_tensor = dataset[i]\n",
    "            output = linear(image_tensor)\n",
    "            prediction = softmax(output)\n",
    "\n",
    "            loss = cross_entropy_loss(prediction, grid_tensor.to(device))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "       \n",
    "        t_2 = time.perf_counter()\n",
    "        print(f\"Epoch {epoch} loss: {epoch_loss} | completed in {t_2-t_1} seconds\")\n",
    "        print(f\"Average loss: {epoch_loss/len(dataset)}\")     \n",
    "\n",
    "        total_loss += epoch_loss\n",
    "    t_3 = time.perf_counter()\n",
    "    print(f\"Training completed in {t_3 - t_0} seconds\")\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 7223.131538510323 | completed in 4.361877731978893 seconds\n",
      "Average loss: 1.4446263077020645\n",
      "Epoch 1 loss: 7153.993084669113 | completed in 4.31763705983758 seconds\n",
      "Average loss: 1.4307986169338227\n",
      "Epoch 2 loss: 7153.667843103409 | completed in 4.304949581623077 seconds\n",
      "Average loss: 1.4307335686206817\n",
      "Epoch 3 loss: 7153.640385866165 | completed in 4.300205413252115 seconds\n",
      "Average loss: 1.4307280771732331\n",
      "Epoch 4 loss: 7153.637858390808 | completed in 4.299834690988064 seconds\n",
      "Average loss: 1.4307275716781616\n",
      "Epoch 5 loss: 7153.6376321315765 | completed in 4.283599615097046 seconds\n",
      "Average loss: 1.4307275264263153\n",
      "Epoch 6 loss: 7153.637609958649 | completed in 4.280631445348263 seconds\n",
      "Average loss: 1.4307275219917297\n",
      "Epoch 7 loss: 7153.63760304451 | completed in 4.277513798326254 seconds\n",
      "Average loss: 1.430727520608902\n",
      "Epoch 8 loss: 7153.637602686882 | completed in 4.277612302452326 seconds\n",
      "Average loss: 1.4307275205373764\n",
      "Epoch 9 loss: 7153.63760137558 | completed in 4.275231879204512 seconds\n",
      "Average loss: 1.430727520275116\n",
      "Training completed in 42.98105662316084 seconds\n"
     ]
    }
   ],
   "source": [
    "linear = LinearLayer(6912, 10)\n",
    "train(linear, data, 10, 1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
