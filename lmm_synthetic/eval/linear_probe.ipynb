{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2024-12-17 16:05:54.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLoading model and tokenizer from /home/allanz/data/vlm_checkpoint/final_model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-17 16:05:56.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-17 16:05:56.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n",
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-12-17 16:05:57.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_multimodal_projector\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mLoading multimodal projector: linear\u001b[0m\n",
      "\u001b[32m2024-12-17 16:05:57.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-17 16:05:57.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from datasets import load_from_disk\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, GPTJForCausalLM\n",
    "from lmm_synthetic.mm_train.gptj_vlm import GPTJ_VLM\n",
    "from lmm_synthetic.mm_train.utils import load_vision_encoder\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths \n",
    "dataset_path = '/data/lmm/generated/v3_spatial_grid_multimodal'\n",
    "vlm_path = '/home/allanz/data/vlm_checkpoint/final_model'\n",
    "lm_path = \"/data/lmm/checkpoints/lm/lm-pretrain-only-checkpoint-1953\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(dataset)\n",
    "\n",
    "# Load VLM and CLIP model\n",
    "def load_model_and_tokenizer(model_path, multimodal=False):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from the specified path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model and tokenizer from {model_path}\")\n",
    "    model, tokenizer = None, None\n",
    "    if multimodal:\n",
    "        model = GPTJ_VLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model.config.pretrained_lm_path)\n",
    "    else:\n",
    "        model = GPTJForCausalLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "vlm, vlm_tokenizer = load_model_and_tokenizer(vlm_path, multimodal=True)\n",
    "\n",
    "clip_vision_model = vlm.vision_encoder.clip_vision_model\n",
    "encoder, image_transforms, _ = load_vision_encoder(\"clip\")\n",
    "\n",
    "#Parse grid\n",
    "def parse_grid(grid_str, K):\n",
    "    \"\"\"\n",
    "    Parse the grid string into a 2D list of grid cells.\n",
    "    \"\"\"\n",
    "    grid_str = '\\n'.join(grid_str.split('\\n')[:K])\n",
    "    rows = grid_str.strip().split('\\n')\n",
    "    return [[cell.strip() for cell in row.split('|') if cell.strip()] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [[0, 1, 2, 7, 8, 9, 14, 15, 16], [2, 3, 4, 9, 10, 11, 16, 17, 18], [4, 5, 6, 11, 12, 13, 18, 19, 20],\n",
    "           [14, 15, 16, 21, 22, 23, 28, 29, 30], [16, 17, 18, 23, 24, 25, 30, 31, 32], [18, 19, 20, 25, 26, 27, 32, 33, 34],\n",
    "           [28, 29, 30, 35, 36, 37, 42, 43, 44], [30, 31, 32, 37, 38, 39, 44, 45, 46], [32, 33, 34, 39, 40, 41, 46, 47, 48]]\n",
    "\n",
    "def position_concat(image_tensor, patch = patches):\n",
    "    \"\"\"\n",
    "    Concats tensors, should return shape 9x1x6912, \n",
    "    1x6912 tensor fore each position in patch, \n",
    "    9 patches\n",
    "    \"\"\"\n",
    "    concat = []\n",
    "    for position in patch:\n",
    "        concat.append(torch.cat([image_tensor[0][i] for i in position]).unsqueeze(0))\n",
    "    \n",
    "    return torch.stack(concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_no_batch(set_type, num_samples):\n",
    "    \"\"\"\n",
    "    Prepares data to feed into model by turning images \n",
    "    into 1 x 9 x 6912 tensors, grid cell information \n",
    "    into 1 x 9 x 10 tensors \n",
    "    \"\"\" \n",
    "    ANIMALS = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder.to(device)\n",
    "\n",
    "    reformatted_data = []\n",
    "    t_0 = time.perf_counter()\n",
    "    \n",
    "    # Extract image tensors and grid cells in each batch\n",
    "    for i in range(num_samples):\n",
    "        image = Image.open(f\"/data/lmm/generated/v3_spatial_grid_multimodal/images/{set_type}_{i}.png\")\n",
    "        image_tensor = image_transforms(image).unsqueeze(0).to(device) \n",
    "        with torch.no_grad():\n",
    "            image_tensor_tokens = encoder(image_tensor)\n",
    "\n",
    "        grid = parse_grid(dataset[set_type][i]['text'], 3)\n",
    "\n",
    "        temp = []\n",
    "        for row in grid:\n",
    "            for animal in row:\n",
    "                #temp_tensor = torch.zeros(1,10)\n",
    "                #temp_tensor[0][ANIMALS.index(animal)] = 1\n",
    "                #temp.append(temp_tensor)\n",
    "                temp.append(ANIMALS.index(animal)) \n",
    "\n",
    "        # Tuple of (image tokens (9 x 1 x 6912), correct labels is list. 1 x 9)\n",
    "        reformatted_data.append((position_concat(image_tensor_tokens), temp)) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i} images\") \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    t_3 = time.perf_counter()\n",
    "    print(f\"Finished preparing data in {t_3 - t_0} seconds\")\n",
    "    return reformatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images\n",
      "Processed 50 images\n",
      "Processed 100 images\n",
      "Processed 150 images\n",
      "Processed 200 images\n",
      "Processed 250 images\n",
      "Processed 300 images\n",
      "Processed 350 images\n",
      "Processed 400 images\n",
      "Processed 450 images\n",
      "Processed 500 images\n",
      "Processed 550 images\n",
      "Processed 600 images\n",
      "Processed 650 images\n",
      "Processed 700 images\n",
      "Processed 750 images\n",
      "Processed 800 images\n",
      "Processed 850 images\n",
      "Processed 900 images\n",
      "Processed 950 images\n",
      "Processed 1000 images\n",
      "Processed 1050 images\n",
      "Processed 1100 images\n",
      "Processed 1150 images\n",
      "Processed 1200 images\n",
      "Processed 1250 images\n",
      "Processed 1300 images\n",
      "Processed 1350 images\n",
      "Processed 1400 images\n",
      "Processed 1450 images\n",
      "Processed 1500 images\n",
      "Processed 1550 images\n",
      "Processed 1600 images\n",
      "Processed 1650 images\n",
      "Processed 1700 images\n",
      "Processed 1750 images\n",
      "Processed 1800 images\n",
      "Processed 1850 images\n",
      "Processed 1900 images\n",
      "Processed 1950 images\n",
      "Processed 2000 images\n",
      "Processed 2050 images\n",
      "Processed 2100 images\n",
      "Processed 2150 images\n",
      "Processed 2200 images\n",
      "Processed 2250 images\n",
      "Processed 2300 images\n",
      "Processed 2350 images\n",
      "Processed 2400 images\n",
      "Processed 2450 images\n",
      "Processed 2500 images\n",
      "Processed 2550 images\n",
      "Processed 2600 images\n",
      "Processed 2650 images\n",
      "Processed 2700 images\n",
      "Processed 2750 images\n",
      "Processed 2800 images\n",
      "Processed 2850 images\n",
      "Processed 2900 images\n",
      "Processed 2950 images\n",
      "Processed 3000 images\n",
      "Processed 3050 images\n",
      "Processed 3100 images\n",
      "Processed 3150 images\n",
      "Processed 3200 images\n",
      "Processed 3250 images\n",
      "Processed 3300 images\n",
      "Processed 3350 images\n",
      "Processed 3400 images\n",
      "Processed 3450 images\n",
      "Processed 3500 images\n",
      "Processed 3550 images\n",
      "Processed 3600 images\n",
      "Processed 3650 images\n",
      "Processed 3700 images\n",
      "Processed 3750 images\n",
      "Processed 3800 images\n",
      "Processed 3850 images\n",
      "Processed 3900 images\n",
      "Processed 3950 images\n",
      "Processed 4000 images\n",
      "Processed 4050 images\n",
      "Processed 4100 images\n",
      "Processed 4150 images\n",
      "Processed 4200 images\n",
      "Processed 4250 images\n",
      "Processed 4300 images\n",
      "Processed 4350 images\n",
      "Processed 4400 images\n",
      "Processed 4450 images\n",
      "Processed 4500 images\n",
      "Processed 4550 images\n",
      "Processed 4600 images\n",
      "Processed 4650 images\n",
      "Processed 4700 images\n",
      "Processed 4750 images\n",
      "Processed 4800 images\n",
      "Processed 4850 images\n",
      "Processed 4900 images\n",
      "Processed 4950 images\n",
      "Processed 5000 images\n",
      "Processed 5050 images\n",
      "Processed 5100 images\n",
      "Processed 5150 images\n",
      "Processed 5200 images\n",
      "Processed 5250 images\n",
      "Processed 5300 images\n",
      "Processed 5350 images\n",
      "Processed 5400 images\n",
      "Processed 5450 images\n",
      "Processed 5500 images\n",
      "Processed 5550 images\n",
      "Processed 5600 images\n",
      "Processed 5650 images\n",
      "Processed 5700 images\n",
      "Processed 5750 images\n",
      "Processed 5800 images\n",
      "Processed 5850 images\n",
      "Processed 5900 images\n",
      "Processed 5950 images\n",
      "Processed 6000 images\n",
      "Processed 6050 images\n",
      "Processed 6100 images\n",
      "Processed 6150 images\n",
      "Processed 6200 images\n",
      "Processed 6250 images\n",
      "Processed 6300 images\n",
      "Processed 6350 images\n",
      "Processed 6400 images\n",
      "Processed 6450 images\n",
      "Processed 6500 images\n",
      "Processed 6550 images\n",
      "Processed 6600 images\n",
      "Processed 6650 images\n",
      "Processed 6700 images\n",
      "Processed 6750 images\n",
      "Processed 6800 images\n",
      "Processed 6850 images\n",
      "Processed 6900 images\n",
      "Processed 6950 images\n",
      "Processed 7000 images\n",
      "Processed 7050 images\n",
      "Processed 7100 images\n",
      "Processed 7150 images\n",
      "Processed 7200 images\n",
      "Processed 7250 images\n",
      "Processed 7300 images\n",
      "Processed 7350 images\n",
      "Processed 7400 images\n",
      "Processed 7450 images\n",
      "Processed 7500 images\n",
      "Processed 7550 images\n",
      "Processed 7600 images\n",
      "Processed 7650 images\n",
      "Processed 7700 images\n",
      "Processed 7750 images\n",
      "Processed 7800 images\n",
      "Processed 7850 images\n",
      "Processed 7900 images\n",
      "Processed 7950 images\n",
      "Processed 8000 images\n",
      "Processed 8050 images\n",
      "Processed 8100 images\n",
      "Processed 8150 images\n",
      "Processed 8200 images\n",
      "Processed 8250 images\n",
      "Processed 8300 images\n",
      "Processed 8350 images\n",
      "Processed 8400 images\n",
      "Processed 8450 images\n",
      "Processed 8500 images\n",
      "Processed 8550 images\n",
      "Processed 8600 images\n",
      "Processed 8650 images\n",
      "Processed 8700 images\n",
      "Processed 8750 images\n",
      "Processed 8800 images\n",
      "Processed 8850 images\n",
      "Processed 8900 images\n",
      "Processed 8950 images\n",
      "Processed 9000 images\n",
      "Processed 9050 images\n",
      "Processed 9100 images\n",
      "Processed 9150 images\n",
      "Processed 9200 images\n",
      "Processed 9250 images\n",
      "Processed 9300 images\n",
      "Processed 9350 images\n",
      "Processed 9400 images\n",
      "Processed 9450 images\n",
      "Processed 9500 images\n",
      "Processed 9550 images\n",
      "Processed 9600 images\n",
      "Processed 9650 images\n",
      "Processed 9700 images\n",
      "Processed 9750 images\n",
      "Processed 9800 images\n",
      "Processed 9850 images\n",
      "Processed 9900 images\n",
      "Processed 9950 images\n",
      "Finished preparing data in 152.2092873826623 seconds\n"
     ]
    }
   ],
   "source": [
    "train_data = prepare_data_no_batch(\"train\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6912])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[9999][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 2, 3, 3, 9, 3, 4, 4]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.concat([train_data[i][0][0] for i in range(10000)], dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6912"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "pos = 0\n",
    "for i in range(10000):\n",
    "    labels.append(train_data[i][1][pos])\n",
    "\n",
    "print(len(labels))\n",
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2 = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.unique(labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbfgs_linear_classifier(position, dataset, num_iterations = 100, lr = 1.0):\n",
    "    \"\"\"\n",
    "    Trains a linear cliassifier using L-BFGS\n",
    "\n",
    "    Args:\n",
    "        position (int): position of the grid cell to train on (0-8)\n",
    "        dataset (list): each tuple should contain 9 x 1 x 6912 tensor and a list with 9 values\n",
    "            1 x 6912 tensor for each position in patch, value corresponding with animal index in position\n",
    "        num_iterations (int): number of iterations for L-BFGS optimization\n",
    "        lr (float): learning rate for L-BFGS optimization\n",
    "\n",
    "    Returns:  \n",
    "        model (nn.Module): trained linear classifier\n",
    "        log_losses (list): list of losses at each iteration\n",
    "    \"\"\"\n",
    "    # Initialize data and labels \n",
    "    data = torch.concat([dataset[i][0][position] for i in range(len(dataset))], dim = 0)\n",
    "    temp_label = []\n",
    "    for i in range(len(dataset)):\n",
    "        temp_label.append(dataset[i][1][position])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    labels = torch.tensor(temp_label).to(device)\n",
    "\n",
    "    # Check inputs\n",
    "    if len(data) == len(labels) and data.dim() == 2 and labels.dim() == 1:\n",
    "        print(\"Data is formatted correctly\")\n",
    "    else:\n",
    "        print(\"Look over format of dataset\")\n",
    "\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "\n",
    "\n",
    "    class LinearClassifier(nn.Module):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super(LinearClassifier, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "    \n",
    "    input_dim = data.shape[1]\n",
    "    print(input_dim)\n",
    "    num_classes = len(torch.unique(labels))\n",
    "    print(num_classes)\n",
    "\n",
    "    model = LinearClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "    # Use cross entropy loss for classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr = lr, max_iter = num_iterations)\n",
    "\n",
    "    log_losses = []\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        log_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "\n",
    "    print(\"Training complete. Final loss: {:.4f}\".format(log_losses[-1]))\n",
    "    return model, log_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is formatted correctly\n",
      "Data shape: torch.Size([10000, 6912])\n",
      "Labels shape: torch.Size([10000])\n",
      "6912\n",
      "10\n",
      "Training complete. Final loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "linear0, linear0_log = train_lbfgs_linear_classifier(0, train_data, num_iterations = 1000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = train_data[0][0][0]\n",
    "input.shape\n",
    "input.to(DEVICE)\n",
    "output = linear0(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 2, 4, 3, 3, 2, 5, 2]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3596, -0.9070, -1.2077, -1.2198, 11.8554, -1.0516, -1.8016, -1.0037,\n",
       "         -1.8169, -1.5476]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
