{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2024-12-19 14:25:27.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model_and_tokenizer\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mLoading model and tokenizer from /home/allanz/data/vlm_checkpoint/final_model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'prompt', 'conversations', 'image'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-19 14:25:29.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-19 14:25:29.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n",
      "/home/allanz/miniconda3/envs/vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-12-19 14:25:29.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_multimodal_projector\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mLoading multimodal projector: linear\u001b[0m\n",
      "\u001b[32m2024-12-19 14:25:29.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading vision encoder: clip\u001b[0m\n",
      "\u001b[32m2024-12-19 14:25:29.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlmm_synthetic.mm_train.utils\u001b[0m:\u001b[36mload_vision_encoder\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mUsing CLIP model as the vision encoder\u001b[0m\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_from_disk\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, GPTJForCausalLM\n",
    "from lmm_synthetic.mm_train.gptj_vlm import GPTJ_VLM\n",
    "from lmm_synthetic.mm_train.utils import load_vision_encoder\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths \n",
    "dataset_path = '/home/allanz/data/datasets/v3.1_spatial_grid_multimodal'\n",
    "vlm_path = '/home/allanz/data/vlm_checkpoint/final_model'\n",
    "lm_path = \"/data/lmm/checkpoints/lm/lm-pretrain-only-checkpoint-1953\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(dataset)\n",
    "\n",
    "# Load VLM and CLIP model\n",
    "def load_model_and_tokenizer(model_path, multimodal=False):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from the specified path.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model and tokenizer from {model_path}\")\n",
    "    model, tokenizer = None, None\n",
    "    if multimodal:\n",
    "        model = GPTJ_VLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model.config.pretrained_lm_path)\n",
    "    else:\n",
    "        model = GPTJForCausalLM.from_pretrained(model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "vlm, vlm_tokenizer = load_model_and_tokenizer(vlm_path, multimodal=True)\n",
    "\n",
    "clip_vision_model = vlm.vision_encoder.clip_vision_model\n",
    "encoder, image_transforms, _ = load_vision_encoder(\"clip\")\n",
    "\n",
    "#Parse grid\n",
    "def parse_grid(grid_str, K):\n",
    "    \"\"\"\n",
    "    Parse the grid string into a 2D list of grid cells.\n",
    "    \"\"\"\n",
    "    grid_str = '\\n'.join(grid_str.split('\\n')[:K])\n",
    "    rows = grid_str.strip().split('\\n')\n",
    "    return [[cell.strip() for cell in row.split('|') if cell.strip()] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [[0, 1, 2, 7, 8, 9, 14, 15, 16], [2, 3, 4, 9, 10, 11, 16, 17, 18], [4, 5, 6, 11, 12, 13, 18, 19, 20],\n",
    "           [14, 15, 16, 21, 22, 23, 28, 29, 30], [16, 17, 18, 23, 24, 25, 30, 31, 32], [18, 19, 20, 25, 26, 27, 32, 33, 34],\n",
    "           [28, 29, 30, 35, 36, 37, 42, 43, 44], [30, 31, 32, 37, 38, 39, 44, 45, 46], [32, 33, 34, 39, 40, 41, 46, 47, 48]]\n",
    "\n",
    "def position_concat(image_tensor, patch = patches):\n",
    "    \"\"\"\n",
    "    Concats tensors, should return shape 9x1x6912, \n",
    "    1x6912 tensor fore each position in patch, \n",
    "    9 patches\n",
    "    \"\"\"\n",
    "    concat = []\n",
    "    for position in patch:\n",
    "        concat.append(torch.cat([image_tensor[0][i] for i in position]).unsqueeze(0))\n",
    "    \n",
    "    return torch.stack(concat)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
   "metadata": {},
   "outputs": [],
   "source": [
    "#refactor \n",
    "def prepare_data_no_batch(set_type, num_samples, prints = False):\n",
    "    \"\"\"\n",
    "    Prepares data to feed into model by turning images \n",
    "    into 1 x 9 x 6912 tensors, grid cell information \n",
    "    into 1 x 9 x 10 tensors \n",
    "    \"\"\" \n",
    "    ANIMALS = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    encoder.to(DEVICE)\n",
    "\n",
    "    reformatted_data = []\n",
    "    t_0 = time.perf_counter()\n",
    "    \n",
    "    # Extract image tensors and grid cells in each batch\n",
    "    for i in range(num_samples):\n",
    "        image = Image.open(dataset[set_type][i]['image'])\n",
    "        image_tensor = image_transforms(image).unsqueeze(0).to(DEVICE) \n",
    "        with torch.no_grad():\n",
    "            image_tensor_tokens = encoder(image_tensor)\n",
    "\n",
    "        grid = parse_grid(dataset[set_type][i]['text'], 3)\n",
    "        \n",
    "        temp = []\n",
    "        for row in grid:\n",
    "            for animal in row:\n",
    "                #temp_tensor = torch.zeros(1,10)\n",
    "                #temp_tensor[0][ANIMALS.index(animal)] = 1\n",
    "                #temp.append(temp_tensor)\n",
    "                temp.append(ANIMALS.index(animal)) \n",
    "\n",
    "        # Tuple of (image tokens (9 x 1 x 6912), correct labels is list. 1 x 9)\n",
    "        reformatted_data.append((position_concat(image_tensor_tokens), temp)) \n",
    "\n",
    "        if i % 200 == 0 and prints == True:\n",
    "            print(f\"Processed {i} images\") \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    t_3 = time.perf_counter()\n",
    "    print(f\"Finished preparing data in {t_3 - t_0} seconds\")\n",
    "    return reformatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing data in 676.7621881328523 seconds\n"
     ]
    }
   ],
   "source": [
    "train_data = prepare_data_no_batch(\"train\", 50000)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][0][0].shape)\n",
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 7,
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbfgs_linear_classifier(position, dataset, num_iterations = 100, lr = 1.0):\n",
    "    \"\"\"\n",
    "    Trains a linear cliassifier using L-BFGS\n",
    "\n",
    "    Args:\n",
    "        position (int): position of the grid cell to train on (0-8)\n",
    "        dataset (list): each tuple should contain 9 x 1 x 6912 tensor and a list with 9 values\n",
    "            1 x 6912 tensor for each position in patch, value corresponding with animal index in position\n",
    "        num_iterations (int): number of iterations for L-BFGS optimization\n",
    "        lr (float): learning rate for L-BFGS optimization\n",
    "\n",
    "    Returns:  \n",
    "        model (nn.Module): trained linear classifier\n",
    "        log_losses (list): list of losses at each iteration\n",
    "    \"\"\"\n",
    "    # Initialize data and labels \n",
    "    data = torch.concat([dataset[i][0][position] for i in range(len(dataset))], dim = 0)\n",
    "    temp_label = []\n",
    "    for i in range(len(dataset)):\n",
    "        temp_label.append(dataset[i][1][position])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    labels = torch.tensor(temp_label).to(DEVICE)\n",
    "\n",
    "    # Check inputs\n",
    "    if len(data) == len(labels) and data.dim() == 2 and labels.dim() == 1:\n",
    "        print(\"Data is formatted correctly\")\n",
    "    else:\n",
    "        print(\"Look over format of dataset\")\n",
    "\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "\n",
    "\n",
    "    class LinearClassifier(nn.Module):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super(LinearClassifier, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "    \n",
    "    input_dim = data.shape[1]\n",
    "    print(input_dim)\n",
    "    num_classes = len(torch.unique(labels))\n",
    "    print(num_classes)\n",
    "\n",
    "    model = LinearClassifier(input_dim, num_classes).to(DEVICE)\n",
    "\n",
    "    # Use cross entropy loss for classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(), lr = lr, max_iter = num_iterations)\n",
    "\n",
    "    log_losses = []\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        log_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "\n",
    "    print(f\"Training complete for position {position}. Final loss: {log_losses[-1]:.4f}\")\n",
    "    return model, log_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 0. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 1. Final loss: 0.0124\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 2. Final loss: 0.0375\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 3. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 4. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 5. Final loss: 0.0000\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 6. Final loss: 0.0001\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 7. Final loss: 0.0450\n",
      "Data is formatted correctly\n",
      "Data shape: torch.Size([50000, 6912])\n",
      "Labels shape: torch.Size([50000])\n",
      "6912\n",
      "10\n",
      "Training complete for position 8. Final loss: 0.0877\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
   "source": [
    "linear0, linear0_log = train_lbfgs_linear_classifier(0, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear1, linear1_log = train_lbfgs_linear_classifier(1, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear2, linear2_log = train_lbfgs_linear_classifier(2, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear3, linear3_log = train_lbfgs_linear_classifier(3, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear4, linear4_log = train_lbfgs_linear_classifier(4, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear5, linear5_log = train_lbfgs_linear_classifier(5, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear6, linear6_log = train_lbfgs_linear_classifier(6, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear7, linear7_log = train_lbfgs_linear_classifier(7, train_data, num_iterations = 100, lr = 1.0)\n",
    "linear8, linear8_log = train_lbfgs_linear_classifier(8, train_data, num_iterations = 100, lr = 1.0)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 9,
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {0: linear0, 1: linear1, 2: linear2, 3: linear3, 4: linear4, 5: linear5, 6: linear6, 7: linear7, 8: linear8}\n",
    "\n",
    "def accuracy(position, dataset, num_samples = 1000, premodel = None):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model on the either test or validation set \n",
    "\n",
    "    Args:\n",
    "        position (int): position of the grid cell to train on (0-8)\n",
    "        dataset (str): either \"test\" or \"validation\"\n",
    "        num_samples (int): number of samples to calculate accuracy on\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): accuracy of the model on the dataset\n",
    "        lowest (str): worst performing class\n",
    "    \"\"\"\n",
    "    if premodel is None:\n",
    "        model = models[position]\n",
    "    else:\n",
    "        model = premodel\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    data = prepare_data_no_batch(dataset, num_samples)\n",
    "\n",
    "    cleaned_data = torch.concat([data[i][0][position] for i in range(len(data))], dim = 0)\n",
    "    temp_label = []\n",
    "    for i in range(len(data)):\n",
    "        temp_label.append(data[i][1][position])\n",
    "\n",
    "    labels = torch.tensor(temp_label).to(DEVICE)\n",
    "\n",
    "    incorrect = 0 \n",
    "    total = 0\n",
    "\n",
    "    incorrect_guesses = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        with torch.no_grad():\n",
    "            output = model(cleaned_data[i])\n",
    "            prediction = torch.argmax(output)\n",
    "            if prediction != labels[i]:\n",
    "                incorrect += 1\n",
    "                incorrect_guesses.append((prediction, labels[i]))\n",
    "            total += 1\n",
    "    \n",
    "    print(f\"Accuracy for position {position} on {dataset}: {1 - incorrect/total:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing data in 13.220105424523354 seconds\n",
      "Accuracy for position 0 on test: 0.9040\n",
      "Finished preparing data in 13.47236692532897 seconds\n",
      "Accuracy for position 0 on validation: 0.9240\n",
      "Finished preparing data in 13.048816934227943 seconds\n",
      "Accuracy for position 1 on test: 0.8760\n",
      "Finished preparing data in 14.871745135635138 seconds\n",
      "Accuracy for position 1 on validation: 0.8600\n",
      "Finished preparing data in 14.855614647269249 seconds\n",
      "Accuracy for position 2 on test: 0.8560\n",
      "Finished preparing data in 13.326650962233543 seconds\n",
      "Accuracy for position 2 on validation: 0.8460\n",
      "Finished preparing data in 12.713330529630184 seconds\n",
      "Accuracy for position 3 on test: 0.9070\n",
      "Finished preparing data in 14.121667683124542 seconds\n",
      "Accuracy for position 3 on validation: 0.9070\n",
      "Finished preparing data in 15.344670206308365 seconds\n",
      "Accuracy for position 4 on test: 0.9070\n",
      "Finished preparing data in 13.490759152919054 seconds\n",
      "Accuracy for position 4 on validation: 0.9020\n",
      "Finished preparing data in 15.113344579935074 seconds\n",
      "Accuracy for position 5 on test: 0.8440\n",
      "Finished preparing data in 16.071187861263752 seconds\n",
      "Accuracy for position 5 on validation: 0.8380\n",
      "Finished preparing data in 14.71763477101922 seconds\n",
      "Accuracy for position 6 on test: 0.8960\n",
      "Finished preparing data in 15.527488477528095 seconds\n",
      "Accuracy for position 6 on validation: 0.8850\n",
      "Finished preparing data in 14.412609539926052 seconds\n",
      "Accuracy for position 7 on test: 0.8710\n",
      "Finished preparing data in 14.765931744128466 seconds\n",
      "Accuracy for position 7 on validation: 0.8590\n",
      "Finished preparing data in 14.592193014919758 seconds\n",
      "Accuracy for position 8 on test: 0.8440\n",
      "Finished preparing data in 13.279693935066462 seconds\n",
      "Accuracy for position 8 on validation: 0.8210\n"
=======
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[1;32m      2\u001b[0m     accuracy(i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(position, dataset, num_samples, premodel)\u001b[0m\n\u001b[1;32m     19\u001b[0m     model \u001b[38;5;241m=\u001b[39m premodel\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 22\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_no_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([data[i][\u001b[38;5;241m0\u001b[39m][position] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m temp_label \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mprepare_data_no_batch\u001b[0;34m(set_type, num_samples, prints)\u001b[0m\n\u001b[1;32m     18\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m image_transforms(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     image_tensor_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m grid \u001b[38;5;241m=\u001b[39m parse_grid(dataset[set_type][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     24\u001b[0m temp \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/lmm/lmm_synthetic/mm_train/utils.py:47\u001b[0m, in \u001b[0;36mload_vision_encoder.<locals>.VisionEncoderWrapper.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Expect images to be pre-transformed and batched [batch_size, 3, H, W]\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_vision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vision_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:844\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    841\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    842\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 844\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    852\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:630\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    622\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    623\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    624\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m         output_attentions,\n\u001b[1;32m    628\u001b[0m     )\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:372\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    369\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    371\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 372\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    380\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vlm/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:273\u001b[0m, in \u001b[0;36mCLIPAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    270\u001b[0m src_len \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    271\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len):\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m     )\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# apply the causal_attention_mask first\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 13b00457ed85af332ab8df289bd3f5cfee91de72
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    accuracy(i, \"test\", 1000)\n",
    "    accuracy(i, \"validation\", 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
