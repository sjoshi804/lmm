{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works\n",
      "Using renderer: notebook_connected\n"
     ]
    }
   ],
   "source": [
    "#Set up stuff from notebook\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "DEVELOPMENT_MODE = False\n",
    "# Detect if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install if in Colab\n",
    "if IN_COLAB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    # Install a faster Node version\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
    "\n",
    "# Hot reload in development mode & not running on the CD\n",
    "if not IN_COLAB:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if not ip.extension_manager.loaded:\n",
    "        ip.extension_manager.load('autoreload')\n",
    "        %autoreload 2\n",
    "\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "print(\"works\")\n",
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")\n",
    "\n",
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model, show_param = False):\n",
    "    count = 0\n",
    "    parameters = []\n",
    "    names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if show_param == True:\n",
    "            print(name, param.size())\n",
    "        names.append([name, param.size()])\n",
    "        parameters.append(param.data)\n",
    "        count += 1\n",
    "    print(f\"Model has {count} named parameters\")\n",
    "    return parameters, names\n",
    "\n",
    "def unpack_names(names, first = 0, last = None):\n",
    "    if last == None:\n",
    "        for item in names[first:len(names)-1]:\n",
    "            print(f\"Name of param: {item[0]:<30} Dimensions of tensor: {item[1]:<30}\")\n",
    "    else:\n",
    "        for item in names[first: last]:\n",
    "            print(f\"Name of param: {item[0]:<30} Dimensions of tensor: {str(item[1]):<30}\")\n",
    "\n",
    "def compare(model_1, model_2):\n",
    "    different = []\n",
    "    model_1_params, model_1_names = get_params(model_1)\n",
    "    model_2_params, model_2_names = get_params(model_2)\n",
    "\n",
    "    if len(model_1_params) != len(model_2_params):\n",
    "        print(\"There are a different number of parameters between the models. Please check model config\")\n",
    "        print(f\"Model 1 has {len(model_1_params)} parameters. Model 2 has {len(model_2_params)}\")\n",
    "\n",
    "    for i in range(min(len(model_1_params),(len(model_2_params)))):\n",
    "        if model_1_names[i][1] != model_2_names[i][1]:\n",
    "            different.append([model_1_names[i], model_2_names[i], (\"index\", i)])\n",
    "\n",
    "    print(f\"There are {len(different)} parameters that are different from each other\")\n",
    "    return different \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hooked_model = HookedTransformer.from_pretrained(\"gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 148 named parameters\n"
     ]
    }
   ],
   "source": [
    "hooked_params, hooked_names = get_params(hooked_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of param: embed.W_E                      Dimensions of tensor: torch.Size([50257, 768])      \n",
      "Name of param: pos_embed.W_pos                Dimensions of tensor: torch.Size([1024, 768])       \n",
      "Name of param: blocks.0.attn.W_Q              Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Name of param: blocks.0.attn.W_O              Dimensions of tensor: torch.Size([12, 64, 768])     \n",
      "Name of param: blocks.0.attn.b_Q              Dimensions of tensor: torch.Size([12, 64])          \n",
      "Name of param: blocks.0.attn.b_O              Dimensions of tensor: torch.Size([768])             \n",
      "Name of param: blocks.0.attn.W_K              Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Name of param: blocks.0.attn.W_V              Dimensions of tensor: torch.Size([12, 768, 64])     \n",
      "Name of param: blocks.0.attn.b_K              Dimensions of tensor: torch.Size([12, 64])          \n",
      "Name of param: blocks.0.attn.b_V              Dimensions of tensor: torch.Size([12, 64])          \n"
     ]
    }
   ],
   "source": [
    "unpack_names(hooked_names, first = 0, last = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 148 named parameters\n"
     ]
    }
   ],
   "source": [
    "hooked_param, hooked_names = get_params(hooked_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 148 named parameters\n"
     ]
    }
   ],
   "source": [
    "gpt2_params, gpt2_names = get_params(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of param: wte.weight                     Dimensions of tensor: torch.Size([50257, 768])      \n",
      "Name of param: wpe.weight                     Dimensions of tensor: torch.Size([1024, 768])       \n",
      "Name of param: h.0.ln_1.weight                Dimensions of tensor: torch.Size([768])             \n",
      "Name of param: h.0.ln_1.bias                  Dimensions of tensor: torch.Size([768])             \n",
      "Name of param: h.0.attn.c_attn.weight         Dimensions of tensor: torch.Size([768, 2304])       \n",
      "Name of param: h.0.attn.c_attn.bias           Dimensions of tensor: torch.Size([2304])            \n",
      "Name of param: h.0.attn.c_proj.weight         Dimensions of tensor: torch.Size([768, 768])        \n",
      "Name of param: h.0.attn.c_proj.bias           Dimensions of tensor: torch.Size([768])             \n",
      "Name of param: h.0.ln_2.weight                Dimensions of tensor: torch.Size([768])             \n",
      "Name of param: h.0.ln_2.bias                  Dimensions of tensor: torch.Size([768])             \n"
     ]
    }
   ],
   "source": [
    "gpt2_unpacked = unpack_names(gpt2_names, first = 0, last = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 148 named parameters\n",
      "Model has 148 named parameters\n",
      "There are 98 parameters that are different from each other\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['blocks.0.attn.W_Q', torch.Size([12, 768, 64])],\n",
       "  ['h.0.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 2)],\n",
       " [['blocks.0.attn.W_O', torch.Size([12, 64, 768])],\n",
       "  ['h.0.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 3)],\n",
       " [['blocks.0.attn.b_Q', torch.Size([12, 64])],\n",
       "  ['h.0.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ('index', 4)],\n",
       " [['blocks.0.attn.b_O', torch.Size([768])],\n",
       "  ['h.0.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ('index', 5)],\n",
       " [['blocks.0.attn.W_K', torch.Size([12, 768, 64])],\n",
       "  ['h.0.attn.c_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 6)]]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = compare(hooked_model, gpt2)\n",
    "\n",
    "diff[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1026, -0.0609, -0.0249,  ...,  0.0203,  0.0658,  0.0478],\n",
       "         [ 0.0191,  0.0242,  0.0404,  ..., -0.0670,  0.0618, -0.0190],\n",
       "         [ 0.0038,  0.0081,  0.0532,  ..., -0.0380,  0.0209,  0.0367],\n",
       "         ...,\n",
       "         [-0.0461, -0.0057,  0.0348,  ...,  0.0152, -0.0104,  0.0085],\n",
       "         [ 0.0309,  0.0370,  0.0160,  ..., -0.0175, -0.0892, -0.0513],\n",
       "         [-0.0747, -0.0391, -0.0486,  ..., -0.0675, -0.0310, -0.0416]],\n",
       "\n",
       "        [[-0.0146,  0.0116, -0.0336,  ..., -0.0270, -0.0309, -0.0234],\n",
       "         [-0.0148,  0.0689,  0.0947,  ...,  0.0179,  0.0131, -0.0445],\n",
       "         [ 0.0237,  0.0168, -0.0566,  ..., -0.0101,  0.0308,  0.0022],\n",
       "         ...,\n",
       "         [ 0.0526,  0.0214, -0.0353,  ..., -0.0663, -0.0429,  0.1076],\n",
       "         [-0.0119,  0.0048, -0.1068,  ...,  0.0063, -0.0678, -0.0890],\n",
       "         [-0.1548,  0.0178, -0.0272,  ..., -0.0034,  0.0690, -0.0079]],\n",
       "\n",
       "        [[ 0.0057,  0.0382,  0.0539,  ...,  0.0343,  0.1000, -0.0263],\n",
       "         [ 0.0242,  0.0208, -0.0582,  ..., -0.0546, -0.0168, -0.0646],\n",
       "         [ 0.0085, -0.0141, -0.0067,  ...,  0.0384, -0.0215, -0.0236],\n",
       "         ...,\n",
       "         [-0.0273, -0.0292, -0.0250,  ...,  0.0247, -0.0160,  0.0393],\n",
       "         [-0.0159, -0.0655, -0.0124,  ..., -0.0081,  0.0238,  0.0312],\n",
       "         [ 0.0520,  0.0244, -0.0172,  ...,  0.0518, -0.0092,  0.0013]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0100, -0.0167, -0.0069,  ...,  0.0381, -0.0421,  0.0290],\n",
       "         [-0.0601,  0.0361,  0.0027,  ..., -0.0408, -0.0452, -0.0233],\n",
       "         [-0.0398, -0.0164,  0.0053,  ..., -0.0206,  0.0110, -0.0119],\n",
       "         ...,\n",
       "         [ 0.0216,  0.0412, -0.0059,  ...,  0.0007,  0.0252, -0.0308],\n",
       "         [-0.0174,  0.0140, -0.0395,  ..., -0.0135, -0.0395, -0.0259],\n",
       "         [ 0.0148, -0.0191, -0.0297,  ..., -0.0008,  0.0098, -0.0586]],\n",
       "\n",
       "        [[ 0.0118,  0.0247, -0.0151,  ...,  0.0283,  0.0145,  0.0276],\n",
       "         [-0.0398,  0.0142,  0.0400,  ...,  0.0124,  0.0240,  0.0005],\n",
       "         [-0.0023,  0.0069, -0.0160,  ...,  0.0113,  0.0185, -0.0260],\n",
       "         ...,\n",
       "         [-0.0074,  0.0702,  0.0375,  ..., -0.0193, -0.0392,  0.0095],\n",
       "         [-0.0020,  0.0241, -0.0167,  ..., -0.0143, -0.0036, -0.0352],\n",
       "         [ 0.0213, -0.0072, -0.0072,  ...,  0.0319,  0.0017,  0.0126]],\n",
       "\n",
       "        [[ 0.0214, -0.0031,  0.0310,  ...,  0.0735, -0.0131, -0.0515],\n",
       "         [ 0.0134,  0.0305,  0.0397,  ..., -0.0127, -0.0294,  0.0258],\n",
       "         [ 0.0478, -0.0044,  0.0065,  ..., -0.0177, -0.0278, -0.0090],\n",
       "         ...,\n",
       "         [-0.0270,  0.0007, -0.0131,  ..., -0.0051,  0.0254,  0.0047],\n",
       "         [ 0.0223, -0.0044, -0.0340,  ...,  0.0164, -0.0120, -0.0029],\n",
       "         [-0.0123, -0.0020, -0.0119,  ...,  0.0425,  0.0256, -0.0122]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_model.blocks[0].attn.W_Q.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2232, 0.1820, 0.1534, 0.1917, 0.2036, 0.1948, 0.1467, 0.1865, 0.2143,\n",
       "        0.1956, 0.2118, 0.2153, 0.1882, 0.2074, 0.1871, 0.2040, 0.2044, 0.1900,\n",
       "        0.1952, 0.0475, 0.1909, 0.2115, 0.1971, 0.2202, 0.1998, 0.2108, 0.2303,\n",
       "        0.1879, 0.1939, 0.2018, 0.1891, 0.1861, 0.1958, 0.1832, 0.1978, 0.2243,\n",
       "        0.0706, 0.1958, 0.1943, 0.1939, 0.1978, 0.1951, 0.1995, 0.1912, 0.2083,\n",
       "        0.2037, 0.1849, 0.1945, 0.2189, 0.0419, 0.1977, 0.1979, 0.0608, 0.1824,\n",
       "        0.2055, 0.0476, 0.1892, 0.2079, 0.2047, 0.2233, 0.2097, 0.2075, 0.2076,\n",
       "        0.1793, 0.1312, 0.1841, 0.1939, 0.1561, 0.0577, 0.1948, 0.2048, 0.1717,\n",
       "        0.1942, 0.1708, 0.1989, 0.1993, 0.2082, 0.1071, 0.1968, 0.1770, 0.2164,\n",
       "        0.1864, 0.1938, 0.2184, 0.1343, 0.1707, 0.0683, 0.1401, 0.1823, 0.2045,\n",
       "        0.2007, 0.1853, 0.1783, 0.1889, 0.1870, 0.1975, 0.2114, 0.2108, 0.2083,\n",
       "        0.2409, 0.1938, 0.2022, 0.0857, 0.1823, 0.1879, 0.1979, 0.1850, 0.1029,\n",
       "        0.1762, 0.1953, 0.2231, 0.2006, 0.2022, 0.2134, 0.1970, 0.1820, 0.0568,\n",
       "        0.2269, 0.1882, 0.1770, 0.1880, 0.1910, 0.1872, 0.1613, 0.1946, 0.1930,\n",
       "        0.1981, 0.2030, 0.1848, 0.2341, 0.1832, 0.1893, 0.2368, 0.2085, 0.1833,\n",
       "        0.2083, 0.2009, 0.2212, 0.1342, 0.0614, 0.1913, 0.1812, 0.1041, 0.1957,\n",
       "        0.1902, 0.1355, 0.2145, 0.1974, 0.1904, 0.1997, 0.1849, 0.1776, 0.2038,\n",
       "        0.1773, 0.1878, 0.1793, 0.1960, 0.1935, 0.1786, 0.1532, 0.1185, 0.2015,\n",
       "        0.1907, 0.2112, 0.1967, 0.2037, 0.1994, 0.0528, 0.1832, 0.1633, 0.1812,\n",
       "        0.1988, 0.1742, 0.2177, 0.1901, 0.1778, 0.0706, 0.1987, 0.2417, 0.1658,\n",
       "        0.1840, 0.1763, 0.1950, 0.2085, 0.1906, 0.2025, 0.1713, 0.2475, 0.1939,\n",
       "        0.1755, 0.1929, 0.1378, 0.1944, 0.1803, 0.1839, 0.1617, 0.1919, 0.1710,\n",
       "        0.1861, 0.1589, 0.2092, 0.2252, 0.1949, 0.2080, 0.1775, 0.1984, 0.1842,\n",
       "        0.1919, 0.2261, 0.1953, 0.1940, 0.2496, 0.2153, 0.0501, 0.1797, 0.2050,\n",
       "        0.2279, 0.1993, 0.2056, 0.2081, 0.1783, 0.1789, 0.1948, 0.1909, 0.1657,\n",
       "        0.1894, 0.1901, 0.1939, 0.1997, 0.1939, 0.1790, 0.2071, 0.1217, 0.1811,\n",
       "        0.1743, 0.2202, 0.1910, 0.1884, 0.2246, 0.1929, 0.2057, 0.1751, 0.1968,\n",
       "        0.1799, 0.1807, 0.1856, 0.1968, 0.2126, 0.1744, 0.2287, 0.1813, 0.2130,\n",
       "        0.2163, 0.2274, 0.1855, 0.1663, 0.1714, 0.1806, 0.1870, 0.2162, 0.1918,\n",
       "        0.1621, 0.1802, 0.2120, 0.1645, 0.2075, 0.1612, 0.1757, 0.1968, 0.2067,\n",
       "        0.0430, 0.0715, 0.1992, 0.1713, 0.2130, 0.2078, 0.0492, 0.1846, 0.2049,\n",
       "        0.1995, 0.1914, 0.1975, 0.1758, 0.2057, 0.1663, 0.2204, 0.2045, 0.1877,\n",
       "        0.0608, 0.0551, 0.2212, 0.1949, 0.1891, 0.2025, 0.1979, 0.1851, 0.1910,\n",
       "        0.1713, 0.1884, 0.1987, 0.0643, 0.1914, 0.2395, 0.1831, 0.1902, 0.1741,\n",
       "        0.1919, 0.1812, 0.0681, 0.2024, 0.1959, 0.0526, 0.1893, 0.2065, 0.0969,\n",
       "        0.1988, 0.1940, 0.1956, 0.2085, 0.2012, 0.0678, 0.1812, 0.1821, 0.1736,\n",
       "        0.1892, 0.1933, 0.0839, 0.1738, 0.2093, 0.1908, 0.1714, 0.1975, 0.2014,\n",
       "        0.1891, 0.1953, 0.2019, 0.2165, 0.1870, 0.1935, 0.2164, 0.1846, 0.1841,\n",
       "        0.1762, 0.2349, 0.1905, 0.1667, 0.1910, 0.2093, 0.1944, 0.2072, 0.2027,\n",
       "        0.0504, 0.1939, 0.2013, 0.1845, 0.1919, 0.0686, 0.1734, 0.1742, 0.1937,\n",
       "        0.2194, 0.0626, 0.0836, 0.1880, 0.1772, 0.0583, 0.2104, 0.1748, 0.1763,\n",
       "        0.1865, 0.2027, 0.1951, 0.2061, 0.1503, 0.0895, 0.1831, 0.1987, 0.0482,\n",
       "        0.1990, 0.2252, 0.2008, 0.1842, 0.1812, 0.2108, 0.2153, 0.1854, 0.2347,\n",
       "        0.1963, 0.2036, 0.1302, 0.2048, 0.2017, 0.2270, 0.1640, 0.1787, 0.1707,\n",
       "        0.2168, 0.1831, 0.1928, 0.1789, 0.1783, 0.1881, 0.0647, 0.1870, 0.1860,\n",
       "        0.1690, 0.1924, 0.1874, 0.0577, 0.1904, 0.1991, 0.1979, 0.2137, 0.1969,\n",
       "        0.2312, 0.2329, 0.2039, 0.2342, 0.2162, 0.1860, 0.0571, 0.2367, 0.2002,\n",
       "        0.1645, 0.1950, 0.1864, 0.1786, 0.1941, 0.1652, 0.1923, 0.0941, 0.1935,\n",
       "        0.1858, 0.1917, 0.1904, 0.1812, 0.1970, 0.1902, 0.2242, 0.0453, 0.1761,\n",
       "        0.1957, 0.1196, 0.2123, 0.2282, 0.1851, 0.1870, 0.1732, 0.1953, 0.1897,\n",
       "        0.2083, 0.2125, 0.1858, 0.0535, 0.1648, 0.0619, 0.1551, 0.2008, 0.1811,\n",
       "        0.0545, 0.2079, 0.2315, 0.1818, 0.2017, 0.2527, 0.2056, 0.1843, 0.1974,\n",
       "        0.1881, 0.1583, 0.1754, 0.1782, 0.2075, 0.1854, 0.1876, 0.2021, 0.1741,\n",
       "        0.1988, 0.1639, 0.0706, 0.1697, 0.1536, 0.1837, 0.1958, 0.2207, 0.1851,\n",
       "        0.2083, 0.1908, 0.1790, 0.1866, 0.1981, 0.2217, 0.1850, 0.2018, 0.1804,\n",
       "        0.1811, 0.0897, 0.0504, 0.1903, 0.1849, 0.1886, 0.1822, 0.2249, 0.0515,\n",
       "        0.2133, 0.1970, 0.1880, 0.1645, 0.2115, 0.2060, 0.1685, 0.0671, 0.1452,\n",
       "        0.1879, 0.2007, 0.2288, 0.1855, 0.1356, 0.2140, 0.1950, 0.1842, 0.1831,\n",
       "        0.1929, 0.2049, 0.1987, 0.0560, 0.1442, 0.0772, 0.0702, 0.1894, 0.1527,\n",
       "        0.1880, 0.1961, 0.1884, 0.1899, 0.2223, 0.1764, 0.2137, 0.2381, 0.1812,\n",
       "        0.0716, 0.2010, 0.2064, 0.1348, 0.1861, 0.1878, 0.1995, 0.1821, 0.1721,\n",
       "        0.1723, 0.1858, 0.0688, 0.1868, 0.2088, 0.0535, 0.1821, 0.2078, 0.1963,\n",
       "        0.2006, 0.1939, 0.1900, 0.1911, 0.1919, 0.1931, 0.1833, 0.2168, 0.1037,\n",
       "        0.1767, 0.2095, 0.2026, 0.1883, 0.2183, 0.1596, 0.1794, 0.1921, 0.2233,\n",
       "        0.1810, 0.2124, 0.2177, 0.1778, 0.1906, 0.1173, 0.2197, 0.1997, 0.2035,\n",
       "        0.1987, 0.1990, 0.2253, 0.1719, 0.1909, 0.1948, 0.1862, 0.1891, 0.2097,\n",
       "        0.1701, 0.2036, 0.1947, 0.1861, 0.1945, 0.1988, 0.1749, 0.2077, 0.1736,\n",
       "        0.1737, 0.1986, 0.1911, 0.2105, 0.1889, 0.0738, 0.1929, 0.1940, 0.1841,\n",
       "        0.1855, 0.1835, 0.1813, 0.1406, 0.1530, 0.1979, 0.1714, 0.1960, 0.1860,\n",
       "        0.1949, 0.1453, 0.0617, 0.2033, 0.1796, 0.1870, 0.0911, 0.1966, 0.1989,\n",
       "        0.1957, 0.1977, 0.1685, 0.1876, 0.2109, 0.1345, 0.1739, 0.1812, 0.1926,\n",
       "        0.2075, 0.1283, 0.1852, 0.2235, 0.1659, 0.1813, 0.1904, 0.1695, 0.2283,\n",
       "        0.1757, 0.1257, 0.1890, 0.2510, 0.2075, 0.2020, 0.1907, 0.0488, 0.1909,\n",
       "        0.2222, 0.1852, 0.1104, 0.1842, 0.1834, 0.1956, 0.2032, 0.1930, 0.1589,\n",
       "        0.1968, 0.1738, 0.1488, 0.1451, 0.0612, 0.1753, 0.1900, 0.2045, 0.0680,\n",
       "        0.1960, 0.1844, 0.1951, 0.1602, 0.0764, 0.1589, 0.1931, 0.1980, 0.1960,\n",
       "        0.1934, 0.2310, 0.1961, 0.1950, 0.1968, 0.1938, 0.1951, 0.1818, 0.1880,\n",
       "        0.1713, 0.1785, 0.1962, 0.1850, 0.1964, 0.2008, 0.0666, 0.1917, 0.1670,\n",
       "        0.2063, 0.1179, 0.1951, 0.1983, 0.1292, 0.1857, 0.1833, 0.0886, 0.2428,\n",
       "        0.1872, 0.2067, 0.1996, 0.1881, 0.1901, 0.1885, 0.1984, 0.0754, 0.2066,\n",
       "        0.0607, 0.0754, 0.2060, 0.2116, 0.0556, 0.1792, 0.1748, 0.1841, 0.1743,\n",
       "        0.1662, 0.1982, 0.1582, 0.1935, 0.2182, 0.2067, 0.1855, 0.1778, 0.1900,\n",
       "        0.2124, 0.1215, 0.2092, 0.1929, 0.2434, 0.1936, 0.1948, 0.0622, 0.1852,\n",
       "        0.1868, 0.2035, 0.2310, 0.1794, 0.1655, 0.1756, 0.2074, 0.2194, 0.2152,\n",
       "        0.0502, 0.2294, 0.1950, 0.2149, 0.2024, 0.1727, 0.0657, 0.1919, 0.1847,\n",
       "        0.1900, 0.1825, 0.1898], requires_grad=True)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.h[0].ln_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'NTK_by_parts_factor': 8.0,\n",
       " 'NTK_by_parts_high_freq_factor': 4.0,\n",
       " 'NTK_by_parts_low_freq_factor': 1.0,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 8.0,\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='cuda'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_NTK_by_parts_rope': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 125 named parameters\n",
      "Name of param: transformer.wte.weight         Dimensions of tensor: torch.Size([50257, 768])      \n",
      "Name of param: transformer.h.0.ln_1.weight    Dimensions of tensor: torch.Size([768])             \n",
      "Name of param: transformer.h.0.ln_1.bias      Dimensions of tensor: torch.Size([768])             \n",
      "Name of param: transformer.h.0.attn.k_proj.weight Dimensions of tensor: torch.Size([768, 768])        \n",
      "Name of param: transformer.h.0.attn.v_proj.weight Dimensions of tensor: torch.Size([768, 768])        \n",
      "Name of param: transformer.h.0.attn.q_proj.weight Dimensions of tensor: torch.Size([768, 768])        \n",
      "Name of param: transformer.h.0.attn.out_proj.weight Dimensions of tensor: torch.Size([768, 768])        \n",
      "Name of param: transformer.h.0.mlp.fc_in.weight Dimensions of tensor: torch.Size([3072, 768])       \n",
      "Name of param: transformer.h.0.mlp.fc_in.bias Dimensions of tensor: torch.Size([3072])            \n",
      "Name of param: transformer.h.0.mlp.fc_out.weight Dimensions of tensor: torch.Size([768, 3072])       \n"
     ]
    }
   ],
   "source": [
    "MODELPATH = r\"C:\\Users\\allan\\ResearchStuff\\checkpoint-1953\"\n",
    "from transformers import AutoTokenizer, GPTJForCausalLM\n",
    "\n",
    "gptj = GPTJForCausalLM.from_pretrained(MODELPATH)\n",
    "gptj_param, gptj_names = get_params(gptj)\n",
    "\n",
    "\n",
    "unpack_names(gptj_names, first = 0, last = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 148 named parameters\n",
      "Model has 125 named parameters\n",
      "There are a different number of parameters between the models. Please check model config\n",
      "Model 1 has 148 parameters. Model 2 has 125\n",
      "There are 103 parameters that are different from each other\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['wpe.weight', torch.Size([1024, 768])],\n",
       "  ['transformer.h.0.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 1)],\n",
       " [['h.0.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.0.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 3)],\n",
       " [['h.0.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.0.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 4)],\n",
       " [['h.0.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.0.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 5)],\n",
       " [['h.0.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.0.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 7)],\n",
       " [['h.0.ln_2.weight', torch.Size([768])],\n",
       "  ['transformer.h.0.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 8)],\n",
       " [['h.0.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.0.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 9)],\n",
       " [['h.0.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.0.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 10)],\n",
       " [['h.0.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.1.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 11)],\n",
       " [['h.0.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.1.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 12)],\n",
       " [['h.0.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.1.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 13)],\n",
       " [['h.1.ln_1.weight', torch.Size([768])],\n",
       "  ['transformer.h.1.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 14)],\n",
       " [['h.1.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.1.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 15)],\n",
       " [['h.1.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.1.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 16)],\n",
       " [['h.1.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.1.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 17)],\n",
       " [['h.1.attn.c_proj.weight', torch.Size([768, 768])],\n",
       "  ['transformer.h.1.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 18)],\n",
       " [['h.1.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.1.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 19)],\n",
       " [['h.1.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.2.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 22)],\n",
       " [['h.1.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.2.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 23)],\n",
       " [['h.1.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.2.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 24)],\n",
       " [['h.1.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.2.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 25)],\n",
       " [['h.2.ln_1.weight', torch.Size([768])],\n",
       "  ['transformer.h.2.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 26)],\n",
       " [['h.2.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.2.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 27)],\n",
       " [['h.2.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.2.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 28)],\n",
       " [['h.2.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.2.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 29)],\n",
       " [['h.2.attn.c_proj.weight', torch.Size([768, 768])],\n",
       "  ['transformer.h.2.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 30)],\n",
       " [['h.2.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.3.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 33)],\n",
       " [['h.2.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.3.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 34)],\n",
       " [['h.2.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.3.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 35)],\n",
       " [['h.2.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.3.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 36)],\n",
       " [['h.2.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.3.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 37)],\n",
       " [['h.3.ln_1.weight', torch.Size([768])],\n",
       "  ['transformer.h.3.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 38)],\n",
       " [['h.3.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.3.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 39)],\n",
       " [['h.3.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.3.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 40)],\n",
       " [['h.3.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.4.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 41)],\n",
       " [['h.3.attn.c_proj.weight', torch.Size([768, 768])],\n",
       "  ['transformer.h.4.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 42)],\n",
       " [['h.3.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.4.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 43)],\n",
       " [['h.3.ln_2.weight', torch.Size([768])],\n",
       "  ['transformer.h.4.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 44)],\n",
       " [['h.3.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.4.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 45)],\n",
       " [['h.3.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.4.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 46)],\n",
       " [['h.3.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.4.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 47)],\n",
       " [['h.3.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.4.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 48)],\n",
       " [['h.3.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.4.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 49)],\n",
       " [['h.4.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.5.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 52)],\n",
       " [['h.4.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.5.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 53)],\n",
       " [['h.4.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.5.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 55)],\n",
       " [['h.4.ln_2.weight', torch.Size([768])],\n",
       "  ['transformer.h.5.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 56)],\n",
       " [['h.4.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.5.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 57)],\n",
       " [['h.4.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.5.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 58)],\n",
       " [['h.4.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.5.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 59)],\n",
       " [['h.4.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.5.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 60)],\n",
       " [['h.5.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.6.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 63)],\n",
       " [['h.5.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.6.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 64)],\n",
       " [['h.5.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.6.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 65)],\n",
       " [['h.5.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.6.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 67)],\n",
       " [['h.5.ln_2.weight', torch.Size([768])],\n",
       "  ['transformer.h.6.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 68)],\n",
       " [['h.5.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.6.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 69)],\n",
       " [['h.5.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.6.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 70)],\n",
       " [['h.5.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.7.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 71)],\n",
       " [['h.5.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.7.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 72)],\n",
       " [['h.5.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.7.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 73)],\n",
       " [['h.6.ln_1.weight', torch.Size([768])],\n",
       "  ['transformer.h.7.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 74)],\n",
       " [['h.6.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.7.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 75)],\n",
       " [['h.6.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.7.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 76)],\n",
       " [['h.6.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.7.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 77)],\n",
       " [['h.6.attn.c_proj.weight', torch.Size([768, 768])],\n",
       "  ['transformer.h.7.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 78)],\n",
       " [['h.6.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.7.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 79)],\n",
       " [['h.6.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.8.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 82)],\n",
       " [['h.6.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.8.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 83)],\n",
       " [['h.6.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.8.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 84)],\n",
       " [['h.6.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.8.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 85)],\n",
       " [['h.7.ln_1.weight', torch.Size([768])],\n",
       "  ['transformer.h.8.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 86)],\n",
       " [['h.7.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.8.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 87)],\n",
       " [['h.7.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.8.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 88)],\n",
       " [['h.7.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.8.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 89)],\n",
       " [['h.7.attn.c_proj.weight', torch.Size([768, 768])],\n",
       "  ['transformer.h.8.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 90)],\n",
       " [['h.7.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.9.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 93)],\n",
       " [['h.7.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.9.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 94)],\n",
       " [['h.7.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.9.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 95)],\n",
       " [['h.7.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.9.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 96)],\n",
       " [['h.7.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.9.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 97)],\n",
       " [['h.8.ln_1.weight', torch.Size([768])],\n",
       "  ['transformer.h.9.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 98)],\n",
       " [['h.8.ln_1.bias', torch.Size([768])],\n",
       "  ['transformer.h.9.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 99)],\n",
       " [['h.8.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.9.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 100)],\n",
       " [['h.8.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.10.ln_1.weight', torch.Size([768])],\n",
       "  ('index', 101)],\n",
       " [['h.8.attn.c_proj.weight', torch.Size([768, 768])],\n",
       "  ['transformer.h.10.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 102)],\n",
       " [['h.8.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.10.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 103)],\n",
       " [['h.8.ln_2.weight', torch.Size([768])],\n",
       "  ['transformer.h.10.attn.v_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 104)],\n",
       " [['h.8.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.10.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 105)],\n",
       " [['h.8.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.10.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 106)],\n",
       " [['h.8.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.10.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 107)],\n",
       " [['h.8.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.10.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 108)],\n",
       " [['h.8.mlp.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.10.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 109)],\n",
       " [['h.9.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['transformer.h.11.ln_1.bias', torch.Size([768])],\n",
       "  ('index', 112)],\n",
       " [['h.9.attn.c_attn.bias', torch.Size([2304])],\n",
       "  ['transformer.h.11.attn.k_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 113)],\n",
       " [['h.9.attn.c_proj.bias', torch.Size([768])],\n",
       "  ['transformer.h.11.attn.q_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 115)],\n",
       " [['h.9.ln_2.weight', torch.Size([768])],\n",
       "  ['transformer.h.11.attn.out_proj.weight', torch.Size([768, 768])],\n",
       "  ('index', 116)],\n",
       " [['h.9.ln_2.bias', torch.Size([768])],\n",
       "  ['transformer.h.11.mlp.fc_in.weight', torch.Size([3072, 768])],\n",
       "  ('index', 117)],\n",
       " [['h.9.mlp.c_fc.weight', torch.Size([768, 3072])],\n",
       "  ['transformer.h.11.mlp.fc_in.bias', torch.Size([3072])],\n",
       "  ('index', 118)],\n",
       " [['h.9.mlp.c_fc.bias', torch.Size([3072])],\n",
       "  ['transformer.h.11.mlp.fc_out.weight', torch.Size([768, 3072])],\n",
       "  ('index', 119)],\n",
       " [['h.9.mlp.c_proj.weight', torch.Size([3072, 768])],\n",
       "  ['transformer.h.11.mlp.fc_out.bias', torch.Size([768])],\n",
       "  ('index', 120)],\n",
       " [['h.10.ln_1.bias', torch.Size([768])],\n",
       "  ['lm_head.weight', torch.Size([50257, 768])],\n",
       "  ('index', 123)],\n",
       " [['h.10.attn.c_attn.weight', torch.Size([768, 2304])],\n",
       "  ['lm_head.bias', torch.Size([50257])],\n",
       "  ('index', 124)]]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(gpt2, gptj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"_name_or_path\": \"C:\\\\Users\\\\allan\\\\ResearchStuff\\\\checkpoint-1953\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptj.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
