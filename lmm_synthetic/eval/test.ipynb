{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM, AutoTokenizer  \n",
    "model_path = r\"C:\\Users\\allan\\ResearchStuff\\checkpoint-1953\"\n",
    "\n",
    "gptj = GPTJForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = NNsight(gptj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = \"GPTJ\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = gptj.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ -0.6713,  -1.8850,   0.1562,  ...,  -0.4049,  -1.4577,  -1.6844],\n",
      "         [ -1.0820,  -1.5449,   0.8616,  ...,   2.4094,  -1.6196,   0.0721],\n",
      "         [  0.4574,  -3.6470,   0.2879,  ...,   2.5716,   1.5716,   0.9055],\n",
      "         ...,\n",
      "         [ 10.3733,  -8.0781,   3.3568,  ...,  17.3844,  -9.1563,  -0.8096],\n",
      "         [-15.1537, -19.1792,  -3.2441,  ...,  27.9173,  11.6917,   4.5771],\n",
      "         [-24.8388, -45.5067, -15.0219,  ...,  52.7458,   2.0810,  -5.5088]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache())\n",
      "(tensor([[[ 1.7886e+00, -1.4395e+01, -1.2068e+00,  ...,  1.4213e+00,\n",
      "           1.2189e+00,  9.3767e+00],\n",
      "         [-1.8669e-01, -9.9831e+00,  5.2910e-01,  ...,  6.7367e+00,\n",
      "          -7.6761e-01,  7.6162e+00],\n",
      "         [-2.4369e+00, -6.1992e+00,  5.6117e-01,  ...,  5.5680e+00,\n",
      "          -1.8104e+00,  1.2986e+00],\n",
      "         ...,\n",
      "         [ 8.1073e+00, -1.7238e+01,  8.3116e+00,  ...,  2.5976e+01,\n",
      "          -6.6501e+00,  8.7413e+00],\n",
      "         [-1.9196e+01, -2.0085e+01,  1.7231e+00,  ...,  4.2003e+01,\n",
      "           5.5231e+00,  3.8826e+00],\n",
      "         [-3.0127e+01, -5.0129e+01, -9.4391e+00,  ...,  6.7050e+01,\n",
      "          -1.3444e-02, -4.0219e+00]]], grad_fn=<AddBackward0>), DynamicCache())\n"
     ]
    }
   ],
   "source": [
    "with test.trace(gen_tokens) as tracer:\n",
    "    h0_output = test.transformer.h[0].output.save()\n",
    "    h1_output = test.transformer.h[1].output.save()\n",
    "\n",
    "print(h0_output)\n",
    "print(h1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "with test.trace(gen_tokens) as tracer:\n",
    "    for i in range(12):\n",
    "        outputs.append((f\"Layer {i} output\", test.transformer.h[i].output.save()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Layer 0 output', (tensor([[[ -0.6713,  -1.8850,   0.1562,  ...,  -0.4049,  -1.4577,  -1.6844],\n",
      "         [ -1.0820,  -1.5449,   0.8616,  ...,   2.4094,  -1.6196,   0.0721],\n",
      "         [  0.4574,  -3.6470,   0.2879,  ...,   2.5716,   1.5716,   0.9055],\n",
      "         ...,\n",
      "         [ 10.3733,  -8.0781,   3.3568,  ...,  17.3844,  -9.1563,  -0.8096],\n",
      "         [-15.1537, -19.1792,  -3.2441,  ...,  27.9173,  11.6917,   4.5771],\n",
      "         [-24.8388, -45.5067, -15.0219,  ...,  52.7458,   2.0810,  -5.5088]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 1 output', (tensor([[[ 1.7886e+00, -1.4395e+01, -1.2068e+00,  ...,  1.4213e+00,\n",
      "           1.2189e+00,  9.3767e+00],\n",
      "         [-1.8669e-01, -9.9831e+00,  5.2910e-01,  ...,  6.7367e+00,\n",
      "          -7.6761e-01,  7.6162e+00],\n",
      "         [-2.4369e+00, -6.1992e+00,  5.6117e-01,  ...,  5.5680e+00,\n",
      "          -1.8104e+00,  1.2986e+00],\n",
      "         ...,\n",
      "         [ 8.1073e+00, -1.7238e+01,  8.3116e+00,  ...,  2.5976e+01,\n",
      "          -6.6501e+00,  8.7413e+00],\n",
      "         [-1.9196e+01, -2.0085e+01,  1.7231e+00,  ...,  4.2003e+01,\n",
      "           5.5231e+00,  3.8826e+00],\n",
      "         [-3.0127e+01, -5.0129e+01, -9.4391e+00,  ...,  6.7050e+01,\n",
      "          -1.3444e-02, -4.0219e+00]]], grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 2 output', (tensor([[[  2.3868, -11.0924,  -1.3673,  ...,  16.3749,  -2.8307,   5.8288],\n",
      "         [  5.1312,  -4.4111,   0.5994,  ...,  20.9291,  -2.5301,   6.3569],\n",
      "         [  4.9760,  -4.6263,  -0.5591,  ...,  18.9429,  -1.2174,   3.0265],\n",
      "         ...,\n",
      "         [ 10.7384,  -1.6136,  13.5008,  ...,   0.5610,  -3.6876,   7.9950],\n",
      "         [-14.9006,  -4.3280,   2.8147,  ...,  14.0892,   8.1437,   1.7465],\n",
      "         [-28.6251, -35.6747,  -8.2409,  ...,  39.1948,   3.5117,  -5.7506]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 3 output', (tensor([[[  1.6015, -10.4381,  -0.1708,  ...,  14.7052,  -3.2042,   4.1097],\n",
      "         [  3.6660,  -4.5505,   1.5236,  ...,  18.1609,  -2.3102,   4.4283],\n",
      "         [  4.2042,  -4.5903,   0.8117,  ...,  17.0495,  -0.6077,   1.8392],\n",
      "         ...,\n",
      "         [ 13.1828,   1.0918,  11.4981,  ...,   0.6876,  -2.2732,   8.0945],\n",
      "         [-12.2235,  -2.1010,  -2.9528,  ...,  11.1318,   9.3041,   0.6463],\n",
      "         [-23.2845, -32.1458, -12.5324,  ...,  33.7556,   5.4245,  -5.0888]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 4 output', (tensor([[[  1.2595,  -9.0216,   0.2235,  ...,  11.4625,  -3.7360,   3.4432],\n",
      "         [  2.7238,  -4.2939,   2.6081,  ...,  14.3352,  -2.2391,   3.2437],\n",
      "         [  2.7064,  -3.4243,   2.1757,  ...,  12.6687,  -0.3125,   1.6210],\n",
      "         ...,\n",
      "         [ 13.6171,   0.3679,  10.0230,  ...,  -0.1137,  -1.3363,   7.5084],\n",
      "         [-10.8018,  -4.6470,  -2.8469,  ...,  10.1172,  10.6114,  -4.1592],\n",
      "         [-20.8433, -32.1737, -11.0311,  ...,  29.2594,   7.2494,  -8.1831]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 5 output', (tensor([[[  0.3035,  -6.9694,  -0.8814,  ...,  10.7265,  -3.0516,   2.8525],\n",
      "         [  1.5755,  -2.8720,   1.4942,  ...,  13.5706,  -1.8969,   2.9174],\n",
      "         [  1.4120,  -2.5078,   1.4990,  ...,  11.6811,   0.0667,   1.0732],\n",
      "         ...,\n",
      "         [ 12.8889,   2.2729,  10.2558,  ...,   1.1333,  -1.5783,   6.4229],\n",
      "         [-11.4308,  -5.8279,  -4.2050,  ...,   4.9466,   9.7335,  -6.5296],\n",
      "         [-18.0505, -29.7790, -11.6701,  ...,  21.8519,   8.1153,  -9.2345]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 6 output', (tensor([[[ -0.1882,  -6.6684,  -0.9110,  ...,   7.6622,  -2.1388,   3.2341],\n",
      "         [  1.1064,  -3.1096,   1.0571,  ...,   9.7983,  -1.0536,   3.1338],\n",
      "         [  0.7773,  -2.7555,   1.8394,  ...,   9.0175,   1.1193,   1.1496],\n",
      "         ...,\n",
      "         [ 14.1224,   1.3974,  10.8406,  ...,  -1.2057,   1.4705,   7.4404],\n",
      "         [-11.8159,  -4.6553,  -4.9753,  ...,   3.3695,   9.6168,  -8.1056],\n",
      "         [-16.1396, -27.2378, -11.7040,  ...,  16.4113,  10.3456,  -8.9967]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 7 output', (tensor([[[  0.2153,  -4.8449,  -0.9570,  ...,   4.9180,  -1.6030,   2.8370],\n",
      "         [  1.5460,  -1.8386,   0.7502,  ...,   6.4334,  -1.2196,   2.5771],\n",
      "         [  0.0863,  -1.3220,   1.6564,  ...,   5.0036,   1.0611,   0.1882],\n",
      "         ...,\n",
      "         [ 14.2463,   2.7565,  11.7214,  ...,   0.8218,   2.5874,   7.9042],\n",
      "         [-11.5164,  -3.1035,  -7.0456,  ...,  -0.3183,   9.7133,  -9.6900],\n",
      "         [-14.2964, -24.1421, -12.4108,  ...,  12.2028,  11.6002,  -9.4313]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 8 output', (tensor([[[ 1.5436e-01, -2.8558e+00,  8.8998e-03,  ...,  2.8404e+00,\n",
      "          -9.5053e-01,  3.4078e+00],\n",
      "         [ 1.8670e+00, -1.4277e-01,  1.6591e+00,  ...,  4.0323e+00,\n",
      "          -6.3729e-01,  2.7482e+00],\n",
      "         [-1.0374e+00, -7.1265e-02,  2.1110e+00,  ...,  3.2158e+00,\n",
      "           1.0795e+00,  8.6598e-01],\n",
      "         ...,\n",
      "         [ 1.3594e+01,  3.8811e+00,  1.3069e+01,  ...,  2.6429e+00,\n",
      "           3.0903e+00,  8.4732e+00],\n",
      "         [-9.2626e+00, -1.9751e+00, -7.8485e+00,  ..., -2.5411e+00,\n",
      "           1.1167e+01, -9.6509e+00],\n",
      "         [-1.2215e+01, -2.1549e+01, -1.1422e+01,  ...,  1.0175e+01,\n",
      "           1.2761e+01, -8.5564e+00]]], grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 9 output', (tensor([[[ -1.1634,  -1.9008,   1.9517,  ...,   1.5229,  -1.3659,   3.2471],\n",
      "         [  0.9180,   0.7006,   3.6610,  ...,   2.4118,  -1.0561,   2.5477],\n",
      "         [ -2.5942,   0.2701,   4.0932,  ...,   2.1435,   0.9404,   0.5358],\n",
      "         ...,\n",
      "         [ 12.6873,   7.5288,  15.5159,  ...,   0.8071,   5.3887,   9.5051],\n",
      "         [ -9.3808,  -1.2336, -10.9295,  ...,  -4.8220,  12.1124, -10.6229],\n",
      "         [ -9.4761, -17.0816,  -6.1642,  ...,   8.4365,  15.4367,  -7.0178]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 10 output', (tensor([[[ -2.4176,  -0.4654,   3.1624,  ...,   1.1281,  -0.7692,   3.4435],\n",
      "         [  0.1196,   1.4508,   4.8782,  ...,   2.1553,  -0.8098,   2.5482],\n",
      "         [ -4.7921,   0.9221,   4.9542,  ...,   1.4626,   1.5180,   0.5484],\n",
      "         ...,\n",
      "         [ 11.8199,  11.5184,  16.3057,  ...,   3.1927,   6.2281,  10.6906],\n",
      "         [ -6.7782,  -0.6795, -15.3863,  ...,  -6.5312,  15.7814, -10.8567],\n",
      "         [ -5.1778, -15.1636, -11.9590,  ...,   3.9363,  19.2381,  -4.5893]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 11 output', (tensor([[[ -3.8036,   4.2859,   4.6002,  ...,   0.6859,  -1.3062,   3.4593],\n",
      "         [ -1.2668,   5.6039,   6.5484,  ...,   1.1877,  -1.5126,   2.2219],\n",
      "         [ -7.3138,   3.8452,   6.3068,  ...,   1.5840,   1.5178,   0.6992],\n",
      "         ...,\n",
      "         [  9.6616,  13.4938,  15.5472,  ...,   4.6712,   8.2413,  11.2492],\n",
      "         [-11.5463,   3.7559, -23.2890,  ..., -11.3482,  19.2644,  -9.4776],\n",
      "         [ -4.9247, -13.6974, -17.3426,  ...,   2.8067,  20.6775,  -3.3944]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n"
     ]
    }
   ],
   "source": [
    "for layer in outputs: \n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\"\"\"output = test.transformer.h[i].output.save()\n",
    "        print(i)\n",
    "        print(output)\n",
    "        print(output.type)\n",
    "        outputs.append((f\"Layer {i} output\", test.transformer.h[i].output.save()))  \"\"\"\n",
    "\n",
    "outputs = []    \n",
    "probs= []\n",
    "with test.trace(gen_tokens) as tracer:\n",
    "    for i in range(12):\n",
    "        outputs.append((f\"Layer {i} output\", test.transformer.h[i].output.save()))\n",
    "        probs.append(test.transformer.h[i].output.save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "new_prob = []\n",
    "\n",
    "for prob in probs:\n",
    "    new = torch.nn.functional.softmax(prob[0], dim=-1)\n",
    "    new_prob.append(new)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
