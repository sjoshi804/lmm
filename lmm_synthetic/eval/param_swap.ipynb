{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works\n",
      "Using renderer: notebook_connected\n"
     ]
    }
   ],
   "source": [
    "#Set up stuff from notebook\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DEVELOPMENT_MODE = False\n",
    "# Detect if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install if in Colab\n",
    "if IN_COLAB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    # Install a faster Node version\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
    "\n",
    "# Hot reload in development mode & not running on the CD\n",
    "if not IN_COLAB:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if not ip.extension_manager.loaded:\n",
    "        ip.extension_manager.load('autoreload')\n",
    "        %autoreload 2\n",
    "\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "print(\"works\")\n",
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")\n",
    "\n",
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model, show_param = False):\n",
    "    count = 0\n",
    "    parameters = []\n",
    "    names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if show_param == True:\n",
    "            print(name, param.size())\n",
    "        names.append([name, param.size()])\n",
    "        parameters.append(param.data)\n",
    "        count += 1\n",
    "    print(f\"Model has {count} named parameters\")\n",
    "    return parameters, names\n",
    "\n",
    "\n",
    "def unpack_names(names, first = 0, last = None):\n",
    "    if last == None:\n",
    "        for item in names[first:len(names)-1]:\n",
    "            print(f\"Param: {item[0]:<40} Dimensions of tensor: {item[1]:<30}\")\n",
    "    else:\n",
    "        for item in names[first: last]:\n",
    "            print(f\"Param: {item[0]:<40} Dimensions of tensor: {str(item[1]):<30}\")\n",
    "\n",
    "\n",
    "def compare(model_1, model_2):\n",
    "    different = []\n",
    "    model_1_params, model_1_names = get_params(model_1)\n",
    "    model_2_params, model_2_names = get_params(model_2)\n",
    "\n",
    "    if len(model_1_params) != len(model_2_params):\n",
    "        print(\"There are a different number of parameters between the models. Please check model config\")\n",
    "        print(f\"Model 1 has {len(model_1_params)} parameters. Model 2 has {len(model_2_params)}\")\n",
    "\n",
    "    for i in range(min(len(model_1_params),(len(model_2_params)))):\n",
    "        if model_1_names[i][1] != model_2_names[i][1]:\n",
    "            different.append([model_1_names[i], model_2_names[i], (\"index\", i)])\n",
    "\n",
    "    print(f\"There are {len(different)} parameters that are different from each other\")\n",
    "    return different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 125 named parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTJForCausalLM        \n",
    "model_path = r\"C:\\Users\\allan\\ResearchStuff\\checkpoint-1953\"\n",
    "\n",
    "gptj = GPTJForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "gptj_params, gptj_names = get_params(gptj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 197 named parameters\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"d_model\" : 768,\n",
    "    \"d_head\" : 64,\n",
    "    \"n_layers\" : 12,\n",
    "    \"n_ctx\" : 1024,\n",
    "    \"n_heads\" : 12,\n",
    "    \"d_mlp\" : 3072, \n",
    "    \"d_vocab\" : 50257,\n",
    "    \"act_fn\" : \"gelu_new\", \n",
    "    \"eps\" : 1e-05,\n",
    "    \"normalization_type\" : \"LN\", \n",
    "    \"positional_embedding_type\" : \"rotary\",\n",
    "    \"rotary_dim\": 64,\n",
    "    \"post_embedding_ln\": False, \n",
    "    \"original_architecture\" : \"gptj\",\n",
    "    \"use_normalization_before_and_after\" : False\n",
    "}\n",
    "\n",
    "\n",
    "hooked = HookedTransformer(config, tokenizer = AutoTokenizer.from_pretrained(model_path))\n",
    "hooked_params, hooked_names = get_params(hooked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_attn(model1 = hooked, model2 = gptj, num_blocks = 12): \n",
    "    for i in range(num_blocks):\n",
    "        # Access state dict\n",
    "        gptj_state_dict = gptj.state_dict()\n",
    "        hooked_state_dict = hooked.state_dict()\n",
    "\n",
    "        gptj_query_weight = f\"transformer.h.{i}.attn.q_proj.weight\"\n",
    "        gptj_value_weight = f\"transformer.h.{i}.attn.v_proj.weight\"\n",
    "        gptj_key_weight = f\"transformer.h.{i}.attn.k_proj.weight\"\n",
    "        gptj_output_weight = f\"transformer.h.{i}.attn.out_proj.weight\"\n",
    "\n",
    "        hooked_query_weight = f\"blocks.{i}.attn.W_Q\"\n",
    "        hooked_value_weight = f\"blocks.{i}.attn.W_V\"\n",
    "        hooked_key_weight = f\"blocks.{i}.attn.W_K\"\n",
    "        hooked_output_weight = f\"blocks.{i}.attn.W_O\"\n",
    "\n",
    "        # Extract weights\n",
    "        query_weight_tensor = gptj_state_dict[gptj_query_weight]\n",
    "        value_weight_tensor = gptj_state_dict[gptj_value_weight]\n",
    "        key_weight_tensor = gptj_state_dict[gptj_key_weight]\n",
    "        output_weight_tensor = gptj_state_dict[gptj_output_weight]\n",
    "\n",
    "        # Reshaping the query, key, and value weights\n",
    "        new_shape_query = query_weight_tensor.size()[:-1] + (12, 64) \n",
    "        query_weight_tensor = query_weight_tensor.view(new_shape_query)\n",
    "\n",
    "        new_shape_value = value_weight_tensor.size()[:-1] + (12, 64)  \n",
    "        value_weight_tensor = value_weight_tensor.view(new_shape_value)\n",
    "\n",
    "        new_shape_key = key_weight_tensor.size()[:-1] + (12, 64)  \n",
    "        key_weight_tensor = key_weight_tensor.view(new_shape_key)\n",
    "\n",
    "        # Output reshaping:\n",
    "        new_shape_output = (12, 64, 768)  # [768, 768] -> [12, 64, 768]\n",
    "        output_weight_tensor = output_weight_tensor.view(new_shape_output)\n",
    "\n",
    "        # Loop through each head \n",
    "        for j in range(12):  \n",
    "            hooked_state_dict[hooked_query_weight][j,:,:] = query_weight_tensor[:,j,:]\n",
    "            hooked_state_dict[hooked_value_weight][j,:,:] = value_weight_tensor[:,j,:]\n",
    "            hooked_state_dict[hooked_key_weight][j,:,:] = key_weight_tensor[:,j,:]\n",
    "\n",
    "            # Output has slightly different dimensions\n",
    "            hooked_state_dict[hooked_output_weight][j,:,:] = output_weight_tensor[j,:,:]\n",
    "\n",
    "        hooked.load_state_dict(hooked_state_dict)\n",
    "\n",
    "        print(f\"Successfully mapped attention weights for block {i} from GPT-J to Hooked Transformer.\")\n",
    "\n",
    "\n",
    "def set_bias_zero(model1 = hooked, model2 = gptj, num_blocks = 12):\n",
    "    hooked_state_dict = hooked.state_dict()\n",
    "\n",
    "    for i in range(12):\n",
    "        hooked_query_bias = f\"blocks.{i}.attn.b_Q\"\n",
    "        hooked_value_bias = f\"blocks.{i}.attn.b_V\"\n",
    "        hooked_key_bias = f\"blocks.{i}.attn.b_K\"\n",
    "        hooked_output_bias = f\"blocks.{i}.attn.b_O\"\n",
    "\n",
    "        hooked_state_dict[hooked_query_bias][i, :] = torch.zeros_like(hooked_state_dict[hooked_query_bias][i, :])\n",
    "        hooked_state_dict[hooked_value_bias][i, :] = torch.zeros_like(hooked_state_dict[hooked_value_bias][i, :])\n",
    "        hooked_state_dict[hooked_key_bias][i, :] = torch.zeros_like(hooked_state_dict[hooked_key_bias][i, :])\n",
    "        hooked_state_dict[hooked_output_bias][:] = torch.zeros_like(hooked_state_dict[hooked_output_bias])\n",
    "\n",
    "    hooked.load_state_dict(hooked_state_dict)\n",
    "\n",
    "def map_mlp(model1 = hooked, model2 = gptj, num_blocks = 12):\n",
    "    for i in range(num_blocks):\n",
    "        gptj_state_dict = gptj.state_dict()\n",
    "        hooked_state_dict = hooked.state_dict()\n",
    "\n",
    "        gptj_mlp_fc_in_weight = f\"transformer.h.{i}.mlp.fc_in.weight\"\n",
    "        gptj_mlp_fc_in_bias = f\"transformer.h.{i}.mlp.fc_in.bias\"\n",
    "        gptj_mlp_fc_out_weight = f\"transformer.h.{i}.mlp.fc_out.weight\"\n",
    "        gptj_mlp_fc_out_bias = f\"transformer.h.{i}.mlp.fc_out.bias\"\n",
    "\n",
    "        hooked_mlp_W_in = f\"blocks.{i}.mlp.W_in\"\n",
    "        hooked_mlp_b_in = f\"blocks.{i}.mlp.b_in\"\n",
    "        hooked_mlp_W_out = f\"blocks.{i}.mlp.W_out\"\n",
    "        hooked_mlp_b_out = f\"blocks.{i}.mlp.b_out\"\n",
    "\n",
    "        mlp_W_in_tensor = gptj_state_dict[gptj_mlp_fc_in_weight]\n",
    "        mlp_b_in_tensor = gptj_state_dict[gptj_mlp_fc_in_bias]\n",
    "        mlp_W_out_tensor = gptj_state_dict[gptj_mlp_fc_out_weight]\n",
    "        mlp_b_out_tensor = gptj_state_dict[gptj_mlp_fc_out_bias]\n",
    "\n",
    "        mlp_W_in_tensor = mlp_W_in_tensor.T  \n",
    "        mlp_W_out_tensor = mlp_W_out_tensor.T \n",
    "\n",
    "        hooked_state_dict[hooked_mlp_W_in] = mlp_W_in_tensor\n",
    "        hooked_state_dict[hooked_mlp_b_in] = mlp_b_in_tensor\n",
    "        hooked_state_dict[hooked_mlp_W_out] = mlp_W_out_tensor\n",
    "        hooked_state_dict[hooked_mlp_b_out] = mlp_b_out_tensor\n",
    "        \n",
    "        hooked.load_state_dict(hooked_state_dict)\n",
    "        print(f\"Successfully mapped MLP weights and biases for block {i} from GPT-J to Hooked Transformer.\")\n",
    "\n",
    "def map_embed(model1=hooked, model2=gptj):\n",
    "    gptj_state_dict = gptj.state_dict()\n",
    "    hooked_state_dict = hooked.state_dict()\n",
    "\n",
    "    gptj_wte_weight = \"transformer.wte.weight\"  \n",
    "    gptj_lm_head_bias = \"lm_head.bias\"         \n",
    "    \n",
    "    hooked_embed_W_E = \"embed.W_E\"             \n",
    "    hooked_unembed_W_U = \"unembed.W_U\"         \n",
    "    hooked_unembed_b_U = \"unembed.b_U\"         \n",
    "\n",
    "    # Extract the weights and biases from GPT-J\n",
    "    wte_weight_tensor = gptj_state_dict[gptj_wte_weight]  # Shape: [50257, 768]\n",
    "    lm_head_bias_tensor = gptj_state_dict[gptj_lm_head_bias]  # Shape: [50257]\n",
    "    \n",
    "    hooked_state_dict[hooked_embed_W_E] = wte_weight_tensor  # [50257, 768] -> [50257, 768]\n",
    "    hooked_state_dict[hooked_unembed_W_U] = wte_weight_tensor.T  # [50257, 768] -> [768, 50257]\n",
    "\n",
    "    hooked_state_dict[hooked_unembed_b_U] = lm_head_bias_tensor  # [50257] -> [50257]\n",
    "\n",
    "    hooked.load_state_dict(hooked_state_dict)\n",
    "    print(\"Successfully mapped embedding/unembedding weights and biases from GPT-J to Hooked Transformer.\")\n",
    "\n",
    "\n",
    "def map_ln_params(model1 = hooked, model2 = gptj, num_blocks = 12): # Model 2's ln params will be transferred to Model 1\n",
    "    for i in range(num_blocks):\n",
    "        gptj_state_dict = gptj.state_dict()\n",
    "        hooked_state_dict = hooked.state_dict()\n",
    "\n",
    "        gptj_ln_1_w = f\"transformer.h.{i}.ln_1.weight\"\n",
    "        gptj_ln_1_b = f\"transformer.h.{i}.ln_1.bias\"\n",
    "\n",
    "        hooked_ln1_w = f\"blocks.{i}.ln1.w\"\n",
    "        hooked_ln1_b = f\"blocks.{i}.ln1.b\"\n",
    "        hooked_ln2_w = f\"blocks.{i}.ln2.w\"\n",
    "        hooked_ln2_b = f\"blocks.{i}.ln2.b\"\n",
    "\n",
    "        if gptj_ln_1_w in gptj_state_dict and gptj_ln_1_b in gptj_state_dict:\n",
    "            # Extract the weight and bias from GPT-J\n",
    "            ln1_w_tensor = gptj_state_dict[gptj_ln_1_w]\n",
    "            ln1_b_tensor = gptj_state_dict[gptj_ln_1_b]\n",
    "\n",
    "            # Now map them to the Hooked Transformer model\n",
    "            if hooked_ln1_w in hooked_state_dict:\n",
    "                print(f\"Copying {gptj_ln_1_w} to {hooked_ln1_w}\")\n",
    "                hooked_state_dict[hooked_ln1_w] = ln1_w_tensor\n",
    "            else:\n",
    "                print(f\"{hooked_ln1_w} not found in Hooked Transformer.\")\n",
    "\n",
    "            if hooked_ln1_b in hooked_state_dict:\n",
    "                print(f\"Copying {gptj_ln_1_b} to {hooked_ln1_b}\")\n",
    "                hooked_state_dict[hooked_ln1_b] = ln1_b_tensor\n",
    "            else:\n",
    "                print(f\"{hooked_ln1_b} not found in Hooked Transformer.\")\n",
    "\n",
    "            # Now copy the same weights to ln2.w and ln2.b in the Hooked Transformer\n",
    "            if hooked_ln2_w in hooked_state_dict:\n",
    "                print(f\"Copying {gptj_ln_1_w} to {hooked_ln2_w}\")\n",
    "                hooked_state_dict[hooked_ln2_w] = ln1_w_tensor  # Same weight for ln2\n",
    "            else:\n",
    "                print(f\"{hooked_ln2_w} not found in Hooked Transformer.\")\n",
    "\n",
    "            if hooked_ln2_b in hooked_state_dict:\n",
    "                print(f\"Copying {gptj_ln_1_b} to {hooked_ln2_b}\")\n",
    "                hooked_state_dict[hooked_ln2_b] = ln1_b_tensor  # Same bias for ln2\n",
    "            else:\n",
    "                print(f\"{hooked_ln2_b} not found in Hooked Transformer.\")\n",
    "        else:\n",
    "            print(f\"Missing {gptj_ln_1_w} or {gptj_ln_1_b} in GPT-J state dict.\")\n",
    "\n",
    "        hooked.load_state_dict(hooked_state_dict)\n",
    "        print(f\"Successfully updated Hooked Transformer with GPT-J layer normalization parameters for block {i}.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_param():\n",
    "    #Swaps hooked model's linear nomalization parameters with custom\n",
    "    map_ln_params()\n",
    "\n",
    "    # Sets bias of Q, V, K, O to 0 \n",
    "    set_bias_zero()\n",
    "    \n",
    "    #Swaps hooked model's Q, V, K, O weights with custom \n",
    "    map_attn()\n",
    "\n",
    "    #Swaps hooked model's MLP weights+biases with custom \n",
    "    map_mlp()\n",
    "\n",
    "    # Swaps embedding, unembed \n",
    "    map_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: tensor(11.4938, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_description_text = \"\"\"## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
    "loss = hooked(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
