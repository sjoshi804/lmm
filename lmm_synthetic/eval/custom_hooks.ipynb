{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM, AutoTokenizer  \n",
    "model_path = r\"C:\\Users\\allan\\ResearchStuff\\checkpoint-1953\"\n",
    "\n",
    "gptj = GPTJForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = NNsight(gptj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = \"GPTJ\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = gptj.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTJBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc_out): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 38,  47,  51,  41, 352, 198, 276, 267, 265, 274, 277, 279,  11, 278,\n",
       "         280,  30, 271,  25, 274, 277, 281,  30, 271,  25, 369, 369, 369, 369,\n",
       "         369, 369, 369, 369, 369, 198, 276, 267, 265, 274, 277, 280,  11, 278,\n",
       "         280,  30, 271,  25, 327, 198, 276, 267, 265, 274, 277, 281,  11, 278,\n",
       "         279,  30, 271,  25, 327, 198, 276, 267, 265, 274, 277, 280,  11, 278,\n",
       "         281,  30, 271,  25, 327, 198, 276, 267, 265, 274, 277, 279,  11, 278,\n",
       "         279,  30, 271,  25, 327, 198, 276, 267, 265, 274, 277, 281,  11, 278,\n",
       "         281,  30]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "with test.trace(gen_tokens) as tracer:\n",
    "    for i in range(12):\n",
    "        outputs.append((f\"Layer {i} output\", test.transformer.h[i].output.save()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Layer 0 output', (tensor([[[ -0.6713,  -1.8850,   0.1562,  ...,  -0.4049,  -1.4577,  -1.6844],\n",
      "         [ -1.0820,  -1.5449,   0.8616,  ...,   2.4094,  -1.6196,   0.0721],\n",
      "         [  0.4574,  -3.6470,   0.2879,  ...,   2.5716,   1.5716,   0.9055],\n",
      "         ...,\n",
      "         [-12.5838,   4.0089,  -9.0687,  ...,   9.0440, -16.7805, -22.5958],\n",
      "         [ -9.2117,  -7.6412,  19.8870,  ...,   7.1713,   4.8419,  -7.7082],\n",
      "         [ -8.4324,  -5.8789, -10.2080,  ...,  11.3134,  -2.3691,  -9.8640]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 1 output', (tensor([[[  1.7886, -14.3946,  -1.2068,  ...,   1.4213,   1.2189,   9.3767],\n",
      "         [ -0.1867,  -9.9831,   0.5291,  ...,   6.7367,  -0.7676,   7.6162],\n",
      "         [ -2.4369,  -6.1992,   0.5612,  ...,   5.5680,  -1.8104,   1.2986],\n",
      "         ...,\n",
      "         [-12.4149,  -4.6207,  -4.6001,  ...,  20.3238, -15.4822, -11.9477],\n",
      "         [-12.1443, -11.5921,  24.4067,  ...,  18.0178,   3.3847,  -5.9256],\n",
      "         [ -9.1087, -12.3617,  -5.6254,  ...,  16.0716,  -0.7852,   0.8298]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 2 output', (tensor([[[  2.3868, -11.0924,  -1.3673,  ...,  16.3749,  -2.8307,   5.8288],\n",
      "         [  5.1312,  -4.4111,   0.5994,  ...,  20.9291,  -2.5301,   6.3569],\n",
      "         [  4.9760,  -4.6263,  -0.5591,  ...,  18.9429,  -1.2174,   3.0265],\n",
      "         ...,\n",
      "         [-13.7506,   3.2040,   1.8331,  ...,  30.5987, -16.1047, -11.9347],\n",
      "         [ -8.4112, -10.6054,  16.2824,  ...,  29.1578,   6.3892,  -3.9110],\n",
      "         [-14.7379,  -7.4311, -10.3385,  ...,  19.4864, -10.7470,  -9.0764]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 3 output', (tensor([[[  1.6015, -10.4381,  -0.1708,  ...,  14.7052,  -3.2042,   4.1097],\n",
      "         [  3.6660,  -4.5505,   1.5236,  ...,  18.1609,  -2.3102,   4.4283],\n",
      "         [  4.2042,  -4.5903,   0.8117,  ...,  17.0495,  -0.6077,   1.8392],\n",
      "         ...,\n",
      "         [-12.6224,   4.6739,   0.7926,  ...,  29.8100, -15.4493, -11.4055],\n",
      "         [ -9.5886,  -9.2076,  13.2408,  ...,  27.1200,   7.9735,  -3.5526],\n",
      "         [-16.8923,  -5.4011, -10.6959,  ...,  17.2813, -11.0971, -10.1743]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 4 output', (tensor([[[  1.2595,  -9.0216,   0.2235,  ...,  11.4625,  -3.7360,   3.4432],\n",
      "         [  2.7238,  -4.2939,   2.6081,  ...,  14.3352,  -2.2391,   3.2437],\n",
      "         [  2.7064,  -3.4243,   2.1757,  ...,  12.6687,  -0.3125,   1.6210],\n",
      "         ...,\n",
      "         [-12.3441,   5.0632,   1.5469,  ...,  28.2381, -14.7150, -11.1322],\n",
      "         [-10.0291,  -9.6526,  13.0317,  ...,  26.8053,   9.4144,  -3.9907],\n",
      "         [-16.4323,  -4.2602,  -9.4010,  ...,  15.4122, -10.7334, -12.2814]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 5 output', (tensor([[[  0.3035,  -6.9694,  -0.8814,  ...,  10.7265,  -3.0516,   2.8525],\n",
      "         [  1.5755,  -2.8720,   1.4942,  ...,  13.5706,  -1.8969,   2.9174],\n",
      "         [  1.4120,  -2.5078,   1.4990,  ...,  11.6811,   0.0667,   1.0732],\n",
      "         ...,\n",
      "         [-10.8742,   7.0037,   2.1535,  ...,  29.6086, -12.9816, -11.5334],\n",
      "         [ -9.0758,  -8.0789,   9.9749,  ...,  28.2498,  12.1068,  -4.5360],\n",
      "         [-14.6224,  -2.2669,  -8.9116,  ...,  17.2957,  -8.7572, -13.8017]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 6 output', (tensor([[[ -0.1882,  -6.6684,  -0.9110,  ...,   7.6622,  -2.1388,   3.2341],\n",
      "         [  1.1064,  -3.1096,   1.0571,  ...,   9.7983,  -1.0536,   3.1338],\n",
      "         [  0.7773,  -2.7555,   1.8394,  ...,   9.0175,   1.1193,   1.1496],\n",
      "         ...,\n",
      "         [ -9.2684,   8.3952,   2.1877,  ...,  28.9750, -11.6253,  -9.8959],\n",
      "         [ -7.7972,  -7.7845,  10.2445,  ...,  24.8063,  13.3425,  -5.2229],\n",
      "         [-12.8384,  -1.6982,  -8.3173,  ...,  16.0942,  -8.3634, -13.5881]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 7 output', (tensor([[[  0.2153,  -4.8449,  -0.9570,  ...,   4.9180,  -1.6030,   2.8370],\n",
      "         [  1.5460,  -1.8386,   0.7502,  ...,   6.4334,  -1.2196,   2.5771],\n",
      "         [  0.0863,  -1.3220,   1.6564,  ...,   5.0036,   1.0611,   0.1882],\n",
      "         ...,\n",
      "         [ -7.3011,   9.5867,   2.5971,  ...,  26.8090, -11.3077,  -9.2725],\n",
      "         [ -5.3706,  -5.9465,  10.6499,  ...,  24.4539,  14.9813,  -5.7206],\n",
      "         [-10.9954,  -1.9467,  -7.5621,  ...,  15.4612,  -9.8106, -14.2035]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 8 output', (tensor([[[ 1.5436e-01, -2.8558e+00,  8.8998e-03,  ...,  2.8404e+00,\n",
      "          -9.5053e-01,  3.4078e+00],\n",
      "         [ 1.8670e+00, -1.4277e-01,  1.6591e+00,  ...,  4.0323e+00,\n",
      "          -6.3729e-01,  2.7482e+00],\n",
      "         [-1.0374e+00, -7.1265e-02,  2.1110e+00,  ...,  3.2158e+00,\n",
      "           1.0795e+00,  8.6598e-01],\n",
      "         ...,\n",
      "         [-7.6653e+00,  1.0864e+01,  2.1543e+00,  ...,  2.5742e+01,\n",
      "          -1.0416e+01, -7.5732e+00],\n",
      "         [-3.0313e+00, -3.6108e+00,  9.5138e+00,  ...,  2.3844e+01,\n",
      "           1.7086e+01, -5.4266e+00],\n",
      "         [-6.0619e+00,  1.0184e+00, -5.6275e+00,  ...,  1.3353e+01,\n",
      "          -7.2399e+00, -1.3442e+01]]], grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 9 output', (tensor([[[ -1.1634,  -1.9008,   1.9517,  ...,   1.5229,  -1.3659,   3.2471],\n",
      "         [  0.9180,   0.7006,   3.6610,  ...,   2.4118,  -1.0561,   2.5477],\n",
      "         [ -2.5942,   0.2701,   4.0932,  ...,   2.1435,   0.9404,   0.5358],\n",
      "         ...,\n",
      "         [ -7.3969,  12.9316,   3.2907,  ...,  25.6337,  -8.3417,  -5.2565],\n",
      "         [ -0.9454,  -2.3600,  12.7978,  ...,  22.2194,  20.5404,  -6.2614],\n",
      "         [ -3.3057,   1.5400,  -3.4093,  ...,  11.3567,  -5.3984, -12.0695]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 10 output', (tensor([[[ -2.4176,  -0.4654,   3.1624,  ...,   1.1281,  -0.7692,   3.4435],\n",
      "         [  0.1196,   1.4508,   4.8782,  ...,   2.1553,  -0.8098,   2.5482],\n",
      "         [ -4.7921,   0.9221,   4.9542,  ...,   1.4626,   1.5180,   0.5484],\n",
      "         ...,\n",
      "         [ -3.7543,  18.5850,   1.2189,  ...,  25.8687,  -6.3652,  -1.2245],\n",
      "         [ -0.9790,   3.2569,  14.3073,  ...,  25.4187,  21.7329,  -4.3028],\n",
      "         [ -0.0707,   2.9369,  -3.3269,  ...,  10.9037,  -2.6758, -11.0129]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n",
      "('Layer 11 output', (tensor([[[-3.8036,  4.2859,  4.6002,  ...,  0.6859, -1.3062,  3.4593],\n",
      "         [-1.2668,  5.6039,  6.5484,  ...,  1.1877, -1.5126,  2.2219],\n",
      "         [-7.3138,  3.8452,  6.3068,  ...,  1.5840,  1.5178,  0.6992],\n",
      "         ...,\n",
      "         [-5.9534, 31.5268,  1.0321,  ..., 31.3515, -5.6153,  2.7510],\n",
      "         [-0.9543, 12.0916, 15.1646,  ..., 26.8213, 24.5662, -0.3071],\n",
      "         [ 0.7029, 11.4842, -9.0536,  ..., 12.9492, -0.2180, -7.6457]]],\n",
      "       grad_fn=<AddBackward0>), DynamicCache()))\n"
     ]
    }
   ],
   "source": [
    "for layer in outputs: \n",
    "    print(layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
