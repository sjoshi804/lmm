{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"/home/allanz/data/datasets/v3.1_spatial_grid_multimodal/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "from lmm_synthetic.data.convert_to_multimodal import parse_grid_from_text\n",
    "\n",
    "def find_text(text, char, index):\n",
    "    count = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == char:\n",
    "            count += 1\n",
    "            if count == index:\n",
    "                return i \n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for multimodal supervised fine-tuning\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the dataset.\n",
    "        split (str): Dataset split (e.g., 'train', 'test').\n",
    "        max_data_size (int, optional): Maximum number of data samples to load. Defaults to -1 (load all).\n",
    "        vision_token_ablation (bool, optional): Whether to perform vision token ablation. Defaults to False.\n",
    "        debug (bool, optional): Whether to enable debug mode. Defaults to False.\n",
    "        image_grid (bool, optional): Whether to have dataset only include image and text grid\n",
    "        sub_sampling (bool, optional): Whether to subsample the conversations\n",
    "        num_samples (int, optional): Number of conversations to subsample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_path: str, \n",
    "        split: str,\n",
    "        max_data_size: int = -1,\n",
    "        vision_token_ablation: bool = False,\n",
    "        debug: bool = False,\n",
    "        image_grid: bool = False,\n",
    "        sub_sampling: bool = False,\n",
    "        num_samples: int = 3\n",
    "    ) -> None:\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.debug = debug\n",
    "        self.vision_token_ablation = vision_token_ablation\n",
    "\n",
    "        # Load the dataset from disk\n",
    "        hf_dataset = load_from_disk(data_path)[split]\n",
    "        self.list_data_dict = []\n",
    "\n",
    "        if image_grid == True:\n",
    "            for sample in hf_dataset:\n",
    "                prompt = sample.get(\"prompt\", \"\")\n",
    "                grid_index = find_text(sample.get(\"text\", \"\"), \"\\n\", 3)\n",
    "                grid = sample.get(\"text\", \"\")[0:grid_index]\n",
    "                conversations = [prompt, grid]\n",
    "                data_dict = {\n",
    "                    \"image\": sample.get(\"image\", None),\n",
    "                    \"prompt\": prompt,\n",
    "                    \"conversations\": conversations\n",
    "                }\n",
    "                if self.debug:\n",
    "                    data_dict[\"text\"] = sample.get(\"text\", \"\")\n",
    "                if self.vision_token_ablation:\n",
    "                    data_dict[\"grid\"] = sample.get('grid', parse_grid_from_text(sample['text']))\n",
    "                self.list_data_dict.append(data_dict)\n",
    "\n",
    "        elif sub_sampling == True:\n",
    "            # Process each sample in the dataset\n",
    "            for sample in hf_dataset:\n",
    "                prompt = sample.get(\"prompt\", \"\")\n",
    "                conversations = []\n",
    "                for i in range(num_samples + 1):\n",
    "                    temp = random.choice(sample[\"conversations\"])\n",
    "                    conversations.append(temp)\n",
    "                data_dict = {\n",
    "                    \"image\": sample.get(\"image\", None),\n",
    "                    \"prompt\": prompt,\n",
    "                    \"conversations\": conversations\n",
    "                }\n",
    "                if self.debug:\n",
    "                    data_dict[\"text\"] = sample.get(\"text\", \"\")\n",
    "                if self.vision_token_ablation:\n",
    "                    data_dict[\"grid\"] = sample.get('grid', parse_grid_from_text(sample['text']))\n",
    "                self.list_data_dict.append(data_dict)\n",
    "        else:\n",
    "        # Process each sample in the dataset\n",
    "            for sample in hf_dataset:\n",
    "                prompt = sample.get(\"prompt\", \"\")\n",
    "                conversations = sample.get(\"conversations\", [])\n",
    "                data_dict = {\n",
    "                    \"image\": sample.get(\"image\", None),\n",
    "                    \"prompt\": prompt,\n",
    "                    \"conversations\": conversations\n",
    "                }\n",
    "                if self.debug:\n",
    "                    data_dict[\"text\"] = sample.get(\"text\", \"\")\n",
    "                if self.vision_token_ablation:\n",
    "                    data_dict[\"grid\"] = sample.get('grid', parse_grid_from_text(sample['text']))\n",
    "                self.list_data_dict.append(data_dict)\n",
    "\n",
    "        # Limit the dataset size if max_data_size is specified\n",
    "        if max_data_size > 0:\n",
    "            self.list_data_dict = self.list_data_dict[:max_data_size]\n",
    "\n",
    "        logger.info(f\"Dataset size: {len(self.list_data_dict)}\")\n",
    "\n",
    "        # Determine whether each sample is text-only\n",
    "        self.is_text_only = [\n",
    "            \"image\" not in source for source in self.list_data_dict\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, List]:\n",
    "        \"\"\"Retrieves the sample at index `i`.\n",
    "\n",
    "        Args:\n",
    "            i (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List]: A dictionary containing the sample data.\n",
    "        \"\"\"\n",
    "        sample = self.list_data_dict[i]\n",
    "        item_dict = {\n",
    "            \"image\": Image.open(sample[\"image\"]).convert(\"RGB\"),\n",
    "            \"prompt\": sample[\"prompt\"],\n",
    "            \"conversations\": sample[\"conversations\"]\n",
    "        }\n",
    "        if self.debug:\n",
    "            item_dict[\"text\"] = sample[\"text\"]\n",
    "        if self.vision_token_ablation:\n",
    "            item_dict[\"grid\"] = sample[\"grid\"]\n",
    "        return item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-22 12:42:11.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mDataset size: 10\u001b[0m\n",
      "\u001b[32m2025-01-22 12:42:18.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mDataset size: 10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are subsampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-22 12:42:25.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mDataset size: 10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "original = LazySupervisedDataset(r\"/home/allanz/data/datasets/v3.1_spatial_grid_multimodal/\", \"train\", 10, False, False)\n",
    "image_grid = LazySupervisedDataset(r\"/home/allanz/data/datasets/v3.1_spatial_grid_multimodal/\", \"train\", 10, False, False, True)\n",
    "sub_sample = LazySupervisedDataset(r\"/home/allanz/data/datasets/v3.1_spatial_grid_multimodal/\", \"train\", 10, False, False, False, True, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=256x256>,\n",
       " 'prompt': \"The grid above is size 3 by 3. Each cell contains an object from ['deer', 'bird', 'dog', 'cat'].\",\n",
       " 'conversations': [['What object is in row 2, column 2?', 'A: bird'],\n",
       "  ['What object is in row 1, column 2?', 'A: cat'],\n",
       "  ['What object is in row 0, column 0?', 'A: deer'],\n",
       "  ['What object is in row 0, column 1?', 'A: cat'],\n",
       "  ['What object is in row 1, column 0?', 'A: deer'],\n",
       "  ['What object is in row 1, column 1?', 'A: cat'],\n",
       "  ['What object is in row 0, column 2?', 'A: bird'],\n",
       "  ['What object is in row 2, column 1?', 'A: dog'],\n",
       "  ['What object is in row 2, column 0?', 'A: bird']]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=256x256>,\n",
       " 'prompt': \"The grid above is size 3 by 3. Each cell contains an object from ['deer', 'bird', 'dog', 'cat'].\",\n",
       " 'conversations': [\"The grid above is size 3 by 3. Each cell contains an object from ['deer', 'bird', 'dog', 'cat'].\",\n",
       "  '| deer | cat | bird |\\n| deer | cat | cat |\\n| bird | dog | bird |']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=256x256>,\n",
       " 'prompt': \"The grid above is size 3 by 3. Each cell contains an object from ['deer', 'bird', 'dog', 'cat'].\",\n",
       " 'conversations': [['What object is in row 0, column 0?', 'A: deer'],\n",
       "  ['What object is in row 0, column 2?', 'A: bird'],\n",
       "  ['What object is in row 0, column 2?', 'A: bird'],\n",
       "  ['What object is in row 2, column 0?', 'A: bird']]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_sample[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
