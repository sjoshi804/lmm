model:
  vocab_size: 50257
  n_positions: 1024
  n_embd: 768
  n_layer: 12
  n_head: 12
  intermediate_size: 3072

data:
  dataset_name: "wikitext"
  dataset_config_name: "wikitext-2-raw-v1"
  max_length: 1024

training:
  output_dir: "./gpt2-model"
  num_train_epochs: 1
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  logging_dir: "./logs"
  logging_steps: 500
  save_total_limit: 3
  save_steps: 1
  deepspeed: "ds_configs/zero2.json"
